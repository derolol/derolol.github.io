<!doctype html>
<html lang="en"><head><meta charset="utf-8"><meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1"><meta name="robots" content="noindex"><meta><title>Category: interview - Derolol Blog</title><link rel="manifest" href="/manifest.json"><meta name="application-name" content="Derolol Blog"><meta name="msapplication-TileImage" content="https://raw.githubusercontent.com/derolol/picgo/main/images/logo.png"><meta name="apple-mobile-web-app-capable" content="yes"><meta name="apple-mobile-web-app-title" content="Derolol Blog"><meta name="apple-mobile-web-app-status-bar-style" content="default"><meta property="og:type" content="blog"><meta property="og:title" content="Derolol Blog"><meta property="og:url" content="https://derolol.github.io/"><meta property="og:site_name" content="Derolol Blog"><meta property="og:locale" content="en_US"><meta property="og:image" content="https://derolol.github.io/img/og_image.png"><meta property="article:author" content="derolol"><meta property="twitter:card" content="summary"><meta property="twitter:image:src" content="https://derolol.github.io/img/og_image.png"><script type="application/ld+json">{"@context":"https://schema.org","@type":"BlogPosting","mainEntityOfPage":{"@type":"WebPage","@id":"https://derolol.github.io"},"headline":"Derolol Blog","image":["https://derolol.github.io/img/og_image.png"],"author":{"@type":"Person","name":"derolol"},"publisher":{"@type":"Organization","name":"Derolol Blog","logo":{"@type":"ImageObject","url":"https://raw.githubusercontent.com/derolol/picgo/main/images/head.png"}},"description":""}</script><link rel="icon" href="https://raw.githubusercontent.com/derolol/picgo/main/images/logo.png"><link rel="stylesheet" href="https://use.fontawesome.com/releases/v6.0.0/css/all.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/highlight.js@11.7.0/styles/atom-one-light.css"><link rel="stylesheet" href="https://fonts.googleapis.com/css2?family=Ubuntu:wght@400;600&amp;family=Source+Code+Pro"><link rel="stylesheet" href="/css/default.css"><style>body>.footer,body>.navbar,body>.section{opacity:0}</style><!--!--><!--!--><!--!--><script src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js" defer></script><!--!--><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/cookieconsent@3.1.1/build/cookieconsent.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/lightgallery@1.10.0/dist/css/lightgallery.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/justifiedGallery@3.8.1/dist/css/justifiedGallery.min.css"><!--!--><!--!--><style>.pace{-webkit-pointer-events:none;pointer-events:none;-webkit-user-select:none;-moz-user-select:none;user-select:none}.pace-inactive{display:none}.pace .pace-progress{background:#3273dc;position:fixed;z-index:2000;top:0;right:100%;width:100%;height:2px}</style><script src="https://cdn.jsdelivr.net/npm/pace-js@1.2.4/pace.min.js"></script><!--!--><!--!--><!-- hexo injector head_end start --><script>
  (function () {
      function switchTab() {
          if (!location.hash) {
            return;
          }

          const id = '#' + CSS.escape(location.hash.substring(1));
          const $tabMenu = document.querySelector(`.tabs a[href="${id}"]`);
          if (!$tabMenu) {
            return;
          }

          const $tabMenuContainer = $tabMenu.parentElement.parentElement;
          Array.from($tabMenuContainer.children).forEach($menu => $menu.classList.remove('is-active'));
          Array.from($tabMenuContainer.querySelectorAll('a'))
              .map($menu => document.getElementById($menu.getAttribute("href").substring(1)))
              .forEach($content => $content.classList.add('is-hidden'));

          if ($tabMenu) {
              $tabMenu.parentElement.classList.add('is-active');
          }
          const $activeTab = document.querySelector(id);
          if ($activeTab) {
              $activeTab.classList.remove('is-hidden');
          }
      }
      switchTab();
      window.addEventListener('hashchange', switchTab, false);
  })();
  </script><!-- hexo injector head_end end --><meta name="generator" content="Hexo 7.2.0"><style>mjx-container[jax="SVG"] {
  direction: ltr;
}

mjx-container[jax="SVG"] > svg {
  overflow: visible;
}

mjx-container[jax="SVG"][display="true"] {
  display: block;
  text-align: center;
  margin: 1em 0;
}

mjx-container[jax="SVG"][justify="left"] {
  text-align: left;
}

mjx-container[jax="SVG"][justify="right"] {
  text-align: right;
}

g[data-mml-node="merror"] > g {
  fill: red;
  stroke: red;
}

g[data-mml-node="merror"] > rect[data-background] {
  fill: yellow;
  stroke: none;
}

g[data-mml-node="mtable"] > line[data-line] {
  stroke-width: 70px;
  fill: none;
}

g[data-mml-node="mtable"] > rect[data-frame] {
  stroke-width: 70px;
  fill: none;
}

g[data-mml-node="mtable"] > .mjx-dashed {
  stroke-dasharray: 140;
}

g[data-mml-node="mtable"] > .mjx-dotted {
  stroke-linecap: round;
  stroke-dasharray: 0,140;
}

g[data-mml-node="mtable"] > svg {
  overflow: visible;
}

[jax="SVG"] mjx-tool {
  display: inline-block;
  position: relative;
  width: 0;
  height: 0;
}

[jax="SVG"] mjx-tool > mjx-tip {
  position: absolute;
  top: 0;
  left: 0;
}

mjx-tool > mjx-tip {
  display: inline-block;
  padding: .2em;
  border: 1px solid #888;
  font-size: 70%;
  background-color: #F8F8F8;
  color: black;
  box-shadow: 2px 2px 5px #AAAAAA;
}

g[data-mml-node="maction"][data-toggle] {
  cursor: pointer;
}

mjx-status {
  display: block;
  position: fixed;
  left: 1em;
  bottom: 1em;
  min-width: 25%;
  padding: .2em .4em;
  border: 1px solid #888;
  font-size: 90%;
  background-color: #F8F8F8;
  color: black;
}

foreignObject[data-mjx-xml] {
  font-family: initial;
  line-height: normal;
  overflow: visible;
}

.MathJax path {
  stroke-width: 3;
}

mjx-container[display="true"] {
  overflow: auto hidden;
}

mjx-container[display="true"] + br {
  display: none;
}
</style></head><body class="is-3-column"><nav class="navbar navbar-main"><div class="container navbar-container"><div class="navbar-brand justify-content-center"><a class="navbar-item navbar-logo" href="/"><img src="https://raw.githubusercontent.com/derolol/picgo/main/images/head.png" alt="Derolol Blog" height="28"></a></div><div class="navbar-menu"><div class="navbar-start"><a class="navbar-item" href="/">Home</a><a class="navbar-item" href="/archives">Archives</a><a class="navbar-item" href="/categories">Categories</a><a class="navbar-item" href="/tags">Tags</a><a class="navbar-item" href="/about">About</a></div><div class="navbar-end"><a class="navbar-item" target="_blank" rel="noopener" title="Download on GitHub" href="https://github.com/ppoffice/hexo-theme-icarus"><i class="fab fa-github"></i></a><a class="navbar-item search" title="Search" href="javascript:;"><i class="fas fa-search"></i></a></div></div></div></nav><section class="section"><div class="container"><div class="columns"><div class="column order-2 column-main is-8-tablet is-8-desktop is-6-widescreen"><div class="card"><div class="card-content"><nav class="breadcrumb" aria-label="breadcrumbs"><ul><li><a href="/categories">Categories</a></li><li class="is-active"><a href="#" aria-current="page">interview</a></li></ul></nav></div></div><div class="card"><article class="card-content article" role="article"><div class="article-meta is-size-7 is-uppercase level is-mobile"><div class="level-left"><span class="level-item">Posted&nbsp;<time dateTime="2024-09-26T02:29:09.000Z" title="2024/9/26 上午10:29:09">2024-09-26</time></span><span class="level-item">Updated&nbsp;<time dateTime="2024-09-26T02:29:09.000Z" title="2024/9/26 上午10:29:09">2024-09-26</time></span><span class="level-item"><a class="link-muted" href="/categories/interview/">interview</a></span></div></div><p class="title is-3 is-size-4-mobile"><a class="link-muted" href="/2024/09/26/interview/experience/">面试经验</a></p><div class="content"><h2 id="语义分割模型研究">语义分割模型研究</h2>
<h3 id="介绍一下经典的语义分割模型">1. 介绍一下经典的语义分割模型</h3>
<ol type="1">
<li><p><strong>FCN (全卷积网络)</strong> FCN
是最早的端到端语义分割模型之一，它通过去除全连接层并替换为卷积层来实现任意输入尺寸的图像处理。FCN
利用跳跃连接来结合不同层次的特征图以恢复细节信息。</p></li>
<li><p><strong>SegNet</strong> SegNet 基于 VGG16
架构，其主要特点是使用了编码器-解码器结构，并且在解码阶段采用了在编码阶段存储的索引来上采样，从而减少了参数量。</p></li>
<li><p><strong>UNet</strong> UNet
在医学图像分割中特别受欢迎，因为它能够有效地处理小数据集。它包含一个收缩路径（下采样）和一个对称的扩展路径（上采样），并在每个解码步骤中从编码器获取相应的特征图。</p></li>
<li><p><strong>DeepLabV3+</strong> DeepLab
系列是谷歌提出的一组模型，它们使用了空洞卷积（又称扩张卷积）来捕捉多尺度信息，并引入了
ASPP
模块来进一步增强模型的多尺度感受野。DeepLabV3+增加了解码模块来恢复分割细节。</p></li>
<li><p><strong>HRNet</strong> HRNet
专注于在整个网络中保持高分辨率表示，通过并行的流来处理不同分辨率的特征图，并通过交换模块来融合这些特征。这有助于保留更多的细节信息，防止信息损失。</p></li>
<li><p><strong>通道、空间注意力机制</strong> 注意力机制（如 SENet
中的通道注意力或 CBAM
中的空间注意力）允许模型专注于重要的特征图区域，同时抑制不相关的背景噪声，提高模型的准确性。</p></li>
<li><p><strong>ViT (Vision Transformer)</strong> ViT 是一个完全基于
Transformer
架构的视觉模型，它将图像切分为固定大小的补丁，并将这些补丁作为序列传递给标准的
Transformer 编码器。这种设计使得 ViT 可以捕捉全局上下文信息。</p></li>
<li><p><strong>SwinFormer</strong> SwinFormer 是专门为视觉任务设计的
Transformer
变体，它提出了窗口注意力机制来实现局部和非局部的特征交互，同时保持计算效率。</p></li>
<li><p><strong>SegFormer</strong> SegFormer
是一个轻量级的分割模型，它结合了 Transformer 编码器（如
MiT）和简单的解码头来实现高效的多尺度特征提取和分割。</p></li>
<li><p><strong>SAM (Segment Anything Model)</strong> SAM
是一种通用的分割模型，它可以用于任何对象的分割任务而无需特定类别的训练数据。它结合了强大的预训练视觉模型和灵活的提示机制来适应不同的分割需求。</p></li>
</ol>
<p><strong>Q: FCN 如何解决不同大小输入的问题？</strong> A: FCN
通过移除最后的全连接层，代之以卷积层，允许模型接受任意大小的输入图像，并产生相同大小的分割图。</p>
<p><strong>Q: UNet 如何应对数据不足的情况？</strong> A: UNet
的设计包括了一个可以学习到更抽象特征的收缩路径，以及一个可以恢复位置信息的扩展路径，这使得它能够在较少的数据上训练而不会过拟合。</p>
<p><strong>Q: DeepLabV3+中的 ASPP 模块是什么？</strong> A: ASPP（Atrous
Spatial Pyramid Pooling）模块通过不同 rate
的空洞卷积来捕捉不同尺度的信息，从而增强模型对物体不同大小的适应能力。</p>
<p><strong>Q: ViT 是如何处理图像输入的？</strong> A: ViT
首先将输入图像划分为固定的补丁，然后将这些补丁展平成一系列向量，并添加位置嵌入，之后这些向量将被送入
Transformer 编码器进行处理。</p>
<h3 id="隧道病害检测领域常用模型">2. 隧道病害检测领域常用模型</h3>
<p>在隧道病害检测领域，尤其是针对细小裂缝的检测，需要模型具备良好的<strong>细节捕捉</strong>能力和<strong>多尺度感知</strong>能力。以下是一些常用的模型及其特点：</p>
<ol type="1">
<li><p><strong>UNet</strong></p>
<ul>
<li><strong>特点</strong>: UNet
因其在医学图像分割中的成功应用而知名，同样适用于隧道病害检测。通过跳层连接（skip
connections），UNet
能够结合高层语义信息与底层细节信息，这对于细小裂缝的检测非常重要。</li>
<li><strong>应用</strong>: 在隧道检测中，UNet
可以帮助识别裂缝等病害，特别是在数据尺度较小的情况下表现良好。</li>
</ul></li>
<li><p><strong>DeepLabV3+</strong></p>
<ul>
<li><strong>特点</strong>: 使用空洞卷积（Atrous
Convolution）来扩大感受野而不增加参数数量，并引入 ASPP
模块来捕获多尺度信息。DeepLabV3+还增加了额外的解码模块来恢复分割细节。</li>
<li><strong>应用</strong>:
对于隧道病害检测而言，DeepLabV3+的多尺度感知能力非常适合识别不同宽度和长度的裂缝。</li>
</ul></li>
<li><p><strong>SENet (Squeeze-and-Excitation Networks)</strong></p>
<ul>
<li><strong>特点</strong>:
通过引入通道注意力机制来动态地调整通道权重，从而让模型更加关注于那些对于分类任务更重要的特征。</li>
<li><strong>应用</strong>: 在隧道病害检测中，SENet
可以通过加强裂缝区域的特征表达，来提高裂缝识别的连续性。</li>
</ul></li>
<li><p><strong>HRNet (High-Resolution Net)</strong></p>
<ul>
<li><strong>特点</strong>: HRNet
在整个网络中都保持高分辨率的特征图，通过并行的多分支结构来融合不同尺度的信息，避免了在恢复分辨率过程中细节信息的丢失。</li>
<li><strong>应用</strong>:
这种特性对于隧道病害检测尤其有用，因为裂缝往往非常细小，需要保持尽可能高的分辨率来确保准确识别。</li>
</ul></li>
<li><p><strong>SegFormer</strong></p>
<ul>
<li><strong>特点</strong>: SegFormer 结合了 Transformer 编码器（如
MiT）和简单的解码头来实现高效的多尺度特征提取。相比于传统的
CNN，Transformer 能够更好地捕捉全局上下文信息。</li>
<li><strong>应用</strong>: 在隧道病害检测中，SegFormer
可以通过其强大的全局感知能力来识别那些跨越较大区域的裂缝或其他病害。</li>
</ul></li>
</ol>
<p><strong>Q: 在隧道病害检测中，为什么 UNet 的跳层连接很重要？</strong>
A: 跳层连接允许 UNet
在解码过程中重新引入编码阶段丢失的细节信息。这对于细小裂缝的检测至关重要，因为裂缝通常很窄，容易在下采样的过程中丢失。</p>
<p><strong>Q: DeepLabV3+的 ASPP 模块是如何工作的？</strong> A: ASPP
模块使用不同膨胀率的空洞卷积来捕捉不同尺度的信息，这样即使在不改变输出尺寸的情况下，也可以有效地扩大模型的感受野。</p>
<p><strong>Q: SENet 的通道注意力机制如何帮助改进病害检测？</strong> A:
通道注意力机制允许 SENet
根据每个通道的重要性动态地调整权重，这意味着模型可以更加聚焦于那些最能代表病害特征的通道，从而提高检测精度。</p>
<p><strong>Q: 在隧道病害检测中，HRNet 如何保证高分辨率特征图？</strong>
A: HRNet
通过保持多个并行的高分辨率流，并在每一层之间交换信息，从而在整个网络中维持高分辨率特征图。这种方法有助于保留更多细节信息，这对于检测细小裂缝非常重要。</p>
<p><strong>Q: SegFormer 如何在病害检测中发挥作用？</strong> A: SegFormer
利用 Transformer
的强大能力来捕捉全局依赖关系，并通过多尺度特征提取来增强对不同大小病害的检测性能。这对于识别隧道中各种类型的病害非常有效。</p>
<h3 id="如何处理小目标和大目标同时存在于一张图片中的情况">3.
如何处理小目标和大目标同时存在于一张图片中的情况？</h3>
<p>在语义分割任务中，经常会遇到图像中小目标和大目标共存的情况。为了同时有效地检测和分割这些不同尺度的目标，可以采用以下几种策略和技术：</p>
<ol type="1">
<li><p><strong>多尺度注意力金字塔 (Multi-Scale Attention
Pyramid)</strong></p>
<ul>
<li><strong>解释</strong>:
多尺度注意力金字塔通过在不同尺度上应用注意力机制来捕获不同大小的目标。这通常涉及到构建一个多尺度特征金字塔，每个尺度上的特征图都会经过注意力机制处理，以突出不同尺度的目标。</li>
<li><strong>例子</strong>: 比如 DeepLab 系列中的 ASPP（Atrous Spatial
Pyramid
Pooling）模块就是一种典型的多尺度处理方式，它通过不同膨胀率的卷积来捕捉多尺度信息。</li>
</ul></li>
<li><p><strong>混合 CNN 和 Transformer (Hybrid CNN and Transformer
Models)</strong></p>
<ul>
<li><strong>解释</strong>: 混合 CNN 和 Transformer
模型结合了传统卷积神经网络在局部特征提取方面的优势和 Transformer
在捕获长距离依赖关系方面的强大能力。这种组合可以在不同尺度上同时处理局部和全局信息。</li>
<li><strong>例子</strong>: SegFormer 就是一个实例，它使用了 Transformer
编码器（如
MiT）来提取多尺度特征，并结合了简单的解码头来进行最终的分割预测。</li>
</ul></li>
</ol>
<p><strong>Q:
为什么多尺度注意力金字塔对于处理小目标和大目标是有效的？</strong> A:
多尺度注意力金字塔通过在多个尺度上进行特征提取，使得模型可以在不同级别的细节上关注目标，从而更好地捕捉到小目标的细节和大目标的整体形状。</p>
<p><strong>Q: SegFormer 如何结合 CNN 和 Transformer 的优点？</strong> A:
SegFormer 使用了基于 Transformer
的编码器来捕捉全局上下文信息，并通过多尺度特征提取来增强对不同大小目标的理解。同时，它的解码头设计简单高效，可以很好地将多尺度特征融合在一起，以实现高质量的分割结果。</p>
<p><strong>Q: 在实际应用中，如何选择合适的多尺度处理方法？</strong> A:
选择多尺度处理方法时，应考虑应用场景的具体需求。如果场景中存在大量的小目标，则应选择那些能够较好地保留细节信息的方法；而对于包含大范围目标的场景，则可能需要更强的全局感知能力。</p>
<p><strong>Q:
除了多尺度处理，还有哪些技术可以用来改善小目标和大目标的分割效果？</strong>
A: 另外一些技术还包括使用金字塔结构（如
FPN）、增强数据（如数据扩增）、改进损失函数（如 Focal
Loss）等方法，这些都可以辅助提升模型在不同尺度目标上的表现。</p>
<h3 id="你在实际项目中使用过哪些语义分割模型遇到了什么挑战怎么解决">4.
你在实际项目中使用过哪些语义分割模型？遇到了什么挑战？怎么解决？</h3>
<p>在实际项目中，我使用过多种语义分割模型来处理隧道病害检测的任务，包括但不限于
UNet、DeepLabV3+、HRNet 和
SegFormer。在具体的应用过程中，确实遇到了一些挑战，并采取了相应的解决方案。</p>
<ol type="1">
<li><p><strong>裂缝和瓷砖剥落病害尺度不平衡</strong></p>
<ul>
<li><strong>挑战</strong>:
不同尺度的病害（如细小裂缝和大面积的瓷砖剥落）在图像中占比不同，这可能导致模型对某一类病害的检测能力较弱。</li>
<li><strong>解决方案</strong>:
<ul>
<li>使用加权交叉熵损失函数，或 Focal
Loss，来平衡不同类别之间的损失贡献，从而提高小目标的检测能力。</li>
<li>结合 Transformer 和 CNN 的优势，利用 Transformer
捕捉全局信息，同时使用 CNN
来提取局部特征，以增强模型对不同尺度病害的感知能力。</li>
</ul></li>
</ul></li>
<li><p><strong>病害图像背景复杂</strong></p>
<ul>
<li><strong>挑战</strong>:
背景复杂度高会干扰模型对病害关键特征的提取，导致误检或漏检。</li>
<li><strong>解决方案</strong>:
<ul>
<li>使用原型特征提取方法，该方法能够学习到特定类别的原型特征，有助于在复杂背景下提取出病害的关键特征。</li>
<li>利用 VQGAN 的预训练 CodeBook
来提取更为鲁棒的特征表示，增强模型的抗干扰能力。</li>
</ul></li>
</ul></li>
<li><p><strong>标注不准确</strong></p>
<ul>
<li><strong>挑战</strong>:
数据标注过程中可能会出现误差，导致模型训练偏差或泛化能力下降。</li>
<li><strong>解决方案</strong>:
<ul>
<li>采用软标签的学习方式，即在训练过程中使用带有不确定性的标签，而非硬性分配标签，这样可以减弱损失函数的强约束，使模型更具弹性。</li>
<li>使用边缘优化方法，如
PointRend，来改进分割边界，或者采用边缘再分割技术，通过形态学操作（如膨胀、腐蚀）来选取合适的像素点，提高分割的准确性。</li>
</ul></li>
</ul></li>
</ol>
<p><strong>Q: Focal Loss 是如何解决类别不平衡问题的？</strong> A: Focal
Loss
通过降低易分类样本的权重，从而更加关注难以分类的样本。这样可以减轻类别不平衡带来的影响，尤其是在小目标检测中表现得尤为明显。</p>
<p><strong>Q: Transformer 和 CNN 的结合如何提高模型性能？</strong> A:
Transformer 擅长捕捉全局依赖关系，而 CNN
则在局部特征提取方面表现出色。将两者结合起来可以互补各自的优势，使得模型既能捕捉到局部细节又能理解整体上下文。</p>
<p><strong>Q: 原型特征提取方法是如何工作的？</strong> A: ProtoPNet
通过学习一组原型特征，并在测试时计算输入图像与这些原型特征之间的相似度，从而决定分类结果。原型特征为一类像素特征的均值，且随着训练不断变化。这种方法可以增强模型对特定类别的理解，并提高其在复杂背景下的鲁棒性。</p>
<p><strong>Q: 使用 VQGAN 的预训练 CodeBook 有什么好处？</strong> A: 使用
VQGAN 的预训练 CodeBook
可以帮助模型学习到更加紧凑和有意义的离散特征表示，这对于处理复杂背景下的病害检测任务非常有帮助，可以减少背景噪声的影响。</p>
<p><strong>Q: PointRend 是如何优化分割边界的？</strong> A: PointRend
通过在分割边界处选择不确定性较高的像素点重预测，来细化分割结果。这种方法可以在保持较高效率的同时，显著提高分割边界的准确性。</p>
<p><strong>Q: 6. 除了
Self-Attention，还有哪些机制可以增强模型的表达能力？</strong> A:
（层归一化（Layer
Normalization）可以帮助模型更好地学习长距离依赖；残差连接（Residual
Connections）有助于梯度流动；使用 Transformer 中的多头注意力机制）</p>
<h2 id="损失函数">损失函数</h2>
<h3 id="介绍一下常用的损失函数">1. 介绍一下常用的损失函数</h3>
<ol type="1">
<li><p><strong>交叉熵损失（Cross Entropy Loss）</strong></p>
<ul>
<li><p><strong>解释</strong>:
交叉熵损失是最常见的分类损失函数之一，它衡量的是模型预测的概率分布与真实标签的概率分布之间的差异。</p></li>
<li><p><strong>代码示例</strong>:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"><span class="keyword">import</span> torch.nn.functional <span class="keyword">as</span> F</span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">CrossEntropyLoss</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, weight=<span class="literal">None</span>, reduction=<span class="string">'mean'</span></span>):</span><br><span class="line">        <span class="built_in">super</span>(CrossEntropyLoss, self).__init__()</span><br><span class="line">        self.weight = weight</span><br><span class="line">        self.reduction = reduction</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, pred, label</span>):</span><br><span class="line">        <span class="comment"># pred (B, C, H, W)</span></span><br><span class="line">        <span class="comment"># label (B, H, W)</span></span><br><span class="line">        <span class="keyword">if</span> <span class="built_in">len</span>(label.shape) &lt; <span class="built_in">len</span>(pred.shape):  <span class="comment"># 如果标签不是one-hot形式</span></span><br><span class="line">            label = F.one_hot(label, num_classes=pred.shape[<span class="number">1</span>])</span><br><span class="line">        pred = F.softmax(pred, dim=<span class="number">1</span>)</span><br><span class="line">        loss = -label * torch.log(pred)</span><br><span class="line">        <span class="keyword">if</span> self.weight <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">            loss = loss * self.weight.view(<span class="number">1</span>, - <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>)</span><br><span class="line">        <span class="keyword">if</span> self.reduction == <span class="string">'mean'</span>:</span><br><span class="line">            <span class="keyword">return</span> loss.mean()</span><br><span class="line">        <span class="keyword">elif</span> self.reduction == <span class="string">'sum'</span>:</span><br><span class="line">            <span class="keyword">return</span> loss.<span class="built_in">sum</span>()</span><br></pre></td></tr></table></figure></li>
</ul></li>
<li><p><strong>Focal Loss</strong></p>
<ul>
<li><p><strong>解释</strong>: Focal Loss
旨在解决类别不平衡的问题，通过在交叉熵的基础上增加一个调节因子来降低容易分类样本的权重，增加难分类样本的权重。</p></li>
<li><p><strong>代码示例</strong>:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">FocalLoss</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, weight=<span class="literal">None</span>, reduction=<span class="string">'mean'</span>, gamma=<span class="number">2</span></span>):</span><br><span class="line">        <span class="built_in">super</span>(FocalLoss, self).__init__()</span><br><span class="line">        self.weight = weight</span><br><span class="line">        self.reduction = reduction</span><br><span class="line">        self.gamma = gamma</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, pred, label</span>):</span><br><span class="line">        <span class="comment"># pred (B, C, H, W)</span></span><br><span class="line">        <span class="comment"># label (B, H, W)</span></span><br><span class="line">        <span class="keyword">if</span> <span class="built_in">len</span>(label.shape) &lt; <span class="built_in">len</span>(pred.shape):</span><br><span class="line">            label = F.one_hot(label, num_classes=pred.shape[<span class="number">1</span>])</span><br><span class="line">        pred = F.softmax(pred, dim=<span class="number">1</span>)</span><br><span class="line">        pt = torch.where(label &gt; <span class="number">0</span>, pred, <span class="number">1</span> - pred)</span><br><span class="line">        loss = -torch.<span class="built_in">pow</span>(<span class="number">1</span> - pt, self.gamma) * label * torch.log(pt)</span><br><span class="line">        <span class="keyword">if</span> self.weight <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">            loss = loss * self.weight.view(<span class="number">1</span>, -<span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>)</span><br><span class="line">        <span class="keyword">if</span> self.reduction == <span class="string">'mean'</span>:</span><br><span class="line">            <span class="keyword">return</span> loss.mean()</span><br><span class="line">        <span class="keyword">elif</span> self.reduction == <span class="string">'sum'</span>:</span><br><span class="line">            <span class="keyword">return</span> loss.<span class="built_in">sum</span>()</span><br></pre></td></tr></table></figure></li>
</ul></li>
<li><p><strong>Dice 损失</strong></p>
<ul>
<li><p><strong>解释</strong>: Dice
损失通常用于分割任务中，特别是当需要关注分割边界时。它定义为两个集合交集的两倍除以它们的并集。</p></li>
<li><p><strong>公式</strong>: ( DICE = 1 - )，其中( X )是预测结果，( Y
)是真实标签。</p></li>
<li><p><strong>代码示例</strong>:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">DiceLoss</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="built_in">super</span>(DiceLoss, self).__init__()</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, pred, label</span>):</span><br><span class="line">        smooth = <span class="number">1.</span></span><br><span class="line">        pred = F.softmax(pred, dim=<span class="number">1</span>)</span><br><span class="line">        <span class="keyword">if</span> <span class="built_in">len</span>(label.shape) &lt; <span class="built_in">len</span>(pred.shape):</span><br><span class="line">            label = F.one_hot(label, num_classes=pred.shape[<span class="number">1</span>])</span><br><span class="line">        intersection = (pred * label).<span class="built_in">sum</span>(dim=(<span class="number">2</span>, <span class="number">3</span>))</span><br><span class="line">        dice_score = (<span class="number">2.</span> * intersection + smooth) / (pred.<span class="built_in">sum</span>(dim=(<span class="number">2</span>, <span class="number">3</span>)) + label.<span class="built_in">sum</span>(dim=(<span class="number">2</span>, <span class="number">3</span>)) + smooth)</span><br><span class="line">        <span class="keyword">return</span> <span class="number">1</span> - dice_score.mean()</span><br></pre></td></tr></table></figure></li>
</ul></li>
</ol>
<p><strong>Q: 为什么交叉熵函数在不平衡数据表现不佳？</strong> A:
交叉熵函数在不平衡数据集中表现不佳的原因在于，它会过度强调多数类别的样本，而忽略少数类别的样本。在不平衡数据集中，少数类别的样本往往包含更有价值的信息，但交叉熵损失函数并没有专门对待这些样本，导致模型倾向于偏向多数类别。</p>
<p><strong>Q: Focal Loss 是如何解决类别不平衡的问题的？</strong> A:
Focal Loss 通过引入一个调节因子( (1 - p_t)^{} )，其中( p_t
)是模型对正确类别的预测概率，(
)是调节因子的指数。这个调节因子降低了容易分类样本的贡献，同时增加了难分类样本的贡献，从而使得模型更加关注那些难以分类的样本。</p>
<p><strong>Q: Class Weight 是如何帮助平衡数据的？</strong> A: Class
Weight
是通过为不同类别赋予不同的权重来实现的，这样可以调整损失函数中各类别对总损失的贡献比例。在不平衡数据集中，可以为少数类别赋予更高的权重，从而增加它们在训练过程中的重要性。</p>
<p><strong>Q: 如何平衡正负样本的比例来改善模型性能？</strong> A:
平衡正负样本比例可以通过多种方法实现，例如过采样少数类别、欠采样多数类别、使用合成样本（如
SMOTE）、调整 Class Weight
等。这些方法可以单独使用或组合使用，以达到更好的模型训练效果。此外，还可以在训练过程中动态调整样本权重，以进一步优化模型性能。</p>
<h2 id="数据集构建">数据集构建</h2>
<h3 id="数据处理的过程">1. <strong>数据处理的过程</strong></h3>
<ul>
<li><strong>数据收集</strong>: 收集来自不同来源的原始数据。</li>
<li><strong>预处理</strong>: 包括数据格式转换、缩放、裁剪等操作。</li>
<li><strong>清洗</strong>: 移除或修正错误、重复或无关的数据。</li>
<li><strong>标注</strong>:
对数据进行标记，以便机器学习模型可以学习到正确的模式。</li>
<li><strong>划分</strong>:
将数据分成训练集、验证集和测试集，用于模型训练、调参和评估。</li>
</ul>
<h3 id="数据收集">2. <strong>数据收集</strong></h3>
<ul>
<li><strong>采集车采集隧道衬砌图像</strong>:
使用专门的采集设备在隧道内拍摄衬砌图像，确保图像质量和涵盖不同类型的衬砌材料。</li>
</ul>
<h3 id="数据标注">3. <strong>数据标注</strong></h3>
<ul>
<li><strong>LabelMe</strong>: 使用 LabelMe
这样的工具进行手动标注，确保标注的准确性和一致性。</li>
</ul>
<h3 id="数据预处理">4. <strong>数据预处理</strong></h3>
<ul>
<li><strong>局部匀光算法</strong>:
对图像进行光照均匀化处理，消除光照不均带来的影响。</li>
<li><strong>数据分块</strong>:
将大图像分割成若干个小块，便于处理和标注。</li>
</ul>
<h3 id="数据清洗">5. <strong>数据清洗</strong></h3>
<ul>
<li><strong>去除重复数据</strong>:
通过哈希或特征匹配等方式识别并删除重复项。</li>
<li><strong>修复损坏文件</strong>: 检查并修复损坏的图像文件。</li>
<li><strong>清理噪声</strong>: 去除不需要的对象或背景干扰。</li>
</ul>
<h3 id="数据划分">6. <strong>数据划分</strong></h3>
<ul>
<li><strong>随机划分</strong>:
使用随机种子将数据集分成训练、验证和测试集。</li>
<li><strong>按类别划分</strong>:
确保每个子集中都有各个类别的代表性样本。</li>
</ul>
<h3 id="如何处理数据集中存在的标注错误">7.
<strong>如何处理数据集中存在的标注错误？</strong></h3>
<ul>
<li><strong>复查标注</strong>:
定期复查已有的标注，确保标注的一致性和准确性。</li>
<li><strong>使用一致性检查</strong>:
应用一致性检查工具来发现和纠正标注错误。</li>
<li><strong>引入专家审查</strong>:
让领域专家复审标注结果，尤其是对于复杂的案例。</li>
</ul>
<h3 id="在数据集构建过程中如何保证数据的多样性和代表性">8.
<strong>在数据集构建过程中，如何保证数据的多样性和代表性？</strong></h3>
<ul>
<li><strong>涵盖不同场景</strong>:
确保数据覆盖隧道的不同部分，如墙壁、天花板、地面等。</li>
<li><strong>不同条件</strong>:
包括白天、夜晚、晴天、雨天等多种天气和光照条件。</li>
<li><strong>不同材质</strong>:
包括瓷砖、混凝土、金属等多种衬砌材料。</li>
<li><strong>不同病害类型</strong>:
包括裂缝、剥落、渗水等多种病害形式。</li>
</ul>
<h3 id="有哪些工具或技术可以用来自动化数据标注过程">9.
<strong>有哪些工具或技术可以用来自动化数据标注过程？</strong></h3>
<ul>
<li><strong>Paddle</strong>: 使用 Paddle
框架提供的工具和服务进行自动化的数据标注。</li>
<li><strong>LabelStudio</strong>: LabelStudio
是一个开源的标注工具，支持多种数据类型和标注任务，可以用于简化数据标注流程。</li>
</ul>
<p><strong>Q: 局部匀光算法是如何工作的？</strong> A:
局部匀光算法通常通过调整图像的局部对比度或亮度来均匀化光照条件。计算局部块的平均灰度，上采样和高斯模糊得到亮度图，计算原图与亮度图的差异还原亮度分布。还可以使用直方图均衡化、Retinex
算法或自适应直方图均衡化（CLAHE）等方法来改善光照不均的问题。</p>
<p><strong>Q: 数据分块的好处是什么？</strong> A:
数据分块可以使处理和标注工作更加高效。通过将图像分割成小块，可以更容易地进行并行处理，同时也有助于标注人员专注于图像的特定区域。</p>
<p><strong>Q: 数据清洗中的“清理噪声”具体指的是什么？</strong> A:</p>
<p><strong>Q: 如何确保数据集划分的合理性？</strong> A:
确保数据集划分合理的方法之一是使用分层抽样，这样可以确保每个子集中都有各个类别的代表性样本。此外，可以使用交叉验证来评估模型在不同子集上的表现。</p>
<p><strong>Q: 自动化数据标注技术的局限性是什么？</strong> A:
自动化数据标注虽然提高了效率，但也可能存在误标的情况，尤其是在数据复杂或变化多端的情况下。因此，通常还需要人工复查和修正标注结果。</p>
<h2 id="训练方法研究">训练方法研究</h2>
<h3 id="mmcv">1. <strong>MMCV</strong></h3>
<ul>
<li><strong>解释</strong>: MMCV 是一个基于 PyTorch
的开放源码计算机视觉工具箱，它提供了丰富的模型实现、数据处理和训练框架。</li>
<li><strong>用途</strong>: MMCV
可以帮助快速搭建和训练模型，同时也提供了很多实用功能，如数据增强、模型融合等。</li>
</ul>
<h3 id="训练框架">2. <strong>训练框架</strong></h3>
<ul>
<li><strong>Config</strong>:
配置文件用于定义整个训练过程的参数，包括数据路径、模型结构、损失函数、优化器等。</li>
<li><strong>Data</strong>: 数据加载器负责读取、预处理和批处理数据。</li>
<li><strong>Model</strong>: 模型定义，包括网络结构和前向传播逻辑。</li>
<li><strong>Loss</strong>:
损失函数用于量化模型预测与真实标签之间的差距。</li>
<li><strong>Trainer</strong>:
训练循环逻辑，包括前向传播、反向传播、优化更新等。</li>
</ul>
<h3 id="pytorchlightning">3. <strong>PytorchLightning</strong></h3>
<ul>
<li><strong>解释</strong>: PytorchLightning 是一个用于简化 PyTorch
模型开发的库，提供了一套简洁的 API 来管理训练流程。</li>
<li><strong>优点</strong>:
减少了样板代码，方便配置和调试，支持多种训练策略如 GPU
分布式训练、混合精度训练等。</li>
</ul>
<h3 id="学习率的选择">4. <strong>学习率的选择</strong></h3>
<ul>
<li><strong>解释</strong>:
学习率决定了模型参数更新的速度。过高会导致训练不稳定甚至发散，过低则会使训练速度过慢。</li>
<li><strong>策略</strong>: 可以通过学习率查找（Learning Rate
Finder）来找到合适的初始学习率。</li>
</ul>
<h3 id="优化器的选择">5. <strong>优化器的选择</strong></h3>
<ul>
<li><strong>解释</strong>: 优化器决定了梯度下降的方式，常用的有
SGD、Adam、RMSprop 等。</li>
<li><strong>选择</strong>: 根据任务特性和模型结构选择合适的优化器，例如
Adam 适合稀疏数据，SGD 适合大规模数据集。</li>
</ul>
<h3 id="显存问题">6. <strong>显存问题</strong></h3>
<ul>
<li><strong>解释</strong>:
训练深度学习模型时，显存不足是一个常见问题。</li>
<li><strong>解决</strong>:
可以通过梯度累积、模型剪枝、分批加载数据等方式缓解显存压力。</li>
</ul>
<h3 id="如何有效地使用迁移学习来加速模型训练">7.
<strong>如何有效地使用迁移学习来加速模型训练？</strong></h3>
<ul>
<li><strong>使用预训练模型作为初始权重</strong>:
从预训练模型加载权重，可以加速收敛并提高性能。</li>
<li><strong>冻结基础层，仅训练顶层</strong>:
在训练初期可以冻结预训练层，只训练新添加的层。</li>
<li><strong>适应性调整预训练模型的输出层以匹配新的任务</strong>:
修改输出层以适应新的任务需求。</li>
</ul>
<h3 id="如何设置学习率衰减策略来避免过拟合">8.
<strong>如何设置学习率衰减策略来避免过拟合？</strong></h3>
<ul>
<li><strong>使用学习率调度器如 StepLR、Cosine Annealing 等</strong>:
通过周期性地降低学习率来促进收敛。</li>
<li><strong>观察验证集上的性能变化来动态调整学习率</strong>:
当验证集性能不再提升时，可以适当降低学习率。</li>
<li><strong>使用早停法（Early Stopping）</strong>:
在验证集性能不再改善时提前终止训练。</li>
</ul>
<h3 id="在训练过程中如何监控和诊断模型的训练状态">9.
<strong>在训练过程中，如何监控和诊断模型的训练状态？</strong></h3>
<ul>
<li><strong>使用 TensorBoard 或其他可视化工具来跟踪损失和指标</strong>:
监控训练过程中的损失曲线和指标变化。</li>
<li><strong>定期保存检查点并进行评估</strong>:
保存中间训练结果，便于后续评估或恢复训练。</li>
<li><strong>记录训练过程中的关键参数和配置</strong>:
保持训练配置的一致性和可重复性。</li>
</ul>
<h3 id="ddp-distributed-data-parallel">10. <strong>DDP (Distributed Data
Parallel)</strong></h3>
<ul>
<li><strong>解释</strong>: DDP 是 PyTorch
中的一个分布式训练工具，允许多个 GPU 并行训练同一个模型。</li>
<li><strong>使用</strong>: 可以通过设置
<code>torch.nn.parallel.DistributedDataParallel</code> 来启用 DDP
训练。</li>
</ul>
<h3 id="混合精度">11. <strong>混合精度</strong></h3>
<ul>
<li><strong>解释</strong>:
混合精度训练是指在模型训练过程中同时使用单精度（32 位）和半精度（16
位）浮点运算。</li>
<li><strong>优势</strong>:
可以减少内存占用，加快训练速度，同时保持较高的精度。</li>
</ul>
<p><strong>Q: MMDetection 有哪些主要组件？</strong> A: MMDetection
主要包括数据读取、模型定义、训练流程管理、评估工具等多个组件，支持多种模型和任务的快速开发。</p>
<p><strong>Q: 如何选择合适的优化器？</strong> A:
选择优化器时需要考虑任务的特性和数据集的特点。例如，对于稀疏数据，Adam
通常表现较好；而对于大规模数据集，SGD
由于其更快的收敛速度可能是更好的选择。</p>
<p><strong>Q: 如何解决显存不足的问题？</strong> A:
可以通过使用梯度累积、模型剪枝、数据分批加载等方法来缓解显存不足的问题。此外，还可以考虑使用更大的显卡或者分布式训练方案。</p>
<p><strong>Q: 学习率查找是如何工作的？</strong> A:
学习率查找通过逐渐增加学习率并观察损失的变化趋势来确定一个好的初始学习率。通常会在训练初期执行一次，以找到损失开始显著下降的学习率值。</p>
<p><strong>Q: 如何使用早停法来避免过拟合？</strong> A:
在训练过程中，当验证集的性能不再提高时，可以停止训练。这通常通过监测验证集上的损失或准确率来实现，当这些指标在一个预定的周期内没有改善时，就触发早停。</p>
<h2 id="低光增强及图像复原模型研究">低光增强及图像复原模型研究</h2>
<h3 id="refinxnet">1. <strong>RefinxNet</strong></h3>
<ul>
<li><strong>解释</strong>: RefinxNet 基于 Refinx
原理分离亮度和细节。</li>
<li><strong>特点</strong>: RefinxNet
通过多级特征融合，增强了对细节的捕捉能力，并且使用残差学习来减少训练难度。</li>
</ul>
<h3 id="zero-dce">2. <strong>Zero-DCE</strong></h3>
<ul>
<li><strong>解释</strong>: Zero-DCE
是一种无参考的低光图像增强方法，它不需要任何参考图像即可完成增强任务。</li>
<li><strong>特点</strong>: Zero-DCE
通过学习图像的亮度和色彩分布来调整图像，使得在没有任何参考图像的情况下也能获得较好的增强效果。</li>
</ul>
<h3 id="ddpm">3. <strong>DDPM</strong></h3>
<ul>
<li><strong>解释</strong>: DDPM（Diffusion Denoising Probabilistic
Models）是一种基于扩散过程的图像生成模型，也可以应用于图像复原任务。</li>
<li><strong>特点</strong>: DDPM
通过逐步去除噪声来恢复图像，适用于多种图像复原任务。</li>
</ul>
<h3 id="controlnet">4. <strong>ControlNet</strong></h3>
<ul>
<li><strong>解释</strong>: ControlNet
是一种可以控制生成过程的模型，可以在图像复原任务中引入额外的控制信号。</li>
<li><strong>特点</strong>: ControlNet
可以根据额外的输入（如边缘图、分割图等）来引导图像复原过程，提高复原的准确性和可控性。</li>
</ul>
<h3 id="locallightenhance">5. <strong>LocalLightEnhance</strong></h3>
<ul>
<li><strong>解释</strong>: LocalLightEnhance
是一种局部光照增强方法，通过局部调整图像的光照来提高图像质量。</li>
<li><strong>特点</strong>: LocalLightEnhance
更加注重局部细节的恢复，适用于需要精细调整光照的场景。</li>
</ul>
<h3 id="position-embedding">6. <strong>Position Embedding</strong></h3>
<ul>
<li><strong>解释</strong>: 位置嵌入（Position Embedding）是在
Transformer 模型中引入的一种机制，用于捕捉序列中元素的位置信息。</li>
<li><strong>公式</strong>: ( pos<em>{(2i)} = (x / i</em>{}) ), (
pos<em>{(2i+1)} = (x / i</em>{}) )</li>
<li><strong>类型</strong>:
包括相对位置编码和空间位置编码，前者用于捕捉序列元素之间的相对位置，后者用于标记序列中元素的空间位置。</li>
</ul>
<h3 id="hunyuandit">7. <strong>HunyuanDiT</strong></h3>
<ul>
<li><strong>解释</strong>: HunyuanDiT
是一种用于图像复原的扩散模型，结合了多尺度特征提取和注意力机制。</li>
<li><strong>特点</strong>: HunyuanDiT
通过多尺度特征提取来捕捉不同层次的细节，并通过注意力机制来加强重要特征的表达。</li>
</ul>
<h3 id="如何评估低光图像增强算法的效果">8.
<strong>如何评估低光图像增强算法的效果？</strong></h3>
<ul>
<li><strong>定量指标</strong>: 使用 LOE（Low Light
Enhancement）、PSNR（峰值信噪比）、SSIM（结构相似性指数）等指标来评估图像质量。</li>
<li><strong>主观反馈</strong>:
通过用户研究或专家评审来获取关于图像视觉效果的主观反馈。</li>
<li><strong>视觉效果</strong>:
比较增强前后图像的视觉效果，评估算法在实际应用中的表现。</li>
</ul>
<h3 id="对于极端低光条件下的图像有哪些有效的增强方法">8.
<strong>对于极端低光条件下的图像，有哪些有效的增强方法？</strong></h3>
<ul>
<li><strong>增强曝光度</strong>:
通过增加曝光度来提高图像的整体亮度。</li>
<li><strong>局部调整</strong>:
使用局部光照增强方法来恢复图像的细节。</li>
<li><strong>多帧融合</strong>:
通过融合多张连续拍摄的低光图像来减少噪声并提高图像质量。</li>
<li><strong>预处理</strong>:
在增强前进行预处理，如去除噪声或增强对比度。</li>
</ul>
<h3 id="如何利用先验知识来改进图像复原模型">9.
<strong>如何利用先验知识来改进图像复原模型？</strong></h3>
<ul>
<li><strong>物理模型</strong>:
引入物理模型来模拟成像过程，帮助模型更好地理解图像退化的原因。</li>
<li><strong>场景特定知识</strong>:
根据特定场景的特点，如光照方向、纹理特征等，对模型进行定制化调整。</li>
<li><strong>上下文信息</strong>:
利用上下文信息（如图像中的语义信息）来指导图像复原过程。</li>
<li><strong>多模态信息</strong>:
结合其他模态的数据（如深度信息、语义分割图等）来增强图像复原的效果。</li>
</ul>
<p><strong>Q: RefinxNet 中的多尺度特征融合是如何实现的？</strong> A:</p>
<p><strong>Q: Zero-DCE 的优势是什么？</strong> A: Zero-DCE
的最大优势在于它不需要参考图像即可完成增强任务，这使得它在实际应用中更加便捷。</p>
<p><strong>Q: DDPM 在图像复原中的作用是什么？</strong> A: DDPM
通过逐步去除图像中的噪声来恢复图像的原始信息，适用于多种复原任务。</p>
<p><strong>Q: ControlNet 在图像复原中的应用有哪些？</strong> A:
ControlNet
可以通过引入额外的控制信号来引导图像复原过程，从而实现更精确的复原效果。</p>
<p><strong>Q: 如何选择合适的定量指标来评估图像增强的效果？</strong> A:
选择指标时需要考虑具体的评估目标，如 LOE 侧重于低光环境下的增强效果，而
PSNR 和 SSIM 则更多关注图像的整体质量。</p>
<p><strong>Q: 在极端低光条件下，多帧融合为什么有效？</strong> A:
多帧融合可以利用多张图像中的信息来补偿单张图像中的信息不足，从而提高图像质量。</p>
<p><strong>Q: 如何利用场景特定知识来改进图像复原模型？</strong> A:
可以根据场景的特点，如光照条件、纹理特征等，对模型进行定制化调整，使其更适合特定场景的应用。</p>
<h2 id="图像处理算法">图像处理算法</h2>
<ol type="1">
<li><p><strong>低通滤波</strong></p>
<ul>
<li><strong>解释</strong>:
低通滤波器用于平滑图像，去除高频噪声，保留低频成分。</li>
<li><strong>应用</strong>: 低通滤波常用于图像去噪、模糊处理等场景。</li>
<li><strong>例子</strong>:
常见的低通滤波器有均值滤波器、高斯滤波器等。</li>
</ul></li>
<li><p><strong>边缘提取算法</strong></p>
<ul>
<li><strong>解释</strong>:
边缘提取算法用于检测图像中的边缘，即像素强度的急剧变化。</li>
<li><strong>应用</strong>:
边缘检测广泛应用于图像分割、特征识别等领域。</li>
<li><strong>例子</strong>: 常见的边缘提取算子有 Sobel 算子、Canny
边缘检测、Prewitt 算子等。</li>
</ul></li>
<li><p><strong>在不同的应用场景中，如何选择适合的滤波器？</strong></p>
<ul>
<li><strong>根据需求选择滤波器类型</strong>:
例如，去噪可以选择高斯滤波，边缘检测可以选择 Sobel 算子。</li>
<li><strong>考虑滤波器的频率响应特性</strong>:
不同滤波器对不同频率成分的处理效果不同，需根据实际需求选择合适的滤波器。</li>
</ul></li>
<li><p><strong>如何结合多种边缘提取算法来获得更好的效果？</strong></p>
<ul>
<li><strong>组合使用</strong>:
可以将多种边缘提取算法的结果进行组合，例如先用 Canny
边缘检测提取粗略边缘，然后再用 Sobel 算子细化边缘。</li>
<li><strong>多尺度处理</strong>:
在不同尺度上应用不同的边缘检测算法，然后综合结果，以提高边缘检测的鲁棒性。</li>
<li><strong>融合算法</strong>:
利用融合算法（如投票机制）来综合多种边缘检测算法的结果，以获得更准确的边缘。</li>
</ul></li>
<li><p><strong>除了传统的边缘检测算法，还有哪些现代方法？</strong></p>
<ul>
<li><strong>深度学习方法</strong>:
使用卷积神经网络（CNN）进行边缘检测，例如基于 U-Net、SegNet
等网络架构的边缘检测模型。</li>
<li><strong>超像素分割</strong>:
利用超像素分割技术，将图像分割成多个超像素区域，然后在超像素级别进行边缘检测。</li>
<li><strong>图割方法</strong>:
利用图论中的图割方法来检测图像中的边缘。</li>
<li><strong>主动轮廓模型（Snake 模型）</strong>:
通过能量最小化的方法来寻找图像中的边缘。</li>
</ul></li>
</ol>
<p><strong>Q: 高斯滤波器和平均滤波器有什么区别？</strong> A:
高斯滤波器通过高斯函数加权平均邻域内的像素值，可以有效地保留图像的主要特征，同时去除噪声。而平均滤波器则是简单地取邻域内像素的平均值，可能会导致图像模糊。</p>
<p><strong>Q: Sobel 算子的工作原理是什么？</strong> A: Sobel
算子通过两个 3x3
的卷积核分别对图像进行水平和垂直方向的卷积，从而计算图像在两个方向上的梯度，进而检测出边缘。</p>
<p><strong>Q: 在图像处理中，如何选择合适的滤波器大小？</strong> A:
滤波器的大小取决于需要处理的特征的尺度。较大的滤波器可以处理更大范围的特征，但可能会丢失细节；较小的滤波器则更适合处理局部特征。</p>
<p><strong>Q: 如何评估边缘检测算法的效果？</strong> A:
可以通过定量指标（如边缘检测的准确率、召回率等）和定性评价（如视觉效果）来评估边缘检测算法的效果。此外，还可以使用人工标注的边缘作为基准来比较算法的性能。</p>
<p><strong>Q: 深度学习在边缘检测中的优势是什么？</strong> A:
深度学习方法可以自动学习特征表示，具有很强的表达能力和适应性，可以处理复杂的边缘检测任务，并且在大数据集上有很好的表现。</p>
<h2 id="分割边缘优化">分割边缘优化</h2>
<ol type="1">
<li>PointRend</li>
<li>Multimul DataSyn</li>
<li>SegRefiner</li>
<li>如何评价分割边缘的质量？</li>
<li>在分割任务中，如何平衡速度和精度？</li>
<li>有没有尝试过将其他领域的技术应用于分割边缘优化？</li>
</ol>
<h2 id="transformer-模型">Transformer 模型</h2>
<ol type="1">
<li>Transformer 模型是如何处理长依赖关系的？</li>
<li>如何将 Transformer 应用于非自然语言处理的任务？</li>
<li>Transformer 模型的局限性是什么？
（计算成本高，尤其是在长序列上；训练数据需求量大；难以捕捉局部特征）</li>
</ol>
<h2 id="diffusion-模型">Diffusion 模型</h2>
<h3 id="diffusion-模型的训练过程">1. <strong>Diffusion
模型的训练过程</strong></h3>
<ul>
<li><strong>正向扩散过程</strong>:
在这一过程中，原始数据被逐渐添加高斯噪声，直到变成纯噪声为止。这一过程可以看作是一个数据退化的过程，目的是构造一个从清晰数据到噪声的连续分布演变。</li>
<li><strong>逆向扩散过程</strong>:
该过程尝试从噪声中逐步去除噪声，逐步重建原始数据。此过程涉及训练一个模型，使其能够预测在某一步骤中应该去除多少噪声。</li>
</ul>
<h3 id="diffusion-模型与-gan-模型的区别">2. <strong>Diffusion 模型与 GAN
模型的区别</strong></h3>
<ul>
<li><strong>显式地学习噪声分布</strong>: Diffusion
模型通过显式地学习噪声分布来生成数据，而 GAN
则是通过生成器和判别器之间的对抗训练来间接学习数据分布。</li>
</ul>
<h3 id="如何在-diffusion-模型中加入条件信息">3. <strong>如何在 Diffusion
模型中加入条件信息？</strong></h3>
<ul>
<li><strong>使用条件向量作为输入的一部分</strong>:
条件信息可以直接作为模型输入的一部分，帮助指导生成过程。</li>
<li><strong>在生成过程中加入外部信息</strong>:
例如，在文本到图像生成中，可以将文本描述嵌入到模型中，如 StableDiffusion
通过交叉注意力机制融合特征。</li>
<li><strong>ControlNet</strong>:
在某些情况下，可以通过增加一个可训练的分支并将条件信息通过跳层连接引入，类似于
ControlNet 的做法。</li>
</ul>
<h3 id="在图像生成任务中diffusion-模型相比传统方法有什么优势">4.
<strong>在图像生成任务中，Diffusion
模型相比传统方法有什么优势？</strong></h3>
<ul>
<li><strong>学习分布时任务目标更简单</strong>: Diffusion
模型通过逐步添加和去除噪声来学习数据分布，这比直接学习复杂的高维数据分布要简单得多。</li>
<li><strong>速度较慢</strong>:
尽管任务目标简单，但由于需要多次迭代去除噪声，因此生成过程相对缓慢。</li>
</ul>
<h3 id="如何评估-diffusion-模型的生成质量">5. <strong>如何评估 Diffusion
模型的生成质量？</strong></h3>
<ul>
<li><strong>结果容易模糊</strong>: Diffusion
模型由于其生成过程的本质，可能会导致生成的图像模糊不清。评估时可以使用诸如
Fréchet Inception Distance (FID) 或 Inception Score (IS)
等指标来量化生成图像的质量。</li>
<li><strong>定性评估</strong>:
除了量化指标外，还可以通过人工视觉评估来检查生成图像的真实感和多样性。</li>
</ul>
<h3 id="diffusion-推理加速">6. <strong>Diffusion 推理加速</strong></h3>
<ul>
<li><strong>并行化</strong>: 利用 GPU
的并行计算能力来加速每一步的去噪过程。</li>
<li><strong>采样步数减少</strong>: DDIM
通过减少去噪步骤的数量来加速推理，但这可能会影响生成图像的质量。</li>
<li><strong>模型剪枝</strong>:
通过对模型进行剪枝，减少不必要的计算资源消耗。</li>
<li><strong>量化</strong>:
通过量化技术减少模型大小和计算复杂度，从而加快推理速度。</li>
</ul>
<p><strong>Q: Diffusion 模型中的正向扩散过程如何实现？</strong> A:
正向扩散过程通常通过一系列高斯噪声的添加来实现，每次添加一定强度的噪声，直到原始数据完全被噪声掩盖。这一过程可以视为数据分布的退化。</p>
<p><strong>Q: Diffusion 模型中的逆向扩散过程是如何去噪的？</strong> A:
逆向扩散过程通过训练一个去噪模型来逐步去除噪声，每次迭代都试图预测并移除一定量的噪声，直至恢复出原始数据。</p>
<p><strong>Q: 控制 Diffusion 模型生成结果的方法有哪些？</strong> A:
可以通过条件输入（如类别标签、文本描述等）来控制生成结果，或者通过调整模型参数（如去噪步数、噪声强度等）来微调生成效果。</p>
<p><strong>Q: Diffusion 模型在生成图像时为什么会模糊？</strong> A: 由于
Diffusion
模型需要多次迭代去除噪声，这个过程中累积的小误差可能会导致生成的图像变得模糊不清。</p>
<p><strong>Q: 如何解决 Diffusion 模型生成结果模糊的问题？</strong> A:
可以尝试改进去噪算法，比如引入注意力机制来增强模型的局部感知能力，或者采用后处理技术（如锐化滤波）来增强生成图像的清晰度。</p>
<h2 id="多模态模型">多模态模型</h2>
<ol type="1">
<li>SAM</li>
<li>CLIP</li>
<li>BLIP</li>
<li>HunyuanDiT</li>
</ol>
<h2 id="大语言模型">大语言模型</h2>
<ol type="1">
<li>RAG 工程</li>
<li>Prompt 优化</li>
</ol>
</div></article></div><div class="card"><article class="card-content article" role="article"><div class="article-meta is-size-7 is-uppercase level is-mobile"><div class="level-left"><span class="level-item">Posted&nbsp;<time dateTime="2024-06-26T09:08:12.000Z" title="2024/6/26 下午5:08:12">2024-06-26</time></span><span class="level-item">Updated&nbsp;<time dateTime="2024-06-26T09:08:12.000Z" title="2024/6/26 下午5:08:12">2024-06-26</time></span><span class="level-item"><a class="link-muted" href="/categories/interview/">interview</a></span></div></div><p class="title is-3 is-size-4-mobile"><a class="link-muted" href="/2024/06/26/knowledge/dataset/">图像数据集 Image Dataset</a></p><div class="content"><h2 id="数据集建立">数据集建立</h2>
<!-- 你是如何收集和整理隧道图像数据的？数据集中包含哪些类型的表观病害？
数据标注过程中遇到了哪些挑战？是如何确保标注质量的？
数据集是否进行了增强？具体采用了哪些数据增强策略？ -->
<h2 id="数据增强">数据增强</h2>
<ul>
<li><p>AutoAugment：搜索最优图像处理操作组合</p></li>
<li><p>RandAugment：已有图像处理集合，确定操作次数 N 和操作幅度
M</p></li>
<li><p>分类任务</p>
<ul>
<li>Mixup：按比例混合图像和标签</li>
<li>Cutout：图像掩码</li>
<li>CutMix：图像掩码混合</li>
</ul></li>
<li><p>目标检测任务</p>
<ul>
<li>Mosaic：图像合成</li>
</ul></li>
<li><p>语义分割任务</p>
<ul>
<li>Copy-Paste：分割结果粘贴到另一张图</li>
</ul></li>
</ul>
<h2 id="数据预处理">数据预处理</h2>
<ul>
<li><p>数据增强</p>
<p>旋转、平移、缩放、翻转、随机色调(H)、饱和度(S)、明度(V)调整、等</p></li>
<li><p>数据归一化 Normalization</p>
<ul>
<li>Min-Max 归一化</li>
<li>Z-Score 归一化</li>
<li>RobustScaler 归一化</li>
</ul></li>
<li><p>One-Hot 编码</p></li>
</ul>
<h2 id="数据类别不平衡问题">数据类别不平衡问题</h2>
<ul>
<li>采样比例</li>
<li>数据生成</li>
</ul>
</div></article></div><div class="card"><article class="card-content article" role="article"><div class="article-meta is-size-7 is-uppercase level is-mobile"><div class="level-left"><span class="level-item">Posted&nbsp;<time dateTime="2024-06-26T08:57:17.000Z" title="2024/6/26 下午4:57:17">2024-06-26</time></span><span class="level-item">Updated&nbsp;<time dateTime="2024-06-26T08:57:17.000Z" title="2024/6/26 下午4:57:17">2024-06-26</time></span><span class="level-item"><a class="link-muted" href="/categories/interview/">interview</a></span></div></div><p class="title is-3 is-size-4-mobile"><a class="link-muted" href="/2024/06/26/knowledge/data-structure/">数据结构 Data Structure</a></p><div class="content"><h2 id="数组">数组</h2>
<ul>
<li>用连续内存存储数据</li>
<li>读写操作复杂度 O(1)</li>
</ul>
<h2 id="字符串">字符串</h2>
<ul>
<li>用连续内存存储字符</li>
</ul>
<h2 id="链表">链表</h2>
<ul>
<li>由指针把若干个节点连接成链状结构</li>
</ul>
<h2 id="树">树</h2>
<ul>
<li>节点之间用指针链接</li>
<li>除根节点之外每个节点只有一个父节点，根节点没有父节点</li>
<li>叶子节点没有子节点</li>
</ul>
<h3 id="二叉树">二叉树</h3>
<ul>
<li>每个节点最多只能有两个子节点</li>
</ul>
<h3 id="二叉搜索树">二叉搜索树</h3>
<ul>
<li>若其左子树不为<code>NULL</code>，则左子树上所有节点的值都<code>＜</code>根节点的值</li>
<li>若其右子树不为<code>NULL</code>，则右子树上所有节点的值都<code>＞</code>根节点的值</li>
<li>其左右子树也分别是二叉搜索树</li>
<li>查询复杂度<span class="math inline"><mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.566ex;" xmlns="http://www.w3.org/2000/svg" width="7.485ex" height="2.262ex" role="img" focusable="false" viewBox="0 -750 3308.3 1000"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="mi"><path data-c="1D442" d="M740 435Q740 320 676 213T511 42T304 -22Q207 -22 138 35T51 201Q50 209 50 244Q50 346 98 438T227 601Q351 704 476 704Q514 704 524 703Q621 689 680 617T740 435ZM637 476Q637 565 591 615T476 665Q396 665 322 605Q242 542 200 428T157 216Q157 126 200 73T314 19Q404 19 485 98T608 313Q637 408 637 476Z"></path></g><g data-mml-node="mo" transform="translate(763,0)"><path data-c="28" d="M94 250Q94 319 104 381T127 488T164 576T202 643T244 695T277 729T302 750H315H319Q333 750 333 741Q333 738 316 720T275 667T226 581T184 443T167 250T184 58T225 -81T274 -167T316 -220T333 -241Q333 -250 318 -250H315H302L274 -226Q180 -141 137 -14T94 250Z"></path></g><g data-mml-node="mi" transform="translate(1152,0)"><path data-c="1D459" d="M117 59Q117 26 142 26Q179 26 205 131Q211 151 215 152Q217 153 225 153H229Q238 153 241 153T246 151T248 144Q247 138 245 128T234 90T214 43T183 6T137 -11Q101 -11 70 11T38 85Q38 97 39 102L104 360Q167 615 167 623Q167 626 166 628T162 632T157 634T149 635T141 636T132 637T122 637Q112 637 109 637T101 638T95 641T94 647Q94 649 96 661Q101 680 107 682T179 688Q194 689 213 690T243 693T254 694Q266 694 266 686Q266 675 193 386T118 83Q118 81 118 75T117 65V59Z"></path></g><g data-mml-node="mi" transform="translate(1450,0)"><path data-c="1D45C" d="M201 -11Q126 -11 80 38T34 156Q34 221 64 279T146 380Q222 441 301 441Q333 441 341 440Q354 437 367 433T402 417T438 387T464 338T476 268Q476 161 390 75T201 -11ZM121 120Q121 70 147 48T206 26Q250 26 289 58T351 142Q360 163 374 216T388 308Q388 352 370 375Q346 405 306 405Q243 405 195 347Q158 303 140 230T121 120Z"></path></g><g data-mml-node="msub" transform="translate(1935,0)"><g data-mml-node="mi"><path data-c="1D454" d="M311 43Q296 30 267 15T206 0Q143 0 105 45T66 160Q66 265 143 353T314 442Q361 442 401 394L404 398Q406 401 409 404T418 412T431 419T447 422Q461 422 470 413T480 394Q480 379 423 152T363 -80Q345 -134 286 -169T151 -205Q10 -205 10 -137Q10 -111 28 -91T74 -71Q89 -71 102 -80T116 -111Q116 -121 114 -130T107 -144T99 -154T92 -162L90 -164H91Q101 -167 151 -167Q189 -167 211 -155Q234 -144 254 -122T282 -75Q288 -56 298 -13Q311 35 311 43ZM384 328L380 339Q377 350 375 354T369 368T359 382T346 393T328 402T306 405Q262 405 221 352Q191 313 171 233T151 117Q151 38 213 38Q269 38 323 108L331 118L384 328Z"></path></g><g data-mml-node="mi" transform="translate(510,-150) scale(0.707)"><path data-c="1D45B" d="M21 287Q22 293 24 303T36 341T56 388T89 425T135 442Q171 442 195 424T225 390T231 369Q231 367 232 367L243 378Q304 442 382 442Q436 442 469 415T503 336T465 179T427 52Q427 26 444 26Q450 26 453 27Q482 32 505 65T540 145Q542 153 560 153Q580 153 580 145Q580 144 576 130Q568 101 554 73T508 17T439 -10Q392 -10 371 17T350 73Q350 92 386 193T423 345Q423 404 379 404H374Q288 404 229 303L222 291L189 157Q156 26 151 16Q138 -11 108 -11Q95 -11 87 -5T76 7T74 17Q74 30 112 180T152 343Q153 348 153 366Q153 405 129 405Q91 405 66 305Q60 285 60 284Q58 278 41 278H27Q21 284 21 287Z"></path></g></g><g data-mml-node="mo" transform="translate(2919.3,0)"><path data-c="29" d="M60 749L64 750Q69 750 74 750H86L114 726Q208 641 251 514T294 250Q294 182 284 119T261 12T224 -76T186 -143T145 -194T113 -227T90 -246Q87 -249 86 -250H74Q66 -250 63 -250T58 -247T55 -238Q56 -237 66 -225Q221 -64 221 250T66 725Q56 737 55 738Q55 746 60 749Z"></path></g></g></g></svg></mjx-container></span></li>
</ul>
<h3 id="堆">堆</h3>
<ul>
<li>最大堆：根节点的值最大</li>
<li>最小堆：根节点的值最小</li>
</ul>
<h2 id="栈">栈</h2>
<ul>
<li>先进后出</li>
</ul>
<h2 id="队列">队列</h2>
<ul>
<li>先进先出</li>
</ul>
<h2 id="图">图</h2>
<h2 id="多线程">多线程</h2>
<h2 id="异常处理">异常处理</h2>
<h2 id="算法">算法</h2>
<h3 id="排序">排序</h3>
<table>
<thead>
<tr class="header">
<th>算法</th>
<th>时间复杂度</th>
<th>稳定性</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>冒泡排序</td>
<td><span class="math inline"><mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.566ex;" xmlns="http://www.w3.org/2000/svg" width="5.832ex" height="2.452ex" role="img" focusable="false" viewBox="0 -833.9 2577.6 1083.9"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="mi"><path data-c="1D442" d="M740 435Q740 320 676 213T511 42T304 -22Q207 -22 138 35T51 201Q50 209 50 244Q50 346 98 438T227 601Q351 704 476 704Q514 704 524 703Q621 689 680 617T740 435ZM637 476Q637 565 591 615T476 665Q396 665 322 605Q242 542 200 428T157 216Q157 126 200 73T314 19Q404 19 485 98T608 313Q637 408 637 476Z"></path></g><g data-mml-node="mo" transform="translate(763,0)"><path data-c="28" d="M94 250Q94 319 104 381T127 488T164 576T202 643T244 695T277 729T302 750H315H319Q333 750 333 741Q333 738 316 720T275 667T226 581T184 443T167 250T184 58T225 -81T274 -167T316 -220T333 -241Q333 -250 318 -250H315H302L274 -226Q180 -141 137 -14T94 250Z"></path></g><g data-mml-node="msup" transform="translate(1152,0)"><g data-mml-node="mi"><path data-c="1D45B" d="M21 287Q22 293 24 303T36 341T56 388T89 425T135 442Q171 442 195 424T225 390T231 369Q231 367 232 367L243 378Q304 442 382 442Q436 442 469 415T503 336T465 179T427 52Q427 26 444 26Q450 26 453 27Q482 32 505 65T540 145Q542 153 560 153Q580 153 580 145Q580 144 576 130Q568 101 554 73T508 17T439 -10Q392 -10 371 17T350 73Q350 92 386 193T423 345Q423 404 379 404H374Q288 404 229 303L222 291L189 157Q156 26 151 16Q138 -11 108 -11Q95 -11 87 -5T76 7T74 17Q74 30 112 180T152 343Q153 348 153 366Q153 405 129 405Q91 405 66 305Q60 285 60 284Q58 278 41 278H27Q21 284 21 287Z"></path></g><g data-mml-node="mn" transform="translate(633,363) scale(0.707)"><path data-c="32" d="M109 429Q82 429 66 447T50 491Q50 562 103 614T235 666Q326 666 387 610T449 465Q449 422 429 383T381 315T301 241Q265 210 201 149L142 93L218 92Q375 92 385 97Q392 99 409 186V189H449V186Q448 183 436 95T421 3V0H50V19V31Q50 38 56 46T86 81Q115 113 136 137Q145 147 170 174T204 211T233 244T261 278T284 308T305 340T320 369T333 401T340 431T343 464Q343 527 309 573T212 619Q179 619 154 602T119 569T109 550Q109 549 114 549Q132 549 151 535T170 489Q170 464 154 447T109 429Z"></path></g></g><g data-mml-node="mo" transform="translate(2188.6,0)"><path data-c="29" d="M60 749L64 750Q69 750 74 750H86L114 726Q208 641 251 514T294 250Q294 182 284 119T261 12T224 -76T186 -143T145 -194T113 -227T90 -246Q87 -249 86 -250H74Q66 -250 63 -250T58 -247T55 -238Q56 -237 66 -225Q221 -64 221 250T66 725Q56 737 55 738Q55 746 60 749Z"></path></g></g></g></svg></mjx-container></span></td>
<td>√</td>
</tr>
<tr class="even">
<td>选择排序</td>
<td><span class="math inline"><mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.566ex;" xmlns="http://www.w3.org/2000/svg" width="5.832ex" height="2.452ex" role="img" focusable="false" viewBox="0 -833.9 2577.6 1083.9"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="mi"><path data-c="1D442" d="M740 435Q740 320 676 213T511 42T304 -22Q207 -22 138 35T51 201Q50 209 50 244Q50 346 98 438T227 601Q351 704 476 704Q514 704 524 703Q621 689 680 617T740 435ZM637 476Q637 565 591 615T476 665Q396 665 322 605Q242 542 200 428T157 216Q157 126 200 73T314 19Q404 19 485 98T608 313Q637 408 637 476Z"></path></g><g data-mml-node="mo" transform="translate(763,0)"><path data-c="28" d="M94 250Q94 319 104 381T127 488T164 576T202 643T244 695T277 729T302 750H315H319Q333 750 333 741Q333 738 316 720T275 667T226 581T184 443T167 250T184 58T225 -81T274 -167T316 -220T333 -241Q333 -250 318 -250H315H302L274 -226Q180 -141 137 -14T94 250Z"></path></g><g data-mml-node="msup" transform="translate(1152,0)"><g data-mml-node="mi"><path data-c="1D45B" d="M21 287Q22 293 24 303T36 341T56 388T89 425T135 442Q171 442 195 424T225 390T231 369Q231 367 232 367L243 378Q304 442 382 442Q436 442 469 415T503 336T465 179T427 52Q427 26 444 26Q450 26 453 27Q482 32 505 65T540 145Q542 153 560 153Q580 153 580 145Q580 144 576 130Q568 101 554 73T508 17T439 -10Q392 -10 371 17T350 73Q350 92 386 193T423 345Q423 404 379 404H374Q288 404 229 303L222 291L189 157Q156 26 151 16Q138 -11 108 -11Q95 -11 87 -5T76 7T74 17Q74 30 112 180T152 343Q153 348 153 366Q153 405 129 405Q91 405 66 305Q60 285 60 284Q58 278 41 278H27Q21 284 21 287Z"></path></g><g data-mml-node="mn" transform="translate(633,363) scale(0.707)"><path data-c="32" d="M109 429Q82 429 66 447T50 491Q50 562 103 614T235 666Q326 666 387 610T449 465Q449 422 429 383T381 315T301 241Q265 210 201 149L142 93L218 92Q375 92 385 97Q392 99 409 186V189H449V186Q448 183 436 95T421 3V0H50V19V31Q50 38 56 46T86 81Q115 113 136 137Q145 147 170 174T204 211T233 244T261 278T284 308T305 340T320 369T333 401T340 431T343 464Q343 527 309 573T212 619Q179 619 154 602T119 569T109 550Q109 549 114 549Q132 549 151 535T170 489Q170 464 154 447T109 429Z"></path></g></g><g data-mml-node="mo" transform="translate(2188.6,0)"><path data-c="29" d="M60 749L64 750Q69 750 74 750H86L114 726Q208 641 251 514T294 250Q294 182 284 119T261 12T224 -76T186 -143T145 -194T113 -227T90 -246Q87 -249 86 -250H74Q66 -250 63 -250T58 -247T55 -238Q56 -237 66 -225Q221 -64 221 250T66 725Q56 737 55 738Q55 746 60 749Z"></path></g></g></g></svg></mjx-container></span></td>
<td>×</td>
</tr>
<tr class="odd">
<td>归并排序</td>
<td><span class="math inline"><mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.566ex;" xmlns="http://www.w3.org/2000/svg" width="8.842ex" height="2.262ex" role="img" focusable="false" viewBox="0 -750 3908.3 1000"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="mi"><path data-c="1D442" d="M740 435Q740 320 676 213T511 42T304 -22Q207 -22 138 35T51 201Q50 209 50 244Q50 346 98 438T227 601Q351 704 476 704Q514 704 524 703Q621 689 680 617T740 435ZM637 476Q637 565 591 615T476 665Q396 665 322 605Q242 542 200 428T157 216Q157 126 200 73T314 19Q404 19 485 98T608 313Q637 408 637 476Z"></path></g><g data-mml-node="mo" transform="translate(763,0)"><path data-c="28" d="M94 250Q94 319 104 381T127 488T164 576T202 643T244 695T277 729T302 750H315H319Q333 750 333 741Q333 738 316 720T275 667T226 581T184 443T167 250T184 58T225 -81T274 -167T316 -220T333 -241Q333 -250 318 -250H315H302L274 -226Q180 -141 137 -14T94 250Z"></path></g><g data-mml-node="mi" transform="translate(1152,0)"><path data-c="1D45B" d="M21 287Q22 293 24 303T36 341T56 388T89 425T135 442Q171 442 195 424T225 390T231 369Q231 367 232 367L243 378Q304 442 382 442Q436 442 469 415T503 336T465 179T427 52Q427 26 444 26Q450 26 453 27Q482 32 505 65T540 145Q542 153 560 153Q580 153 580 145Q580 144 576 130Q568 101 554 73T508 17T439 -10Q392 -10 371 17T350 73Q350 92 386 193T423 345Q423 404 379 404H374Q288 404 229 303L222 291L189 157Q156 26 151 16Q138 -11 108 -11Q95 -11 87 -5T76 7T74 17Q74 30 112 180T152 343Q153 348 153 366Q153 405 129 405Q91 405 66 305Q60 285 60 284Q58 278 41 278H27Q21 284 21 287Z"></path></g><g data-mml-node="mi" transform="translate(1752,0)"><path data-c="1D459" d="M117 59Q117 26 142 26Q179 26 205 131Q211 151 215 152Q217 153 225 153H229Q238 153 241 153T246 151T248 144Q247 138 245 128T234 90T214 43T183 6T137 -11Q101 -11 70 11T38 85Q38 97 39 102L104 360Q167 615 167 623Q167 626 166 628T162 632T157 634T149 635T141 636T132 637T122 637Q112 637 109 637T101 638T95 641T94 647Q94 649 96 661Q101 680 107 682T179 688Q194 689 213 690T243 693T254 694Q266 694 266 686Q266 675 193 386T118 83Q118 81 118 75T117 65V59Z"></path></g><g data-mml-node="mi" transform="translate(2050,0)"><path data-c="1D45C" d="M201 -11Q126 -11 80 38T34 156Q34 221 64 279T146 380Q222 441 301 441Q333 441 341 440Q354 437 367 433T402 417T438 387T464 338T476 268Q476 161 390 75T201 -11ZM121 120Q121 70 147 48T206 26Q250 26 289 58T351 142Q360 163 374 216T388 308Q388 352 370 375Q346 405 306 405Q243 405 195 347Q158 303 140 230T121 120Z"></path></g><g data-mml-node="msub" transform="translate(2535,0)"><g data-mml-node="mi"><path data-c="1D454" d="M311 43Q296 30 267 15T206 0Q143 0 105 45T66 160Q66 265 143 353T314 442Q361 442 401 394L404 398Q406 401 409 404T418 412T431 419T447 422Q461 422 470 413T480 394Q480 379 423 152T363 -80Q345 -134 286 -169T151 -205Q10 -205 10 -137Q10 -111 28 -91T74 -71Q89 -71 102 -80T116 -111Q116 -121 114 -130T107 -144T99 -154T92 -162L90 -164H91Q101 -167 151 -167Q189 -167 211 -155Q234 -144 254 -122T282 -75Q288 -56 298 -13Q311 35 311 43ZM384 328L380 339Q377 350 375 354T369 368T359 382T346 393T328 402T306 405Q262 405 221 352Q191 313 171 233T151 117Q151 38 213 38Q269 38 323 108L331 118L384 328Z"></path></g><g data-mml-node="mi" transform="translate(510,-150) scale(0.707)"><path data-c="1D45B" d="M21 287Q22 293 24 303T36 341T56 388T89 425T135 442Q171 442 195 424T225 390T231 369Q231 367 232 367L243 378Q304 442 382 442Q436 442 469 415T503 336T465 179T427 52Q427 26 444 26Q450 26 453 27Q482 32 505 65T540 145Q542 153 560 153Q580 153 580 145Q580 144 576 130Q568 101 554 73T508 17T439 -10Q392 -10 371 17T350 73Q350 92 386 193T423 345Q423 404 379 404H374Q288 404 229 303L222 291L189 157Q156 26 151 16Q138 -11 108 -11Q95 -11 87 -5T76 7T74 17Q74 30 112 180T152 343Q153 348 153 366Q153 405 129 405Q91 405 66 305Q60 285 60 284Q58 278 41 278H27Q21 284 21 287Z"></path></g></g><g data-mml-node="mo" transform="translate(3519.3,0)"><path data-c="29" d="M60 749L64 750Q69 750 74 750H86L114 726Q208 641 251 514T294 250Q294 182 284 119T261 12T224 -76T186 -143T145 -194T113 -227T90 -246Q87 -249 86 -250H74Q66 -250 63 -250T58 -247T55 -238Q56 -237 66 -225Q221 -64 221 250T66 725Q56 737 55 738Q55 746 60 749Z"></path></g></g></g></svg></mjx-container></span></td>
<td>√</td>
</tr>
<tr class="even">
<td>快速排序</td>
<td><span class="math inline"><mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.566ex;" xmlns="http://www.w3.org/2000/svg" width="8.842ex" height="2.262ex" role="img" focusable="false" viewBox="0 -750 3908.3 1000"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="mi"><path data-c="1D442" d="M740 435Q740 320 676 213T511 42T304 -22Q207 -22 138 35T51 201Q50 209 50 244Q50 346 98 438T227 601Q351 704 476 704Q514 704 524 703Q621 689 680 617T740 435ZM637 476Q637 565 591 615T476 665Q396 665 322 605Q242 542 200 428T157 216Q157 126 200 73T314 19Q404 19 485 98T608 313Q637 408 637 476Z"></path></g><g data-mml-node="mo" transform="translate(763,0)"><path data-c="28" d="M94 250Q94 319 104 381T127 488T164 576T202 643T244 695T277 729T302 750H315H319Q333 750 333 741Q333 738 316 720T275 667T226 581T184 443T167 250T184 58T225 -81T274 -167T316 -220T333 -241Q333 -250 318 -250H315H302L274 -226Q180 -141 137 -14T94 250Z"></path></g><g data-mml-node="mi" transform="translate(1152,0)"><path data-c="1D45B" d="M21 287Q22 293 24 303T36 341T56 388T89 425T135 442Q171 442 195 424T225 390T231 369Q231 367 232 367L243 378Q304 442 382 442Q436 442 469 415T503 336T465 179T427 52Q427 26 444 26Q450 26 453 27Q482 32 505 65T540 145Q542 153 560 153Q580 153 580 145Q580 144 576 130Q568 101 554 73T508 17T439 -10Q392 -10 371 17T350 73Q350 92 386 193T423 345Q423 404 379 404H374Q288 404 229 303L222 291L189 157Q156 26 151 16Q138 -11 108 -11Q95 -11 87 -5T76 7T74 17Q74 30 112 180T152 343Q153 348 153 366Q153 405 129 405Q91 405 66 305Q60 285 60 284Q58 278 41 278H27Q21 284 21 287Z"></path></g><g data-mml-node="mi" transform="translate(1752,0)"><path data-c="1D459" d="M117 59Q117 26 142 26Q179 26 205 131Q211 151 215 152Q217 153 225 153H229Q238 153 241 153T246 151T248 144Q247 138 245 128T234 90T214 43T183 6T137 -11Q101 -11 70 11T38 85Q38 97 39 102L104 360Q167 615 167 623Q167 626 166 628T162 632T157 634T149 635T141 636T132 637T122 637Q112 637 109 637T101 638T95 641T94 647Q94 649 96 661Q101 680 107 682T179 688Q194 689 213 690T243 693T254 694Q266 694 266 686Q266 675 193 386T118 83Q118 81 118 75T117 65V59Z"></path></g><g data-mml-node="mi" transform="translate(2050,0)"><path data-c="1D45C" d="M201 -11Q126 -11 80 38T34 156Q34 221 64 279T146 380Q222 441 301 441Q333 441 341 440Q354 437 367 433T402 417T438 387T464 338T476 268Q476 161 390 75T201 -11ZM121 120Q121 70 147 48T206 26Q250 26 289 58T351 142Q360 163 374 216T388 308Q388 352 370 375Q346 405 306 405Q243 405 195 347Q158 303 140 230T121 120Z"></path></g><g data-mml-node="msub" transform="translate(2535,0)"><g data-mml-node="mi"><path data-c="1D454" d="M311 43Q296 30 267 15T206 0Q143 0 105 45T66 160Q66 265 143 353T314 442Q361 442 401 394L404 398Q406 401 409 404T418 412T431 419T447 422Q461 422 470 413T480 394Q480 379 423 152T363 -80Q345 -134 286 -169T151 -205Q10 -205 10 -137Q10 -111 28 -91T74 -71Q89 -71 102 -80T116 -111Q116 -121 114 -130T107 -144T99 -154T92 -162L90 -164H91Q101 -167 151 -167Q189 -167 211 -155Q234 -144 254 -122T282 -75Q288 -56 298 -13Q311 35 311 43ZM384 328L380 339Q377 350 375 354T369 368T359 382T346 393T328 402T306 405Q262 405 221 352Q191 313 171 233T151 117Q151 38 213 38Q269 38 323 108L331 118L384 328Z"></path></g><g data-mml-node="mi" transform="translate(510,-150) scale(0.707)"><path data-c="1D45B" d="M21 287Q22 293 24 303T36 341T56 388T89 425T135 442Q171 442 195 424T225 390T231 369Q231 367 232 367L243 378Q304 442 382 442Q436 442 469 415T503 336T465 179T427 52Q427 26 444 26Q450 26 453 27Q482 32 505 65T540 145Q542 153 560 153Q580 153 580 145Q580 144 576 130Q568 101 554 73T508 17T439 -10Q392 -10 371 17T350 73Q350 92 386 193T423 345Q423 404 379 404H374Q288 404 229 303L222 291L189 157Q156 26 151 16Q138 -11 108 -11Q95 -11 87 -5T76 7T74 17Q74 30 112 180T152 343Q153 348 153 366Q153 405 129 405Q91 405 66 305Q60 285 60 284Q58 278 41 278H27Q21 284 21 287Z"></path></g></g><g data-mml-node="mo" transform="translate(3519.3,0)"><path data-c="29" d="M60 749L64 750Q69 750 74 750H86L114 726Q208 641 251 514T294 250Q294 182 284 119T261 12T224 -76T186 -143T145 -194T113 -227T90 -246Q87 -249 86 -250H74Q66 -250 63 -250T58 -247T55 -238Q56 -237 66 -225Q221 -64 221 250T66 725Q56 737 55 738Q55 746 60 749Z"></path></g></g></g></svg></mjx-container></span></td>
<td>×</td>
</tr>
<tr class="odd">
<td>堆排序</td>
<td><span class="math inline"><mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.566ex;" xmlns="http://www.w3.org/2000/svg" width="8.842ex" height="2.262ex" role="img" focusable="false" viewBox="0 -750 3908.3 1000"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="mi"><path data-c="1D442" d="M740 435Q740 320 676 213T511 42T304 -22Q207 -22 138 35T51 201Q50 209 50 244Q50 346 98 438T227 601Q351 704 476 704Q514 704 524 703Q621 689 680 617T740 435ZM637 476Q637 565 591 615T476 665Q396 665 322 605Q242 542 200 428T157 216Q157 126 200 73T314 19Q404 19 485 98T608 313Q637 408 637 476Z"></path></g><g data-mml-node="mo" transform="translate(763,0)"><path data-c="28" d="M94 250Q94 319 104 381T127 488T164 576T202 643T244 695T277 729T302 750H315H319Q333 750 333 741Q333 738 316 720T275 667T226 581T184 443T167 250T184 58T225 -81T274 -167T316 -220T333 -241Q333 -250 318 -250H315H302L274 -226Q180 -141 137 -14T94 250Z"></path></g><g data-mml-node="mi" transform="translate(1152,0)"><path data-c="1D45B" d="M21 287Q22 293 24 303T36 341T56 388T89 425T135 442Q171 442 195 424T225 390T231 369Q231 367 232 367L243 378Q304 442 382 442Q436 442 469 415T503 336T465 179T427 52Q427 26 444 26Q450 26 453 27Q482 32 505 65T540 145Q542 153 560 153Q580 153 580 145Q580 144 576 130Q568 101 554 73T508 17T439 -10Q392 -10 371 17T350 73Q350 92 386 193T423 345Q423 404 379 404H374Q288 404 229 303L222 291L189 157Q156 26 151 16Q138 -11 108 -11Q95 -11 87 -5T76 7T74 17Q74 30 112 180T152 343Q153 348 153 366Q153 405 129 405Q91 405 66 305Q60 285 60 284Q58 278 41 278H27Q21 284 21 287Z"></path></g><g data-mml-node="mi" transform="translate(1752,0)"><path data-c="1D459" d="M117 59Q117 26 142 26Q179 26 205 131Q211 151 215 152Q217 153 225 153H229Q238 153 241 153T246 151T248 144Q247 138 245 128T234 90T214 43T183 6T137 -11Q101 -11 70 11T38 85Q38 97 39 102L104 360Q167 615 167 623Q167 626 166 628T162 632T157 634T149 635T141 636T132 637T122 637Q112 637 109 637T101 638T95 641T94 647Q94 649 96 661Q101 680 107 682T179 688Q194 689 213 690T243 693T254 694Q266 694 266 686Q266 675 193 386T118 83Q118 81 118 75T117 65V59Z"></path></g><g data-mml-node="mi" transform="translate(2050,0)"><path data-c="1D45C" d="M201 -11Q126 -11 80 38T34 156Q34 221 64 279T146 380Q222 441 301 441Q333 441 341 440Q354 437 367 433T402 417T438 387T464 338T476 268Q476 161 390 75T201 -11ZM121 120Q121 70 147 48T206 26Q250 26 289 58T351 142Q360 163 374 216T388 308Q388 352 370 375Q346 405 306 405Q243 405 195 347Q158 303 140 230T121 120Z"></path></g><g data-mml-node="msub" transform="translate(2535,0)"><g data-mml-node="mi"><path data-c="1D454" d="M311 43Q296 30 267 15T206 0Q143 0 105 45T66 160Q66 265 143 353T314 442Q361 442 401 394L404 398Q406 401 409 404T418 412T431 419T447 422Q461 422 470 413T480 394Q480 379 423 152T363 -80Q345 -134 286 -169T151 -205Q10 -205 10 -137Q10 -111 28 -91T74 -71Q89 -71 102 -80T116 -111Q116 -121 114 -130T107 -144T99 -154T92 -162L90 -164H91Q101 -167 151 -167Q189 -167 211 -155Q234 -144 254 -122T282 -75Q288 -56 298 -13Q311 35 311 43ZM384 328L380 339Q377 350 375 354T369 368T359 382T346 393T328 402T306 405Q262 405 221 352Q191 313 171 233T151 117Q151 38 213 38Q269 38 323 108L331 118L384 328Z"></path></g><g data-mml-node="mi" transform="translate(510,-150) scale(0.707)"><path data-c="1D45B" d="M21 287Q22 293 24 303T36 341T56 388T89 425T135 442Q171 442 195 424T225 390T231 369Q231 367 232 367L243 378Q304 442 382 442Q436 442 469 415T503 336T465 179T427 52Q427 26 444 26Q450 26 453 27Q482 32 505 65T540 145Q542 153 560 153Q580 153 580 145Q580 144 576 130Q568 101 554 73T508 17T439 -10Q392 -10 371 17T350 73Q350 92 386 193T423 345Q423 404 379 404H374Q288 404 229 303L222 291L189 157Q156 26 151 16Q138 -11 108 -11Q95 -11 87 -5T76 7T74 17Q74 30 112 180T152 343Q153 348 153 366Q153 405 129 405Q91 405 66 305Q60 285 60 284Q58 278 41 278H27Q21 284 21 287Z"></path></g></g><g data-mml-node="mo" transform="translate(3519.3,0)"><path data-c="29" d="M60 749L64 750Q69 750 74 750H86L114 726Q208 641 251 514T294 250Q294 182 284 119T261 12T224 -76T186 -143T145 -194T113 -227T90 -246Q87 -249 86 -250H74Q66 -250 63 -250T58 -247T55 -238Q56 -237 66 -225Q221 -64 221 250T66 725Q56 737 55 738Q55 746 60 749Z"></path></g></g></g></svg></mjx-container></span></td>
<td>×</td>
</tr>
</tbody>
</table>
<h3 id="查找">查找</h3>
<ul>
<li>二分查找</li>
</ul>
<h3 id="动态规划">动态规划</h3>
</div></article></div><div class="card"><article class="card-content article" role="article"><div class="article-meta is-size-7 is-uppercase level is-mobile"><div class="level-left"><span class="level-item">Posted&nbsp;<time dateTime="2024-06-26T08:03:04.000Z" title="2024/6/26 下午4:03:04">2024-06-26</time></span><span class="level-item">Updated&nbsp;<time dateTime="2024-06-26T08:03:04.000Z" title="2024/6/26 下午4:03:04">2024-06-26</time></span><span class="level-item"><a class="link-muted" href="/categories/interview/">interview</a></span></div></div><p class="title is-3 is-size-4-mobile"><a class="link-muted" href="/2024/06/26/knowledge/computer-vision/">计算机视觉 Computer Vision</a></p><div class="content"><blockquote>
<p><a target="_blank" rel="noopener" href="https://github.com/WeThinkIn/Interview-for-Algorithm-Engineer">【三年面试五年模拟】算法工程师的求职面试秘籍</a>
&gt; <a target="_blank" rel="noopener" href="https://cloud.tencent.com/developer/article/1558355">从
ReLU 到 GELU，一文概览神经网络的激活函数</a></p>
</blockquote>
<ul>
<li><p>https://github.com/DWCTOD/interview/blob/master/detail/%E4%BD%9C%E4%B8%9A%E5%B8%AE%20%E8%A7%86%E8%A7%89%E7%AE%97%E6%B3%95%E5%B7%A5%E7%A8%8B%E5%B8%88%20%E9%9D%A2%E7%BB%8F%EF%BC%882020%E5%B1%8A%EF%BC%89.md</p></li>
<li><p>https://github.com/GYee/CV_interviews_Q-A</p></li>
</ul>
<h2 id="图像处理与计算机视觉基础">图像处理与计算机视觉基础</h2>
<h3 id="基本概念">基本概念</h3>
<h3 id="原理">原理</h3>
<h3 id="图像预处理">图像预处理</h3>
<h3 id="特征提取">特征提取</h3>
<h3 id="对象检测">对象检测</h3>
<h3 id="图像分类">图像分类</h3>
<h3 id="图像分割">图像分割</h3>
<h3 id="opencv-库">OpenCV 库</h3>
<h4 id="读写图像">读写图像</h4>
<h4 id="图像滤波">图像滤波</h4>
<h4 id="几何变换">几何变换</h4>
<h4 id="特征检测与描述">特征检测与描述</h4>
<h2 id="深度学习基础">深度学习基础</h2>
<h3 id="梯度下降">梯度下降</h3>
<h3 id="滑动平均">滑动平均</h3>
<h3 id="模型微调fine-tuning">模型微调（Fine-tuning）</h3>
<h3 id="基础模块">基础模块</h3>
<h4 id="池化层-pooling-layer">池化层 Pooling Layer</h4>
<h4 id="归一化层-normalization-layer">归一化层 Normalization Layer</h4>
<ul>
<li>BN，Batch Normalization</li>
<li>IN，Instance Normalization</li>
<li>LN，Layer Normalization</li>
<li>GN，Group Normalization</li>
</ul>
<p>BN 是怎么做，作用是什么</p>
<h4 id="激活层">激活层</h4>
<ol type="1">
<li><p>常见激活函数</p>
<ul>
<li>Sigmoid</li>
<li>Tanh</li>
<li>ReLU</li>
<li>LeakyReLU</li>
<li>SoftPlus</li>
<li>ELU</li>
<li>SELU，自归一化</li>
<li>Swish，类 Sigmoid 作为开关</li>
<li>GELU</li>
<li>GLU</li>
</ul></li>
<li><p>特性</p>
<ul>
<li>梯度消失 存在偏导过小</li>
<li>梯度爆炸 偏导累乘过大</li>
<li>梯度裁剪</li>
<li>输出均值为 0 能避免每次权重只能往单一反向变化</li>
<li>ReLU 计算复杂度低</li>
<li>ReLU
的负半轴为输出值增加稀疏性，减少计算量，但同时会让一些神经元不能更新</li>
<li>SoftPlus，ReLU 的平滑</li>
</ul></li>
</ol>
<h4 id="全连接层-linear">全连接层 Linear</h4>
<h4 id="嵌入层-embedding">嵌入层 Embedding</h4>
<h4 id="卷积层-convolution">卷积层 Convolution</h4>
<ul>
<li><p>特征</p>
<ul>
<li>局部感知、权值共享、平移不变、多核</li>
</ul></li>
<li><p>1×1 卷积</p>
<ul>
<li>特征增强</li>
<li>特征融合</li>
<li>改变通道数</li>
<li>分类</li>
</ul></li>
<li><p>空洞卷积</p></li>
<li><p>分组卷积</p></li>
</ul>
<h4 id="转置卷积层-transpose-convolution">转置卷积层 Transpose
Convolution</h4>
<h3 id="优化模块">优化模块</h3>
<h4 id="残差结构-residual-connection">残差结构 Residual Connection</h4>
<p>将输入与层输出相加</p>
<ul>
<li><p>优势</p>
<ul>
<li>缓解梯度消失，增加网络深度</li>
<li>保留信息，特征重用</li>
</ul></li>
</ul>
<h4 id="空间金字塔池化spatial-pyramid-poolingspp">空间金字塔池化（Spatial
Pyramid Pooling，SPP）</h4>
<h4 id="空洞空间金字塔池化atrous-spatial-pyramid-poolingaspp">空洞空间金字塔池化（Atrous
Spatial Pyramid Pooling，ASPP）</h4>
<h4 id="hdc">HDC</h4>
<h4 id="可变形卷积-deformable-convolution">可变形卷积 Deformable
Convolution</h4>
<h4 id="可分离卷积-separable-convolution">可分离卷积 Separable
Convolution</h4>
<h4 id="transformer">Transformer</h4>
<ul>
<li><p>模块结构</p>
<ul>
<li>多头自注意力机制（Multi-Head Self-Attention Mechanism）</li>
<li>前馈神经网络（Feed-Forward Neural Network）</li>
<li>层归一化（Layer Normalization）</li>
<li>残差连接（Residual Connections）</li>
</ul></li>
<li><p>Self-Attention</p>
<p><span class="math display"><mjx-container class="MathJax" jax="SVG" display="true"><svg style="vertical-align: -2.308ex;" xmlns="http://www.w3.org/2000/svg" width="40.947ex" height="5.741ex" role="img" focusable="false" viewBox="0 -1517.7 18098.7 2537.7"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="mi"><path data-c="1D434" d="M208 74Q208 50 254 46Q272 46 272 35Q272 34 270 22Q267 8 264 4T251 0Q249 0 239 0T205 1T141 2Q70 2 50 0H42Q35 7 35 11Q37 38 48 46H62Q132 49 164 96Q170 102 345 401T523 704Q530 716 547 716H555H572Q578 707 578 706L606 383Q634 60 636 57Q641 46 701 46Q726 46 726 36Q726 34 723 22Q720 7 718 4T704 0Q701 0 690 0T651 1T578 2Q484 2 455 0H443Q437 6 437 9T439 27Q443 40 445 43L449 46H469Q523 49 533 63L521 213H283L249 155Q208 86 208 74ZM516 260Q516 271 504 416T490 562L463 519Q447 492 400 412L310 260L413 259Q516 259 516 260Z"></path></g><g data-mml-node="mi" transform="translate(750,0)"><path data-c="1D461" d="M26 385Q19 392 19 395Q19 399 22 411T27 425Q29 430 36 430T87 431H140L159 511Q162 522 166 540T173 566T179 586T187 603T197 615T211 624T229 626Q247 625 254 615T261 596Q261 589 252 549T232 470L222 433Q222 431 272 431H323Q330 424 330 420Q330 398 317 385H210L174 240Q135 80 135 68Q135 26 162 26Q197 26 230 60T283 144Q285 150 288 151T303 153H307Q322 153 322 145Q322 142 319 133Q314 117 301 95T267 48T216 6T155 -11Q125 -11 98 4T59 56Q57 64 57 83V101L92 241Q127 382 128 383Q128 385 77 385H26Z"></path></g><g data-mml-node="mi" transform="translate(1111,0)"><path data-c="1D461" d="M26 385Q19 392 19 395Q19 399 22 411T27 425Q29 430 36 430T87 431H140L159 511Q162 522 166 540T173 566T179 586T187 603T197 615T211 624T229 626Q247 625 254 615T261 596Q261 589 252 549T232 470L222 433Q222 431 272 431H323Q330 424 330 420Q330 398 317 385H210L174 240Q135 80 135 68Q135 26 162 26Q197 26 230 60T283 144Q285 150 288 151T303 153H307Q322 153 322 145Q322 142 319 133Q314 117 301 95T267 48T216 6T155 -11Q125 -11 98 4T59 56Q57 64 57 83V101L92 241Q127 382 128 383Q128 385 77 385H26Z"></path></g><g data-mml-node="mi" transform="translate(1472,0)"><path data-c="1D452" d="M39 168Q39 225 58 272T107 350T174 402T244 433T307 442H310Q355 442 388 420T421 355Q421 265 310 237Q261 224 176 223Q139 223 138 221Q138 219 132 186T125 128Q125 81 146 54T209 26T302 45T394 111Q403 121 406 121Q410 121 419 112T429 98T420 82T390 55T344 24T281 -1T205 -11Q126 -11 83 42T39 168ZM373 353Q367 405 305 405Q272 405 244 391T199 357T170 316T154 280T149 261Q149 260 169 260Q282 260 327 284T373 353Z"></path></g><g data-mml-node="mi" transform="translate(1938,0)"><path data-c="1D45B" d="M21 287Q22 293 24 303T36 341T56 388T89 425T135 442Q171 442 195 424T225 390T231 369Q231 367 232 367L243 378Q304 442 382 442Q436 442 469 415T503 336T465 179T427 52Q427 26 444 26Q450 26 453 27Q482 32 505 65T540 145Q542 153 560 153Q580 153 580 145Q580 144 576 130Q568 101 554 73T508 17T439 -10Q392 -10 371 17T350 73Q350 92 386 193T423 345Q423 404 379 404H374Q288 404 229 303L222 291L189 157Q156 26 151 16Q138 -11 108 -11Q95 -11 87 -5T76 7T74 17Q74 30 112 180T152 343Q153 348 153 366Q153 405 129 405Q91 405 66 305Q60 285 60 284Q58 278 41 278H27Q21 284 21 287Z"></path></g><g data-mml-node="mi" transform="translate(2538,0)"><path data-c="1D461" d="M26 385Q19 392 19 395Q19 399 22 411T27 425Q29 430 36 430T87 431H140L159 511Q162 522 166 540T173 566T179 586T187 603T197 615T211 624T229 626Q247 625 254 615T261 596Q261 589 252 549T232 470L222 433Q222 431 272 431H323Q330 424 330 420Q330 398 317 385H210L174 240Q135 80 135 68Q135 26 162 26Q197 26 230 60T283 144Q285 150 288 151T303 153H307Q322 153 322 145Q322 142 319 133Q314 117 301 95T267 48T216 6T155 -11Q125 -11 98 4T59 56Q57 64 57 83V101L92 241Q127 382 128 383Q128 385 77 385H26Z"></path></g><g data-mml-node="mi" transform="translate(2899,0)"><path data-c="1D456" d="M184 600Q184 624 203 642T247 661Q265 661 277 649T290 619Q290 596 270 577T226 557Q211 557 198 567T184 600ZM21 287Q21 295 30 318T54 369T98 420T158 442Q197 442 223 419T250 357Q250 340 236 301T196 196T154 83Q149 61 149 51Q149 26 166 26Q175 26 185 29T208 43T235 78T260 137Q263 149 265 151T282 153Q302 153 302 143Q302 135 293 112T268 61T223 11T161 -11Q129 -11 102 10T74 74Q74 91 79 106T122 220Q160 321 166 341T173 380Q173 404 156 404H154Q124 404 99 371T61 287Q60 286 59 284T58 281T56 279T53 278T49 278T41 278H27Q21 284 21 287Z"></path></g><g data-mml-node="mi" transform="translate(3244,0)"><path data-c="1D45C" d="M201 -11Q126 -11 80 38T34 156Q34 221 64 279T146 380Q222 441 301 441Q333 441 341 440Q354 437 367 433T402 417T438 387T464 338T476 268Q476 161 390 75T201 -11ZM121 120Q121 70 147 48T206 26Q250 26 289 58T351 142Q360 163 374 216T388 308Q388 352 370 375Q346 405 306 405Q243 405 195 347Q158 303 140 230T121 120Z"></path></g><g data-mml-node="mi" transform="translate(3729,0)"><path data-c="1D45B" d="M21 287Q22 293 24 303T36 341T56 388T89 425T135 442Q171 442 195 424T225 390T231 369Q231 367 232 367L243 378Q304 442 382 442Q436 442 469 415T503 336T465 179T427 52Q427 26 444 26Q450 26 453 27Q482 32 505 65T540 145Q542 153 560 153Q580 153 580 145Q580 144 576 130Q568 101 554 73T508 17T439 -10Q392 -10 371 17T350 73Q350 92 386 193T423 345Q423 404 379 404H374Q288 404 229 303L222 291L189 157Q156 26 151 16Q138 -11 108 -11Q95 -11 87 -5T76 7T74 17Q74 30 112 180T152 343Q153 348 153 366Q153 405 129 405Q91 405 66 305Q60 285 60 284Q58 278 41 278H27Q21 284 21 287Z"></path></g><g data-mml-node="mo" transform="translate(4329,0)"><path data-c="28" d="M94 250Q94 319 104 381T127 488T164 576T202 643T244 695T277 729T302 750H315H319Q333 750 333 741Q333 738 316 720T275 667T226 581T184 443T167 250T184 58T225 -81T274 -167T316 -220T333 -241Q333 -250 318 -250H315H302L274 -226Q180 -141 137 -14T94 250Z"></path></g><g data-mml-node="mi" transform="translate(4718,0)"><path data-c="1D444" d="M399 -80Q399 -47 400 -30T402 -11V-7L387 -11Q341 -22 303 -22Q208 -22 138 35T51 201Q50 209 50 244Q50 346 98 438T227 601Q351 704 476 704Q514 704 524 703Q621 689 680 617T740 435Q740 255 592 107Q529 47 461 16L444 8V3Q444 2 449 -24T470 -66T516 -82Q551 -82 583 -60T625 -3Q631 11 638 11Q647 11 649 2Q649 -6 639 -34T611 -100T557 -165T481 -194Q399 -194 399 -87V-80ZM636 468Q636 523 621 564T580 625T530 655T477 665Q429 665 379 640Q277 591 215 464T153 216Q153 110 207 59Q231 38 236 38V46Q236 86 269 120T347 155Q372 155 390 144T417 114T429 82T435 55L448 64Q512 108 557 185T619 334T636 468ZM314 18Q362 18 404 39L403 49Q399 104 366 115Q354 117 347 117Q344 117 341 117T337 118Q317 118 296 98T274 52Q274 18 314 18Z"></path></g><g data-mml-node="mo" transform="translate(5509,0)"><path data-c="2C" d="M78 35T78 60T94 103T137 121Q165 121 187 96T210 8Q210 -27 201 -60T180 -117T154 -158T130 -185T117 -194Q113 -194 104 -185T95 -172Q95 -168 106 -156T131 -126T157 -76T173 -3V9L172 8Q170 7 167 6T161 3T152 1T140 0Q113 0 96 17Z"></path></g><g data-mml-node="mi" transform="translate(5953.7,0)"><path data-c="1D43E" d="M285 628Q285 635 228 637Q205 637 198 638T191 647Q191 649 193 661Q199 681 203 682Q205 683 214 683H219Q260 681 355 681Q389 681 418 681T463 682T483 682Q500 682 500 674Q500 669 497 660Q496 658 496 654T495 648T493 644T490 641T486 639T479 638T470 637T456 637Q416 636 405 634T387 623L306 305Q307 305 490 449T678 597Q692 611 692 620Q692 635 667 637Q651 637 651 648Q651 650 654 662T659 677Q662 682 676 682Q680 682 711 681T791 680Q814 680 839 681T869 682Q889 682 889 672Q889 650 881 642Q878 637 862 637Q787 632 726 586Q710 576 656 534T556 455L509 418L518 396Q527 374 546 329T581 244Q656 67 661 61Q663 59 666 57Q680 47 717 46H738Q744 38 744 37T741 19Q737 6 731 0H720Q680 3 625 3Q503 3 488 0H478Q472 6 472 9T474 27Q478 40 480 43T491 46H494Q544 46 544 71Q544 75 517 141T485 216L427 354L359 301L291 248L268 155Q245 63 245 58Q245 51 253 49T303 46H334Q340 37 340 35Q340 19 333 5Q328 0 317 0Q314 0 280 1T180 2Q118 2 85 2T49 1Q31 1 31 11Q31 13 34 25Q38 41 42 43T65 46Q92 46 125 49Q139 52 144 61Q147 65 216 339T285 628Z"></path></g><g data-mml-node="mo" transform="translate(6842.7,0)"><path data-c="2C" d="M78 35T78 60T94 103T137 121Q165 121 187 96T210 8Q210 -27 201 -60T180 -117T154 -158T130 -185T117 -194Q113 -194 104 -185T95 -172Q95 -168 106 -156T131 -126T157 -76T173 -3V9L172 8Q170 7 167 6T161 3T152 1T140 0Q113 0 96 17Z"></path></g><g data-mml-node="mi" transform="translate(7287.3,0)"><path data-c="1D449" d="M52 648Q52 670 65 683H76Q118 680 181 680Q299 680 320 683H330Q336 677 336 674T334 656Q329 641 325 637H304Q282 635 274 635Q245 630 242 620Q242 618 271 369T301 118L374 235Q447 352 520 471T595 594Q599 601 599 609Q599 633 555 637Q537 637 537 648Q537 649 539 661Q542 675 545 679T558 683Q560 683 570 683T604 682T668 681Q737 681 755 683H762Q769 676 769 672Q769 655 760 640Q757 637 743 637Q730 636 719 635T698 630T682 623T670 615T660 608T652 599T645 592L452 282Q272 -9 266 -16Q263 -18 259 -21L241 -22H234Q216 -22 216 -15Q213 -9 177 305Q139 623 138 626Q133 637 76 637H59Q52 642 52 648Z"></path></g><g data-mml-node="mo" transform="translate(8056.3,0)"><path data-c="29" d="M60 749L64 750Q69 750 74 750H86L114 726Q208 641 251 514T294 250Q294 182 284 119T261 12T224 -76T186 -143T145 -194T113 -227T90 -246Q87 -249 86 -250H74Q66 -250 63 -250T58 -247T55 -238Q56 -237 66 -225Q221 -64 221 250T66 725Q56 737 55 738Q55 746 60 749Z"></path></g><g data-mml-node="mo" transform="translate(8723.1,0)"><path data-c="3D" d="M56 347Q56 360 70 367H707Q722 359 722 347Q722 336 708 328L390 327H72Q56 332 56 347ZM56 153Q56 168 72 173H708Q722 163 722 153Q722 140 707 133H70Q56 140 56 153Z"></path></g><g data-mml-node="mi" transform="translate(9778.9,0)"><path data-c="1D446" d="M308 24Q367 24 416 76T466 197Q466 260 414 284Q308 311 278 321T236 341Q176 383 176 462Q176 523 208 573T273 648Q302 673 343 688T407 704H418H425Q521 704 564 640Q565 640 577 653T603 682T623 704Q624 704 627 704T632 705Q645 705 645 698T617 577T585 459T569 456Q549 456 549 465Q549 471 550 475Q550 478 551 494T553 520Q553 554 544 579T526 616T501 641Q465 662 419 662Q362 662 313 616T263 510Q263 480 278 458T319 427Q323 425 389 408T456 390Q490 379 522 342T554 242Q554 216 546 186Q541 164 528 137T492 78T426 18T332 -20Q320 -22 298 -22Q199 -22 144 33L134 44L106 13Q83 -14 78 -18T65 -22Q52 -22 52 -14Q52 -11 110 221Q112 227 130 227H143Q149 221 149 216Q149 214 148 207T144 186T142 153Q144 114 160 87T203 47T255 29T308 24Z"></path></g><g data-mml-node="mi" transform="translate(10423.9,0)"><path data-c="1D45C" d="M201 -11Q126 -11 80 38T34 156Q34 221 64 279T146 380Q222 441 301 441Q333 441 341 440Q354 437 367 433T402 417T438 387T464 338T476 268Q476 161 390 75T201 -11ZM121 120Q121 70 147 48T206 26Q250 26 289 58T351 142Q360 163 374 216T388 308Q388 352 370 375Q346 405 306 405Q243 405 195 347Q158 303 140 230T121 120Z"></path></g><g data-mml-node="mi" transform="translate(10908.9,0)"><path data-c="1D453" d="M118 -162Q120 -162 124 -164T135 -167T147 -168Q160 -168 171 -155T187 -126Q197 -99 221 27T267 267T289 382V385H242Q195 385 192 387Q188 390 188 397L195 425Q197 430 203 430T250 431Q298 431 298 432Q298 434 307 482T319 540Q356 705 465 705Q502 703 526 683T550 630Q550 594 529 578T487 561Q443 561 443 603Q443 622 454 636T478 657L487 662Q471 668 457 668Q445 668 434 658T419 630Q412 601 403 552T387 469T380 433Q380 431 435 431Q480 431 487 430T498 424Q499 420 496 407T491 391Q489 386 482 386T428 385H372L349 263Q301 15 282 -47Q255 -132 212 -173Q175 -205 139 -205Q107 -205 81 -186T55 -132Q55 -95 76 -78T118 -61Q162 -61 162 -103Q162 -122 151 -136T127 -157L118 -162Z"></path></g><g data-mml-node="mi" transform="translate(11458.9,0)"><path data-c="1D461" d="M26 385Q19 392 19 395Q19 399 22 411T27 425Q29 430 36 430T87 431H140L159 511Q162 522 166 540T173 566T179 586T187 603T197 615T211 624T229 626Q247 625 254 615T261 596Q261 589 252 549T232 470L222 433Q222 431 272 431H323Q330 424 330 420Q330 398 317 385H210L174 240Q135 80 135 68Q135 26 162 26Q197 26 230 60T283 144Q285 150 288 151T303 153H307Q322 153 322 145Q322 142 319 133Q314 117 301 95T267 48T216 6T155 -11Q125 -11 98 4T59 56Q57 64 57 83V101L92 241Q127 382 128 383Q128 385 77 385H26Z"></path></g><g data-mml-node="mi" transform="translate(11819.9,0)"><path data-c="1D45A" d="M21 287Q22 293 24 303T36 341T56 388T88 425T132 442T175 435T205 417T221 395T229 376L231 369Q231 367 232 367L243 378Q303 442 384 442Q401 442 415 440T441 433T460 423T475 411T485 398T493 385T497 373T500 364T502 357L510 367Q573 442 659 442Q713 442 746 415T780 336Q780 285 742 178T704 50Q705 36 709 31T724 26Q752 26 776 56T815 138Q818 149 821 151T837 153Q857 153 857 145Q857 144 853 130Q845 101 831 73T785 17T716 -10Q669 -10 648 17T627 73Q627 92 663 193T700 345Q700 404 656 404H651Q565 404 506 303L499 291L466 157Q433 26 428 16Q415 -11 385 -11Q372 -11 364 -4T353 8T350 18Q350 29 384 161L420 307Q423 322 423 345Q423 404 379 404H374Q288 404 229 303L222 291L189 157Q156 26 151 16Q138 -11 108 -11Q95 -11 87 -5T76 7T74 17Q74 30 112 181Q151 335 151 342Q154 357 154 369Q154 405 129 405Q107 405 92 377T69 316T57 280Q55 278 41 278H27Q21 284 21 287Z"></path></g><g data-mml-node="mi" transform="translate(12697.9,0)"><path data-c="1D44E" d="M33 157Q33 258 109 349T280 441Q331 441 370 392Q386 422 416 422Q429 422 439 414T449 394Q449 381 412 234T374 68Q374 43 381 35T402 26Q411 27 422 35Q443 55 463 131Q469 151 473 152Q475 153 483 153H487Q506 153 506 144Q506 138 501 117T481 63T449 13Q436 0 417 -8Q409 -10 393 -10Q359 -10 336 5T306 36L300 51Q299 52 296 50Q294 48 292 46Q233 -10 172 -10Q117 -10 75 30T33 157ZM351 328Q351 334 346 350T323 385T277 405Q242 405 210 374T160 293Q131 214 119 129Q119 126 119 118T118 106Q118 61 136 44T179 26Q217 26 254 59T298 110Q300 114 325 217T351 328Z"></path></g><g data-mml-node="mi" transform="translate(13226.9,0)"><path data-c="1D465" d="M52 289Q59 331 106 386T222 442Q257 442 286 424T329 379Q371 442 430 442Q467 442 494 420T522 361Q522 332 508 314T481 292T458 288Q439 288 427 299T415 328Q415 374 465 391Q454 404 425 404Q412 404 406 402Q368 386 350 336Q290 115 290 78Q290 50 306 38T341 26Q378 26 414 59T463 140Q466 150 469 151T485 153H489Q504 153 504 145Q504 144 502 134Q486 77 440 33T333 -11Q263 -11 227 52Q186 -10 133 -10H127Q78 -10 57 16T35 71Q35 103 54 123T99 143Q142 143 142 101Q142 81 130 66T107 46T94 41L91 40Q91 39 97 36T113 29T132 26Q168 26 194 71Q203 87 217 139T245 247T261 313Q266 340 266 352Q266 380 251 392T217 404Q177 404 142 372T93 290Q91 281 88 280T72 278H58Q52 284 52 289Z"></path></g><g data-mml-node="mo" transform="translate(13798.9,0)"><path data-c="28" d="M94 250Q94 319 104 381T127 488T164 576T202 643T244 695T277 729T302 750H315H319Q333 750 333 741Q333 738 316 720T275 667T226 581T184 443T167 250T184 58T225 -81T274 -167T316 -220T333 -241Q333 -250 318 -250H315H302L274 -226Q180 -141 137 -14T94 250Z"></path></g><g data-mml-node="mfrac" transform="translate(14187.9,0)"><g data-mml-node="mrow" transform="translate(220,676)"><g data-mml-node="mi"><path data-c="1D444" d="M399 -80Q399 -47 400 -30T402 -11V-7L387 -11Q341 -22 303 -22Q208 -22 138 35T51 201Q50 209 50 244Q50 346 98 438T227 601Q351 704 476 704Q514 704 524 703Q621 689 680 617T740 435Q740 255 592 107Q529 47 461 16L444 8V3Q444 2 449 -24T470 -66T516 -82Q551 -82 583 -60T625 -3Q631 11 638 11Q647 11 649 2Q649 -6 639 -34T611 -100T557 -165T481 -194Q399 -194 399 -87V-80ZM636 468Q636 523 621 564T580 625T530 655T477 665Q429 665 379 640Q277 591 215 464T153 216Q153 110 207 59Q231 38 236 38V46Q236 86 269 120T347 155Q372 155 390 144T417 114T429 82T435 55L448 64Q512 108 557 185T619 334T636 468ZM314 18Q362 18 404 39L403 49Q399 104 366 115Q354 117 347 117Q344 117 341 117T337 118Q317 118 296 98T274 52Q274 18 314 18Z"></path></g><g data-mml-node="msup" transform="translate(791,0)"><g data-mml-node="mi"><path data-c="1D43E" d="M285 628Q285 635 228 637Q205 637 198 638T191 647Q191 649 193 661Q199 681 203 682Q205 683 214 683H219Q260 681 355 681Q389 681 418 681T463 682T483 682Q500 682 500 674Q500 669 497 660Q496 658 496 654T495 648T493 644T490 641T486 639T479 638T470 637T456 637Q416 636 405 634T387 623L306 305Q307 305 490 449T678 597Q692 611 692 620Q692 635 667 637Q651 637 651 648Q651 650 654 662T659 677Q662 682 676 682Q680 682 711 681T791 680Q814 680 839 681T869 682Q889 682 889 672Q889 650 881 642Q878 637 862 637Q787 632 726 586Q710 576 656 534T556 455L509 418L518 396Q527 374 546 329T581 244Q656 67 661 61Q663 59 666 57Q680 47 717 46H738Q744 38 744 37T741 19Q737 6 731 0H720Q680 3 625 3Q503 3 488 0H478Q472 6 472 9T474 27Q478 40 480 43T491 46H494Q544 46 544 71Q544 75 517 141T485 216L427 354L359 301L291 248L268 155Q245 63 245 58Q245 51 253 49T303 46H334Q340 37 340 35Q340 19 333 5Q328 0 317 0Q314 0 280 1T180 2Q118 2 85 2T49 1Q31 1 31 11Q31 13 34 25Q38 41 42 43T65 46Q92 46 125 49Q139 52 144 61Q147 65 216 339T285 628Z"></path></g><g data-mml-node="mi" transform="translate(974,363) scale(0.707)"><path data-c="1D447" d="M40 437Q21 437 21 445Q21 450 37 501T71 602L88 651Q93 669 101 677H569H659Q691 677 697 676T704 667Q704 661 687 553T668 444Q668 437 649 437Q640 437 637 437T631 442L629 445Q629 451 635 490T641 551Q641 586 628 604T573 629Q568 630 515 631Q469 631 457 630T439 622Q438 621 368 343T298 60Q298 48 386 46Q418 46 427 45T436 36Q436 31 433 22Q429 4 424 1L422 0Q419 0 415 0Q410 0 363 1T228 2Q99 2 64 0H49Q43 6 43 9T45 27Q49 40 55 46H83H94Q174 46 189 55Q190 56 191 56Q196 59 201 76T241 233Q258 301 269 344Q339 619 339 625Q339 630 310 630H279Q212 630 191 624Q146 614 121 583T67 467Q60 445 57 441T43 437H40Z"></path></g></g></g><g data-mml-node="msqrt" transform="translate(464.2,-855.6)"><g transform="translate(853,0)"><g data-mml-node="msub"><g data-mml-node="mi"><path data-c="1D451" d="M366 683Q367 683 438 688T511 694Q523 694 523 686Q523 679 450 384T375 83T374 68Q374 26 402 26Q411 27 422 35Q443 55 463 131Q469 151 473 152Q475 153 483 153H487H491Q506 153 506 145Q506 140 503 129Q490 79 473 48T445 8T417 -8Q409 -10 393 -10Q359 -10 336 5T306 36L300 51Q299 52 296 50Q294 48 292 46Q233 -10 172 -10Q117 -10 75 30T33 157Q33 205 53 255T101 341Q148 398 195 420T280 442Q336 442 364 400Q369 394 369 396Q370 400 396 505T424 616Q424 629 417 632T378 637H357Q351 643 351 645T353 664Q358 683 366 683ZM352 326Q329 405 277 405Q242 405 210 374T160 293Q131 214 119 129Q119 126 119 118T118 106Q118 61 136 44T179 26Q233 26 290 98L298 109L352 326Z"></path></g><g data-mml-node="mi" transform="translate(553,-150) scale(0.707)"><path data-c="1D458" d="M121 647Q121 657 125 670T137 683Q138 683 209 688T282 694Q294 694 294 686Q294 679 244 477Q194 279 194 272Q213 282 223 291Q247 309 292 354T362 415Q402 442 438 442Q468 442 485 423T503 369Q503 344 496 327T477 302T456 291T438 288Q418 288 406 299T394 328Q394 353 410 369T442 390L458 393Q446 405 434 405H430Q398 402 367 380T294 316T228 255Q230 254 243 252T267 246T293 238T320 224T342 206T359 180T365 147Q365 130 360 106T354 66Q354 26 381 26Q429 26 459 145Q461 153 479 153H483Q499 153 499 144Q499 139 496 130Q455 -11 378 -11Q333 -11 305 15T277 90Q277 108 280 121T283 145Q283 167 269 183T234 206T200 217T182 220H180Q168 178 159 139T145 81T136 44T129 20T122 7T111 -2Q98 -11 83 -11Q66 -11 57 -1T48 16Q48 26 85 176T158 471L195 616Q196 629 188 632T149 637H144Q134 637 131 637T124 640T121 647Z"></path></g></g></g><g data-mml-node="mo" transform="translate(0,35.6)"><path data-c="221A" d="M95 178Q89 178 81 186T72 200T103 230T169 280T207 309Q209 311 212 311H213Q219 311 227 294T281 177Q300 134 312 108L397 -77Q398 -77 501 136T707 565T814 786Q820 800 834 800Q841 800 846 794T853 782V776L620 293L385 -193Q381 -200 366 -200Q357 -200 354 -197Q352 -195 256 15L160 225L144 214Q129 202 113 190T95 178Z"></path></g><rect width="971.4" height="60" x="853" y="775.6"></rect></g><rect width="2512.8" height="60" x="120" y="220"></rect></g><g data-mml-node="mo" transform="translate(16940.7,0)"><path data-c="29" d="M60 749L64 750Q69 750 74 750H86L114 726Q208 641 251 514T294 250Q294 182 284 119T261 12T224 -76T186 -143T145 -194T113 -227T90 -246Q87 -249 86 -250H74Q66 -250 63 -250T58 -247T55 -238Q56 -237 66 -225Q221 -64 221 250T66 725Q56 737 55 738Q55 746 60 749Z"></path></g><g data-mml-node="mi" transform="translate(17329.7,0)"><path data-c="1D449" d="M52 648Q52 670 65 683H76Q118 680 181 680Q299 680 320 683H330Q336 677 336 674T334 656Q329 641 325 637H304Q282 635 274 635Q245 630 242 620Q242 618 271 369T301 118L374 235Q447 352 520 471T595 594Q599 601 599 609Q599 633 555 637Q537 637 537 648Q537 649 539 661Q542 675 545 679T558 683Q560 683 570 683T604 682T668 681Q737 681 755 683H762Q769 676 769 672Q769 655 760 640Q757 637 743 637Q730 636 719 635T698 630T682 623T670 615T660 608T652 599T645 592L452 282Q272 -9 266 -16Q263 -18 259 -21L241 -22H234Q216 -22 216 -15Q213 -9 177 305Q139 623 138 626Q133 637 76 637H59Q52 642 52 648Z"></path></g></g></g></svg></mjx-container></span></p></li>
<li><p>除以<span class="math inline"><mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.372ex;" xmlns="http://www.w3.org/2000/svg" width="4.128ex" height="2.398ex" role="img" focusable="false" viewBox="0 -895.6 1824.4 1060"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="msqrt"><g transform="translate(853,0)"><g data-mml-node="msub"><g data-mml-node="mi"><path data-c="1D451" d="M366 683Q367 683 438 688T511 694Q523 694 523 686Q523 679 450 384T375 83T374 68Q374 26 402 26Q411 27 422 35Q443 55 463 131Q469 151 473 152Q475 153 483 153H487H491Q506 153 506 145Q506 140 503 129Q490 79 473 48T445 8T417 -8Q409 -10 393 -10Q359 -10 336 5T306 36L300 51Q299 52 296 50Q294 48 292 46Q233 -10 172 -10Q117 -10 75 30T33 157Q33 205 53 255T101 341Q148 398 195 420T280 442Q336 442 364 400Q369 394 369 396Q370 400 396 505T424 616Q424 629 417 632T378 637H357Q351 643 351 645T353 664Q358 683 366 683ZM352 326Q329 405 277 405Q242 405 210 374T160 293Q131 214 119 129Q119 126 119 118T118 106Q118 61 136 44T179 26Q233 26 290 98L298 109L352 326Z"></path></g><g data-mml-node="mi" transform="translate(553,-150) scale(0.707)"><path data-c="1D458" d="M121 647Q121 657 125 670T137 683Q138 683 209 688T282 694Q294 694 294 686Q294 679 244 477Q194 279 194 272Q213 282 223 291Q247 309 292 354T362 415Q402 442 438 442Q468 442 485 423T503 369Q503 344 496 327T477 302T456 291T438 288Q418 288 406 299T394 328Q394 353 410 369T442 390L458 393Q446 405 434 405H430Q398 402 367 380T294 316T228 255Q230 254 243 252T267 246T293 238T320 224T342 206T359 180T365 147Q365 130 360 106T354 66Q354 26 381 26Q429 26 459 145Q461 153 479 153H483Q499 153 499 144Q499 139 496 130Q455 -11 378 -11Q333 -11 305 15T277 90Q277 108 280 121T283 145Q283 167 269 183T234 206T200 217T182 220H180Q168 178 159 139T145 81T136 44T129 20T122 7T111 -2Q98 -11 83 -11Q66 -11 57 -1T48 16Q48 26 85 176T158 471L195 616Q196 629 188 632T149 637H144Q134 637 131 637T124 640T121 647Z"></path></g></g></g><g data-mml-node="mo" transform="translate(0,35.6)"><path data-c="221A" d="M95 178Q89 178 81 186T72 200T103 230T169 280T207 309Q209 311 212 311H213Q219 311 227 294T281 177Q300 134 312 108L397 -77Q398 -77 501 136T707 565T814 786Q820 800 834 800Q841 800 846 794T853 782V776L620 293L385 -193Q381 -200 366 -200Q357 -200 354 -197Q352 -195 256 15L160 225L144 214Q129 202 113 190T95 178Z"></path></g><rect width="971.4" height="60" x="853" y="775.6"></rect></g></g></g></svg></mjx-container></span>的原因</p>
<ul>
<li>矩阵计算导致的元素值整体偏大，从而引发梯度消失</li>
<li>计算后的数据平方差为<span class="math inline"><mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.357ex;" xmlns="http://www.w3.org/2000/svg" width="2.198ex" height="1.927ex" role="img" focusable="false" viewBox="0 -694 971.4 851.8"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="msub"><g data-mml-node="mi"><path data-c="1D451" d="M366 683Q367 683 438 688T511 694Q523 694 523 686Q523 679 450 384T375 83T374 68Q374 26 402 26Q411 27 422 35Q443 55 463 131Q469 151 473 152Q475 153 483 153H487H491Q506 153 506 145Q506 140 503 129Q490 79 473 48T445 8T417 -8Q409 -10 393 -10Q359 -10 336 5T306 36L300 51Q299 52 296 50Q294 48 292 46Q233 -10 172 -10Q117 -10 75 30T33 157Q33 205 53 255T101 341Q148 398 195 420T280 442Q336 442 364 400Q369 394 369 396Q370 400 396 505T424 616Q424 629 417 632T378 637H357Q351 643 351 645T353 664Q358 683 366 683ZM352 326Q329 405 277 405Q242 405 210 374T160 293Q131 214 119 129Q119 126 119 118T118 106Q118 61 136 44T179 26Q233 26 290 98L298 109L352 326Z"></path></g><g data-mml-node="mi" transform="translate(553,-150) scale(0.707)"><path data-c="1D458" d="M121 647Q121 657 125 670T137 683Q138 683 209 688T282 694Q294 694 294 686Q294 679 244 477Q194 279 194 272Q213 282 223 291Q247 309 292 354T362 415Q402 442 438 442Q468 442 485 423T503 369Q503 344 496 327T477 302T456 291T438 288Q418 288 406 299T394 328Q394 353 410 369T442 390L458 393Q446 405 434 405H430Q398 402 367 380T294 316T228 255Q230 254 243 252T267 246T293 238T320 224T342 206T359 180T365 147Q365 130 360 106T354 66Q354 26 381 26Q429 26 459 145Q461 153 479 153H483Q499 153 499 144Q499 139 496 130Q455 -11 378 -11Q333 -11 305 15T277 90Q277 108 280 121T283 145Q283 167 269 183T234 206T200 217T182 220H180Q168 178 159 139T145 81T136 44T129 20T122 7T111 -2Q98 -11 83 -11Q66 -11 57 -1T48 16Q48 26 85 176T158 471L195 616Q196 629 188 632T149 637H144Q134 637 131 637T124 640T121 647Z"></path></g></g></g></g></svg></mjx-container></span>，除以<span class="math inline"><mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.372ex;" xmlns="http://www.w3.org/2000/svg" width="4.128ex" height="2.398ex" role="img" focusable="false" viewBox="0 -895.6 1824.4 1060"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="msqrt"><g transform="translate(853,0)"><g data-mml-node="msub"><g data-mml-node="mi"><path data-c="1D451" d="M366 683Q367 683 438 688T511 694Q523 694 523 686Q523 679 450 384T375 83T374 68Q374 26 402 26Q411 27 422 35Q443 55 463 131Q469 151 473 152Q475 153 483 153H487H491Q506 153 506 145Q506 140 503 129Q490 79 473 48T445 8T417 -8Q409 -10 393 -10Q359 -10 336 5T306 36L300 51Q299 52 296 50Q294 48 292 46Q233 -10 172 -10Q117 -10 75 30T33 157Q33 205 53 255T101 341Q148 398 195 420T280 442Q336 442 364 400Q369 394 369 396Q370 400 396 505T424 616Q424 629 417 632T378 637H357Q351 643 351 645T353 664Q358 683 366 683ZM352 326Q329 405 277 405Q242 405 210 374T160 293Q131 214 119 129Q119 126 119 118T118 106Q118 61 136 44T179 26Q233 26 290 98L298 109L352 326Z"></path></g><g data-mml-node="mi" transform="translate(553,-150) scale(0.707)"><path data-c="1D458" d="M121 647Q121 657 125 670T137 683Q138 683 209 688T282 694Q294 694 294 686Q294 679 244 477Q194 279 194 272Q213 282 223 291Q247 309 292 354T362 415Q402 442 438 442Q468 442 485 423T503 369Q503 344 496 327T477 302T456 291T438 288Q418 288 406 299T394 328Q394 353 410 369T442 390L458 393Q446 405 434 405H430Q398 402 367 380T294 316T228 255Q230 254 243 252T267 246T293 238T320 224T342 206T359 180T365 147Q365 130 360 106T354 66Q354 26 381 26Q429 26 459 145Q461 153 479 153H483Q499 153 499 144Q499 139 496 130Q455 -11 378 -11Q333 -11 305 15T277 90Q277 108 280 121T283 145Q283 167 269 183T234 206T200 217T182 220H180Q168 178 159 139T145 81T136 44T129 20T122 7T111 -2Q98 -11 83 -11Q66 -11 57 -1T48 16Q48 26 85 176T158 471L195 616Q196 629 188 632T149 637H144Q134 637 131 637T124 640T121 647Z"></path></g></g></g><g data-mml-node="mo" transform="translate(0,35.6)"><path data-c="221A" d="M95 178Q89 178 81 186T72 200T103 230T169 280T207 309Q209 311 212 311H213Q219 311 227 294T281 177Q300 134 312 108L397 -77Q398 -77 501 136T707 565T814 786Q820 800 834 800Q841 800 846 794T853 782V776L620 293L385 -193Q381 -200 366 -200Q357 -200 354 -197Q352 -195 256 15L160 225L144 214Q129 202 113 190T95 178Z"></path></g><rect width="971.4" height="60" x="853" y="775.6"></rect></g></g></g></svg></mjx-container></span>，将分布的方差纠正回接近 1</li>
</ul></li>
<li><p>并行化的体现</p>
<ul>
<li>序列计算多头注意力</li>
</ul></li>
<li><p>影响计算量的因素</p>
<ul>
<li>序列长度：点积、矩阵乘</li>
<li>头数量</li>
</ul></li>
<li><p>优势</p>
<ul>
<li>并行处理整个序列</li>
<li>长距离依赖</li>
</ul></li>
<li><p>缺点</p>
<ul>
<li>计算量大</li>
<li>超参调优</li>
<li>超长序列处理能力</li>
</ul></li>
</ul>
<h4 id="cross-attention">Cross-Attention</h4>
<h4 id="convolutional-attention">Convolutional Attention</h4>
<ul>
<li>SENet -CBAM</li>
</ul>
<h3 id="基础模型">基础模型</h3>
<h4 id="前馈神经网络">前馈神经网络</h4>
<h4 id="卷积神经网络cnn">卷积神经网络（CNN）</h4>
<h4 id="循环神经网络rnn">循环神经网络（RNN）</h4>
<h4 id="长短时记忆网络lstm">长短时记忆网络（LSTM）</h4>
<h4 id="inception">Inception</h4>
<h3 id="模型调优">模型调优</h3>
<h4 id="模型优化">模型优化</h4>
<!--
细小裂缝检测与原型特征增强:
你是如何实现原型特征增强的？这种方法如何帮助模型捕捉细小裂缝的特征？
在提升对细小裂缝检测准确率方面，你采取了哪些评估指标？如何衡量改进效果？ -->
<!-- 图像预处理与重建技术:
详细解释一下Diffusion模型在隧道图像重建中的具体应用，它是如何减少噪声和补偿信息损失的？
低通滤波器的选择标准是什么？如何确定最佳的滤波参数？
光照嵌入是如何实现的？它如何帮助改善图像的亮度控制和质量？ -->
<h4 id="正则化">正则化</h4>
<ul>
<li>L1 正则化</li>
<li>L2 正则化</li>
</ul>
<h4 id="损失函数">损失函数</h4>
<p>已知 softmax 输出概率序列与实际分布概率序列，计算两者交叉熵</p>
<h4 id="超参数调整">超参数调整</h4>
<!--
如何进行模型的交叉验证？有没有进行过超参数调优？过程和结果如何？ -->
<p>在深度学习中，超参数（Hyperparameters）是指在训练开始前设置的模型参数，不是通过训练学习得到的。超参数的选择对模型性能有很大的影响，不同的超参数设置可能导致显著不同的训练结果。</p>
<h4 id="优化器选择">优化器选择</h4>
<ul>
<li>SGD</li>
<li>AdaGrad</li>
<li>RMSProp</li>
<li>Adam</li>
</ul>
<h4 id="学习率衰减">学习率衰减</h4>
<p>LR 中的连续值特征是如何处理的 为什么 LR 要先对数据进行归一化处理 LR
用了 sigmoid 函数，那么 LR 是线性模型还是非线性模型，为什么</p>
<ul>
<li>线性</li>
<li>分段</li>
<li>余弦</li>
<li>WarmUp</li>
<li>周期性</li>
</ul>
<h4 id="缩放法则-scaling-law">缩放法则 Scaling-Law</h4>
<p>在 AI
领域中，描述模型性能如何随着模型规模（如<strong>参数数量、训练数据量、计算资源</strong>等）变化而变化的一组经验法则</p>
<ul>
<li><p>应用</p>
<ol type="1">
<li><p>设计更大规模的模型</p>
<p>指导研究人员如何设计和训练更大规模的模型，以实现更高的性能</p></li>
<li><p>优化资源分配</p>
<p>如确定是否应增加模型参数数量、增加训练数据量，还是增加计算资源，以实现最优的性能提升</p></li>
<li><p>预测性能</p>
<p>根据现有模型的性能和缩放法则，可以预测更大规模模型的性能</p></li>
</ol></li>
</ul>
<h3 id="常见模型评估指标">常见模型评估指标</h3>
<!-- 你采用了哪些方法来验证模型性能？精度、召回率、F1分数等评价指标的具体数值是多少？ -->
<h4 id="准确率">准确率</h4>
<h4 id="召回率">召回率</h4>
<h4 id="f1-分数">F1 分数</h4>
<h4 id="浮点数运算次数-flops">浮点数运算次数 FLOPs</h4>
<h4 id="帧每秒-fps">帧每秒 FPS</h4>
<h2 id="深度学习框架">深度学习框架</h2>
<h3 id="训练范式">训练范式</h3>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 加载数据</span></span><br><span class="line">dataloader = DataLoader()</span><br><span class="line"></span><br><span class="line"><span class="comment"># 加载模型</span></span><br><span class="line">model = MyModel()</span><br><span class="line"></span><br><span class="line"><span class="comment"># 损失函数</span></span><br><span class="line">criterion = nn.CrossEntropyLoss()</span><br><span class="line"></span><br><span class="line"><span class="comment"># 优化器</span></span><br><span class="line">optimizer = optim.Adam(model.parameters(), lr=<span class="number">0.001</span>)</span><br><span class="line"><span class="comment"># 学习率衰减策略</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 训练num_epochs</span></span><br><span class="line"><span class="keyword">for</span> epoch <span class="keyword">in</span> <span class="built_in">range</span>(num_epochs):</span><br><span class="line">  <span class="comment"># 遍历完整数据集</span></span><br><span class="line">  <span class="keyword">for</span> inputs, labels <span class="keyword">in</span> dataloader:</span><br><span class="line">    <span class="comment"># 梯度置零</span></span><br><span class="line">    optimizer.zero_grad()</span><br><span class="line">    <span class="comment"># 模型推理</span></span><br><span class="line">    outputs = model(inputs)</span><br><span class="line">    <span class="comment"># 计算损失</span></span><br><span class="line">    loss = criterion(outputs, labels)</span><br><span class="line">    <span class="comment"># 累加梯度</span></span><br><span class="line">    loss.backward()</span><br><span class="line">    <span class="comment"># 梯度更新</span></span><br><span class="line">    optimizer.step()</span><br></pre></td></tr></table></figure>
<h3 id="评估范式">评估范式</h3>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 设置模型评估模式</span></span><br><span class="line">model.<span class="built_in">eval</span>()</span><br><span class="line"><span class="comment"># 取消梯度更新</span></span><br><span class="line"><span class="keyword">with</span> torch.no_grad():</span><br><span class="line">  <span class="keyword">for</span> inputs, labels <span class="keyword">in</span> test_dataloader:</span><br><span class="line">    outputs = model(inputs)</span><br><span class="line">    <span class="comment"># 计算准确率或其他指标</span></span><br></pre></td></tr></table></figure>
<h3 id="pytorch">PyTorch</h3>
<h4 id="tensor">Tensor</h4>
<ul>
<li><p>存储</p>
<ul>
<li>头信息区（Tensor）：tensor
的形状（size）、步长（stride）、数据类型（type）等</li>
<li>存储区（Storage）：数据</li>
</ul></li>
<li><p>stride 属性</p>
<p>指定维度中一个元素到下一个元素的步长</p></li>
<li><p>维度变换</p>
<table>
<colgroup>
<col style="width: 10%">
<col style="width: 15%">
<col style="width: 73%">
</colgroup>
<thead>
<tr class="header">
<th>类型</th>
<th>方法</th>
<th>描述</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>维度顺序</td>
<td>permute</td>
<td>指定维度重排，返回共享存储区的 tensor</td>
</tr>
<tr class="even">
<td></td>
<td>transpose</td>
<td>交换维度，返回共享存储区的 tensor</td>
</tr>
<tr class="odd">
<td>形状变换</td>
<td>view</td>
<td>返回共享存储区 tensor，要求存储连续，否则调用 contiguous</td>
</tr>
<tr class="even">
<td></td>
<td>contiguous</td>
<td>开辟新的存储区构建连续 tensor</td>
</tr>
<tr class="odd">
<td></td>
<td>reshape</td>
<td>若连续则返回原 tensor，否则创建新 tensor</td>
</tr>
<tr class="even">
<td>广播</td>
<td>broadcast_to</td>
<td></td>
</tr>
<tr class="odd">
<td>冗余维度</td>
<td>squeeze</td>
<td>压缩维度</td>
</tr>
<tr class="even">
<td></td>
<td>unsqueeze</td>
<td>展开维度</td>
</tr>
<tr class="odd">
<td>扩展维度</td>
<td>expand</td>
<td>扩展大小为 1 的维度</td>
</tr>
<tr class="even">
<td></td>
<td>repeat</td>
<td>按照指定维度重复 tensor</td>
</tr>
<tr class="odd">
<td>展平维度</td>
<td>flatten</td>
<td></td>
</tr>
<tr class="even">
<td></td>
<td>ravel</td>
<td></td>
</tr>
<tr class="odd">
<td>维度剪裁</td>
<td>narrow</td>
<td></td>
</tr>
<tr class="even">
<td>维度展开</td>
<td>unfold</td>
<td></td>
</tr>
</tbody>
</table></li>
<li><p>张量乘法</p>
<table>
<thead>
<tr class="header">
<th style="text-align: center;">方法</th>
<th style="text-align: left;">应用</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: center;"><code>torch.matmul()</code></td>
<td style="text-align: left;">多维矩阵相乘</td>
</tr>
<tr class="even">
<td style="text-align: center;"><code>torch.mm()</code></td>
<td style="text-align: left;">2 维矩阵相乘</td>
</tr>
<tr class="odd">
<td style="text-align: center;"><code>torch.bmm()</code></td>
<td style="text-align: left;">批矩阵相乘</td>
</tr>
<tr class="even">
<td style="text-align: center;"><code>torch.dot()</code></td>
<td style="text-align: left;">点积</td>
</tr>
<tr class="odd">
<td style="text-align: center;"><code>torch.mv()</code></td>
<td style="text-align: left;">矩阵向量相乘</td>
</tr>
<tr class="even">
<td style="text-align: center;"><code>torch.einsum()</code></td>
<td style="text-align: left;">复杂张量运算
<code>torch.einsum('ij,jk-&gt;ik', a, b)</code></td>
</tr>
</tbody>
</table></li>
<li><p>张量合并与拆分</p>
<ul>
<li>stack 扩展维度拼接</li>
<li>cat 根据维度拼接</li>
<li>split 按大小分</li>
<li>chunk 按块分</li>
</ul></li>
</ul>
<h4 id="nn.module">nn.Module</h4>
<p>模块基类</p>
<h4 id="nn.sequential">nn.Sequential</h4>
<p>线性模块容器</p>
<ul>
<li>计算图</li>
<li>环境搭建</li>
<li>数据加载</li>
<li>模型定义</li>
<li>训练</li>
<li>验证</li>
<li>保存</li>
<li>加载模型</li>
</ul>
<h3 id="pytorchlightning">PytorchLightning</h3>
<!-- 为什么选择PytorchLightning作为开发框架？它为你的项目带来了哪些便利？
请详细介绍你的模型架构，包括所使用的网络层、损失函数的选择以及优化器的配置。
在使用PytorchLightning时，如何进行模型的训练、验证和测试阶段的管理？ -->
<h2 id="逻辑思维与项目经验">逻辑思维与项目经验</h2>
<h3 id="逻辑思维">逻辑思维</h3>
<p>准备通过解决实际问题来展示你的逻辑思维能力和数据分析洞察力，可以是以往项目中的案例分析。</p>
<h3 id="团队合作与挑战接受度">团队合作与挑战接受度</h3>
<p>思考并准备实例说明你如何在团队中有效沟通、协作解决问题，以及面对技术挑战时的态度和解决策略。</p>
</div></article></div><div class="card"><article class="card-content article" role="article"><div class="article-meta is-size-7 is-uppercase level is-mobile"><div class="level-left"><span class="level-item">Posted&nbsp;<time dateTime="2024-06-26T08:03:04.000Z" title="2024/6/26 下午4:03:04">2024-06-26</time></span><span class="level-item">Updated&nbsp;<time dateTime="2024-06-26T08:03:04.000Z" title="2024/6/26 下午4:03:04">2024-06-26</time></span><span class="level-item"><a class="link-muted" href="/categories/interview/">interview</a></span></div></div><p class="title is-3 is-size-4-mobile"><a class="link-muted" href="/2024/06/26/knowledge/cv/">计算机视觉 Computer Vision</a></p><div class="content"><blockquote>
<p><a target="_blank" rel="noopener" href="https://github.com/WeThinkIn/Interview-for-Algorithm-Engineer">【三年面试五年模拟】算法工程师的求职面试秘籍</a>
&gt; <a target="_blank" rel="noopener" href="https://cloud.tencent.com/developer/article/1558355">从
ReLU 到 GELU，一文概览神经网络的激活函数</a></p>
</blockquote>
<ul>
<li><p>https://github.com/DWCTOD/interview/blob/master/detail/%E4%BD%9C%E4%B8%9A%E5%B8%AE%20%E8%A7%86%E8%A7%89%E7%AE%97%E6%B3%95%E5%B7%A5%E7%A8%8B%E5%B8%88%20%E9%9D%A2%E7%BB%8F%EF%BC%882020%E5%B1%8A%EF%BC%89.md</p></li>
<li><p>https://github.com/GYee/CV_interviews_Q-A</p></li>
</ul>
<h2 id="图像处理与计算机视觉基础">图像处理与计算机视觉基础</h2>
<h3 id="基本概念">基本概念</h3>
<h3 id="原理">原理</h3>
<h3 id="图像预处理">图像预处理</h3>
<h3 id="特征提取">特征提取</h3>
<h3 id="对象检测">对象检测</h3>
<h3 id="图像分类">图像分类</h3>
<h3 id="图像分割">图像分割</h3>
<h3 id="opencv-库">OpenCV 库</h3>
<h4 id="读写图像">读写图像</h4>
<h4 id="图像滤波">图像滤波</h4>
<h4 id="几何变换">几何变换</h4>
<h4 id="特征检测与描述">特征检测与描述</h4>
<h2 id="深度学习基础">深度学习基础</h2>
<h3 id="梯度下降">梯度下降</h3>
<h3 id="滑动平均">滑动平均</h3>
<h3 id="模型微调fine-tuning">模型微调（Fine-tuning）</h3>
<h3 id="基础模块">基础模块</h3>
<h4 id="池化层-pooling-layer">池化层 Pooling Layer</h4>
<h4 id="归一化层-normalization-layer">归一化层 Normalization Layer</h4>
<ul>
<li>BN，Batch Normalization</li>
<li>IN，Instance Normalization</li>
<li>LN，Layer Normalization</li>
<li>GN，Group Normalization</li>
</ul>
<p>BN 是怎么做，作用是什么</p>
<h4 id="激活层">激活层</h4>
<ol type="1">
<li><p>常见激活函数</p>
<ul>
<li>Sigmoid</li>
<li>Tanh</li>
<li>ReLU</li>
<li>LeakyReLU</li>
<li>SoftPlus</li>
<li>ELU</li>
<li>SELU，自归一化</li>
<li>Swish，类 Sigmoid 作为开关</li>
<li>GELU</li>
<li>GLU</li>
</ul></li>
<li><p>特性</p>
<ul>
<li>梯度消失 存在偏导过小</li>
<li>梯度爆炸 偏导累乘过大</li>
<li>梯度裁剪</li>
<li>输出均值为 0 能避免每次权重只能往单一反向变化</li>
<li>ReLU 计算复杂度低</li>
<li>ReLU
的负半轴为输出值增加稀疏性，减少计算量，但同时会让一些神经元不能更新</li>
<li>SoftPlus，ReLU 的平滑</li>
</ul></li>
</ol>
<h4 id="全连接层-linear">全连接层 Linear</h4>
<h4 id="嵌入层-embedding">嵌入层 Embedding</h4>
<h4 id="卷积层-convolution">卷积层 Convolution</h4>
<ul>
<li><p>特征</p>
<ul>
<li>局部感知、权值共享、平移不变、多核</li>
</ul></li>
<li><p>1×1 卷积</p>
<ul>
<li>特征增强</li>
<li>特征融合</li>
<li>改变通道数</li>
<li>分类</li>
</ul></li>
<li><p>空洞卷积</p></li>
<li><p>分组卷积</p></li>
</ul>
<h4 id="转置卷积层-transpose-convolution">转置卷积层 Transpose
Convolution</h4>
<h3 id="上采样">上采样</h3>
<ol type="1">
<li>插值</li>
<li>反卷积</li>
<li>PixelShuffle</li>
</ol>
<h3 id="优化模块">优化模块</h3>
<h4 id="残差结构-residual-connection">残差结构 Residual Connection</h4>
<p>将输入与层输出相加</p>
<ul>
<li><p>优势</p>
<ul>
<li>缓解梯度消失，增加网络深度</li>
<li>保留信息，特征重用</li>
</ul></li>
</ul>
<h4 id="空间金字塔池化spatial-pyramid-poolingspp">空间金字塔池化（Spatial
Pyramid Pooling，SPP）</h4>
<h4 id="空洞空间金字塔池化atrous-spatial-pyramid-poolingaspp">空洞空间金字塔池化（Atrous
Spatial Pyramid Pooling，ASPP）</h4>
<h4 id="hdc">HDC</h4>
<h4 id="可变形卷积-deformable-convolution">可变形卷积 Deformable
Convolution</h4>
<h4 id="可分离卷积-separable-convolution">可分离卷积 Separable
Convolution</h4>
<h4 id="transformer">Transformer</h4>
<ul>
<li><p>模块结构</p>
<ul>
<li>多头自注意力机制（Multi-Head Self-Attention Mechanism）</li>
<li>前馈神经网络（Feed-Forward Neural Network）</li>
<li>层归一化（Layer Normalization）</li>
<li>残差连接（Residual Connections）</li>
</ul></li>
<li><p>Self-Attention</p>
<p><span class="math display"><mjx-container class="MathJax" jax="SVG" display="true"><svg style="vertical-align: -2.308ex;" xmlns="http://www.w3.org/2000/svg" width="40.947ex" height="5.741ex" role="img" focusable="false" viewBox="0 -1517.7 18098.7 2537.7"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="mi"><path data-c="1D434" d="M208 74Q208 50 254 46Q272 46 272 35Q272 34 270 22Q267 8 264 4T251 0Q249 0 239 0T205 1T141 2Q70 2 50 0H42Q35 7 35 11Q37 38 48 46H62Q132 49 164 96Q170 102 345 401T523 704Q530 716 547 716H555H572Q578 707 578 706L606 383Q634 60 636 57Q641 46 701 46Q726 46 726 36Q726 34 723 22Q720 7 718 4T704 0Q701 0 690 0T651 1T578 2Q484 2 455 0H443Q437 6 437 9T439 27Q443 40 445 43L449 46H469Q523 49 533 63L521 213H283L249 155Q208 86 208 74ZM516 260Q516 271 504 416T490 562L463 519Q447 492 400 412L310 260L413 259Q516 259 516 260Z"></path></g><g data-mml-node="mi" transform="translate(750,0)"><path data-c="1D461" d="M26 385Q19 392 19 395Q19 399 22 411T27 425Q29 430 36 430T87 431H140L159 511Q162 522 166 540T173 566T179 586T187 603T197 615T211 624T229 626Q247 625 254 615T261 596Q261 589 252 549T232 470L222 433Q222 431 272 431H323Q330 424 330 420Q330 398 317 385H210L174 240Q135 80 135 68Q135 26 162 26Q197 26 230 60T283 144Q285 150 288 151T303 153H307Q322 153 322 145Q322 142 319 133Q314 117 301 95T267 48T216 6T155 -11Q125 -11 98 4T59 56Q57 64 57 83V101L92 241Q127 382 128 383Q128 385 77 385H26Z"></path></g><g data-mml-node="mi" transform="translate(1111,0)"><path data-c="1D461" d="M26 385Q19 392 19 395Q19 399 22 411T27 425Q29 430 36 430T87 431H140L159 511Q162 522 166 540T173 566T179 586T187 603T197 615T211 624T229 626Q247 625 254 615T261 596Q261 589 252 549T232 470L222 433Q222 431 272 431H323Q330 424 330 420Q330 398 317 385H210L174 240Q135 80 135 68Q135 26 162 26Q197 26 230 60T283 144Q285 150 288 151T303 153H307Q322 153 322 145Q322 142 319 133Q314 117 301 95T267 48T216 6T155 -11Q125 -11 98 4T59 56Q57 64 57 83V101L92 241Q127 382 128 383Q128 385 77 385H26Z"></path></g><g data-mml-node="mi" transform="translate(1472,0)"><path data-c="1D452" d="M39 168Q39 225 58 272T107 350T174 402T244 433T307 442H310Q355 442 388 420T421 355Q421 265 310 237Q261 224 176 223Q139 223 138 221Q138 219 132 186T125 128Q125 81 146 54T209 26T302 45T394 111Q403 121 406 121Q410 121 419 112T429 98T420 82T390 55T344 24T281 -1T205 -11Q126 -11 83 42T39 168ZM373 353Q367 405 305 405Q272 405 244 391T199 357T170 316T154 280T149 261Q149 260 169 260Q282 260 327 284T373 353Z"></path></g><g data-mml-node="mi" transform="translate(1938,0)"><path data-c="1D45B" d="M21 287Q22 293 24 303T36 341T56 388T89 425T135 442Q171 442 195 424T225 390T231 369Q231 367 232 367L243 378Q304 442 382 442Q436 442 469 415T503 336T465 179T427 52Q427 26 444 26Q450 26 453 27Q482 32 505 65T540 145Q542 153 560 153Q580 153 580 145Q580 144 576 130Q568 101 554 73T508 17T439 -10Q392 -10 371 17T350 73Q350 92 386 193T423 345Q423 404 379 404H374Q288 404 229 303L222 291L189 157Q156 26 151 16Q138 -11 108 -11Q95 -11 87 -5T76 7T74 17Q74 30 112 180T152 343Q153 348 153 366Q153 405 129 405Q91 405 66 305Q60 285 60 284Q58 278 41 278H27Q21 284 21 287Z"></path></g><g data-mml-node="mi" transform="translate(2538,0)"><path data-c="1D461" d="M26 385Q19 392 19 395Q19 399 22 411T27 425Q29 430 36 430T87 431H140L159 511Q162 522 166 540T173 566T179 586T187 603T197 615T211 624T229 626Q247 625 254 615T261 596Q261 589 252 549T232 470L222 433Q222 431 272 431H323Q330 424 330 420Q330 398 317 385H210L174 240Q135 80 135 68Q135 26 162 26Q197 26 230 60T283 144Q285 150 288 151T303 153H307Q322 153 322 145Q322 142 319 133Q314 117 301 95T267 48T216 6T155 -11Q125 -11 98 4T59 56Q57 64 57 83V101L92 241Q127 382 128 383Q128 385 77 385H26Z"></path></g><g data-mml-node="mi" transform="translate(2899,0)"><path data-c="1D456" d="M184 600Q184 624 203 642T247 661Q265 661 277 649T290 619Q290 596 270 577T226 557Q211 557 198 567T184 600ZM21 287Q21 295 30 318T54 369T98 420T158 442Q197 442 223 419T250 357Q250 340 236 301T196 196T154 83Q149 61 149 51Q149 26 166 26Q175 26 185 29T208 43T235 78T260 137Q263 149 265 151T282 153Q302 153 302 143Q302 135 293 112T268 61T223 11T161 -11Q129 -11 102 10T74 74Q74 91 79 106T122 220Q160 321 166 341T173 380Q173 404 156 404H154Q124 404 99 371T61 287Q60 286 59 284T58 281T56 279T53 278T49 278T41 278H27Q21 284 21 287Z"></path></g><g data-mml-node="mi" transform="translate(3244,0)"><path data-c="1D45C" d="M201 -11Q126 -11 80 38T34 156Q34 221 64 279T146 380Q222 441 301 441Q333 441 341 440Q354 437 367 433T402 417T438 387T464 338T476 268Q476 161 390 75T201 -11ZM121 120Q121 70 147 48T206 26Q250 26 289 58T351 142Q360 163 374 216T388 308Q388 352 370 375Q346 405 306 405Q243 405 195 347Q158 303 140 230T121 120Z"></path></g><g data-mml-node="mi" transform="translate(3729,0)"><path data-c="1D45B" d="M21 287Q22 293 24 303T36 341T56 388T89 425T135 442Q171 442 195 424T225 390T231 369Q231 367 232 367L243 378Q304 442 382 442Q436 442 469 415T503 336T465 179T427 52Q427 26 444 26Q450 26 453 27Q482 32 505 65T540 145Q542 153 560 153Q580 153 580 145Q580 144 576 130Q568 101 554 73T508 17T439 -10Q392 -10 371 17T350 73Q350 92 386 193T423 345Q423 404 379 404H374Q288 404 229 303L222 291L189 157Q156 26 151 16Q138 -11 108 -11Q95 -11 87 -5T76 7T74 17Q74 30 112 180T152 343Q153 348 153 366Q153 405 129 405Q91 405 66 305Q60 285 60 284Q58 278 41 278H27Q21 284 21 287Z"></path></g><g data-mml-node="mo" transform="translate(4329,0)"><path data-c="28" d="M94 250Q94 319 104 381T127 488T164 576T202 643T244 695T277 729T302 750H315H319Q333 750 333 741Q333 738 316 720T275 667T226 581T184 443T167 250T184 58T225 -81T274 -167T316 -220T333 -241Q333 -250 318 -250H315H302L274 -226Q180 -141 137 -14T94 250Z"></path></g><g data-mml-node="mi" transform="translate(4718,0)"><path data-c="1D444" d="M399 -80Q399 -47 400 -30T402 -11V-7L387 -11Q341 -22 303 -22Q208 -22 138 35T51 201Q50 209 50 244Q50 346 98 438T227 601Q351 704 476 704Q514 704 524 703Q621 689 680 617T740 435Q740 255 592 107Q529 47 461 16L444 8V3Q444 2 449 -24T470 -66T516 -82Q551 -82 583 -60T625 -3Q631 11 638 11Q647 11 649 2Q649 -6 639 -34T611 -100T557 -165T481 -194Q399 -194 399 -87V-80ZM636 468Q636 523 621 564T580 625T530 655T477 665Q429 665 379 640Q277 591 215 464T153 216Q153 110 207 59Q231 38 236 38V46Q236 86 269 120T347 155Q372 155 390 144T417 114T429 82T435 55L448 64Q512 108 557 185T619 334T636 468ZM314 18Q362 18 404 39L403 49Q399 104 366 115Q354 117 347 117Q344 117 341 117T337 118Q317 118 296 98T274 52Q274 18 314 18Z"></path></g><g data-mml-node="mo" transform="translate(5509,0)"><path data-c="2C" d="M78 35T78 60T94 103T137 121Q165 121 187 96T210 8Q210 -27 201 -60T180 -117T154 -158T130 -185T117 -194Q113 -194 104 -185T95 -172Q95 -168 106 -156T131 -126T157 -76T173 -3V9L172 8Q170 7 167 6T161 3T152 1T140 0Q113 0 96 17Z"></path></g><g data-mml-node="mi" transform="translate(5953.7,0)"><path data-c="1D43E" d="M285 628Q285 635 228 637Q205 637 198 638T191 647Q191 649 193 661Q199 681 203 682Q205 683 214 683H219Q260 681 355 681Q389 681 418 681T463 682T483 682Q500 682 500 674Q500 669 497 660Q496 658 496 654T495 648T493 644T490 641T486 639T479 638T470 637T456 637Q416 636 405 634T387 623L306 305Q307 305 490 449T678 597Q692 611 692 620Q692 635 667 637Q651 637 651 648Q651 650 654 662T659 677Q662 682 676 682Q680 682 711 681T791 680Q814 680 839 681T869 682Q889 682 889 672Q889 650 881 642Q878 637 862 637Q787 632 726 586Q710 576 656 534T556 455L509 418L518 396Q527 374 546 329T581 244Q656 67 661 61Q663 59 666 57Q680 47 717 46H738Q744 38 744 37T741 19Q737 6 731 0H720Q680 3 625 3Q503 3 488 0H478Q472 6 472 9T474 27Q478 40 480 43T491 46H494Q544 46 544 71Q544 75 517 141T485 216L427 354L359 301L291 248L268 155Q245 63 245 58Q245 51 253 49T303 46H334Q340 37 340 35Q340 19 333 5Q328 0 317 0Q314 0 280 1T180 2Q118 2 85 2T49 1Q31 1 31 11Q31 13 34 25Q38 41 42 43T65 46Q92 46 125 49Q139 52 144 61Q147 65 216 339T285 628Z"></path></g><g data-mml-node="mo" transform="translate(6842.7,0)"><path data-c="2C" d="M78 35T78 60T94 103T137 121Q165 121 187 96T210 8Q210 -27 201 -60T180 -117T154 -158T130 -185T117 -194Q113 -194 104 -185T95 -172Q95 -168 106 -156T131 -126T157 -76T173 -3V9L172 8Q170 7 167 6T161 3T152 1T140 0Q113 0 96 17Z"></path></g><g data-mml-node="mi" transform="translate(7287.3,0)"><path data-c="1D449" d="M52 648Q52 670 65 683H76Q118 680 181 680Q299 680 320 683H330Q336 677 336 674T334 656Q329 641 325 637H304Q282 635 274 635Q245 630 242 620Q242 618 271 369T301 118L374 235Q447 352 520 471T595 594Q599 601 599 609Q599 633 555 637Q537 637 537 648Q537 649 539 661Q542 675 545 679T558 683Q560 683 570 683T604 682T668 681Q737 681 755 683H762Q769 676 769 672Q769 655 760 640Q757 637 743 637Q730 636 719 635T698 630T682 623T670 615T660 608T652 599T645 592L452 282Q272 -9 266 -16Q263 -18 259 -21L241 -22H234Q216 -22 216 -15Q213 -9 177 305Q139 623 138 626Q133 637 76 637H59Q52 642 52 648Z"></path></g><g data-mml-node="mo" transform="translate(8056.3,0)"><path data-c="29" d="M60 749L64 750Q69 750 74 750H86L114 726Q208 641 251 514T294 250Q294 182 284 119T261 12T224 -76T186 -143T145 -194T113 -227T90 -246Q87 -249 86 -250H74Q66 -250 63 -250T58 -247T55 -238Q56 -237 66 -225Q221 -64 221 250T66 725Q56 737 55 738Q55 746 60 749Z"></path></g><g data-mml-node="mo" transform="translate(8723.1,0)"><path data-c="3D" d="M56 347Q56 360 70 367H707Q722 359 722 347Q722 336 708 328L390 327H72Q56 332 56 347ZM56 153Q56 168 72 173H708Q722 163 722 153Q722 140 707 133H70Q56 140 56 153Z"></path></g><g data-mml-node="mi" transform="translate(9778.9,0)"><path data-c="1D446" d="M308 24Q367 24 416 76T466 197Q466 260 414 284Q308 311 278 321T236 341Q176 383 176 462Q176 523 208 573T273 648Q302 673 343 688T407 704H418H425Q521 704 564 640Q565 640 577 653T603 682T623 704Q624 704 627 704T632 705Q645 705 645 698T617 577T585 459T569 456Q549 456 549 465Q549 471 550 475Q550 478 551 494T553 520Q553 554 544 579T526 616T501 641Q465 662 419 662Q362 662 313 616T263 510Q263 480 278 458T319 427Q323 425 389 408T456 390Q490 379 522 342T554 242Q554 216 546 186Q541 164 528 137T492 78T426 18T332 -20Q320 -22 298 -22Q199 -22 144 33L134 44L106 13Q83 -14 78 -18T65 -22Q52 -22 52 -14Q52 -11 110 221Q112 227 130 227H143Q149 221 149 216Q149 214 148 207T144 186T142 153Q144 114 160 87T203 47T255 29T308 24Z"></path></g><g data-mml-node="mi" transform="translate(10423.9,0)"><path data-c="1D45C" d="M201 -11Q126 -11 80 38T34 156Q34 221 64 279T146 380Q222 441 301 441Q333 441 341 440Q354 437 367 433T402 417T438 387T464 338T476 268Q476 161 390 75T201 -11ZM121 120Q121 70 147 48T206 26Q250 26 289 58T351 142Q360 163 374 216T388 308Q388 352 370 375Q346 405 306 405Q243 405 195 347Q158 303 140 230T121 120Z"></path></g><g data-mml-node="mi" transform="translate(10908.9,0)"><path data-c="1D453" d="M118 -162Q120 -162 124 -164T135 -167T147 -168Q160 -168 171 -155T187 -126Q197 -99 221 27T267 267T289 382V385H242Q195 385 192 387Q188 390 188 397L195 425Q197 430 203 430T250 431Q298 431 298 432Q298 434 307 482T319 540Q356 705 465 705Q502 703 526 683T550 630Q550 594 529 578T487 561Q443 561 443 603Q443 622 454 636T478 657L487 662Q471 668 457 668Q445 668 434 658T419 630Q412 601 403 552T387 469T380 433Q380 431 435 431Q480 431 487 430T498 424Q499 420 496 407T491 391Q489 386 482 386T428 385H372L349 263Q301 15 282 -47Q255 -132 212 -173Q175 -205 139 -205Q107 -205 81 -186T55 -132Q55 -95 76 -78T118 -61Q162 -61 162 -103Q162 -122 151 -136T127 -157L118 -162Z"></path></g><g data-mml-node="mi" transform="translate(11458.9,0)"><path data-c="1D461" d="M26 385Q19 392 19 395Q19 399 22 411T27 425Q29 430 36 430T87 431H140L159 511Q162 522 166 540T173 566T179 586T187 603T197 615T211 624T229 626Q247 625 254 615T261 596Q261 589 252 549T232 470L222 433Q222 431 272 431H323Q330 424 330 420Q330 398 317 385H210L174 240Q135 80 135 68Q135 26 162 26Q197 26 230 60T283 144Q285 150 288 151T303 153H307Q322 153 322 145Q322 142 319 133Q314 117 301 95T267 48T216 6T155 -11Q125 -11 98 4T59 56Q57 64 57 83V101L92 241Q127 382 128 383Q128 385 77 385H26Z"></path></g><g data-mml-node="mi" transform="translate(11819.9,0)"><path data-c="1D45A" d="M21 287Q22 293 24 303T36 341T56 388T88 425T132 442T175 435T205 417T221 395T229 376L231 369Q231 367 232 367L243 378Q303 442 384 442Q401 442 415 440T441 433T460 423T475 411T485 398T493 385T497 373T500 364T502 357L510 367Q573 442 659 442Q713 442 746 415T780 336Q780 285 742 178T704 50Q705 36 709 31T724 26Q752 26 776 56T815 138Q818 149 821 151T837 153Q857 153 857 145Q857 144 853 130Q845 101 831 73T785 17T716 -10Q669 -10 648 17T627 73Q627 92 663 193T700 345Q700 404 656 404H651Q565 404 506 303L499 291L466 157Q433 26 428 16Q415 -11 385 -11Q372 -11 364 -4T353 8T350 18Q350 29 384 161L420 307Q423 322 423 345Q423 404 379 404H374Q288 404 229 303L222 291L189 157Q156 26 151 16Q138 -11 108 -11Q95 -11 87 -5T76 7T74 17Q74 30 112 181Q151 335 151 342Q154 357 154 369Q154 405 129 405Q107 405 92 377T69 316T57 280Q55 278 41 278H27Q21 284 21 287Z"></path></g><g data-mml-node="mi" transform="translate(12697.9,0)"><path data-c="1D44E" d="M33 157Q33 258 109 349T280 441Q331 441 370 392Q386 422 416 422Q429 422 439 414T449 394Q449 381 412 234T374 68Q374 43 381 35T402 26Q411 27 422 35Q443 55 463 131Q469 151 473 152Q475 153 483 153H487Q506 153 506 144Q506 138 501 117T481 63T449 13Q436 0 417 -8Q409 -10 393 -10Q359 -10 336 5T306 36L300 51Q299 52 296 50Q294 48 292 46Q233 -10 172 -10Q117 -10 75 30T33 157ZM351 328Q351 334 346 350T323 385T277 405Q242 405 210 374T160 293Q131 214 119 129Q119 126 119 118T118 106Q118 61 136 44T179 26Q217 26 254 59T298 110Q300 114 325 217T351 328Z"></path></g><g data-mml-node="mi" transform="translate(13226.9,0)"><path data-c="1D465" d="M52 289Q59 331 106 386T222 442Q257 442 286 424T329 379Q371 442 430 442Q467 442 494 420T522 361Q522 332 508 314T481 292T458 288Q439 288 427 299T415 328Q415 374 465 391Q454 404 425 404Q412 404 406 402Q368 386 350 336Q290 115 290 78Q290 50 306 38T341 26Q378 26 414 59T463 140Q466 150 469 151T485 153H489Q504 153 504 145Q504 144 502 134Q486 77 440 33T333 -11Q263 -11 227 52Q186 -10 133 -10H127Q78 -10 57 16T35 71Q35 103 54 123T99 143Q142 143 142 101Q142 81 130 66T107 46T94 41L91 40Q91 39 97 36T113 29T132 26Q168 26 194 71Q203 87 217 139T245 247T261 313Q266 340 266 352Q266 380 251 392T217 404Q177 404 142 372T93 290Q91 281 88 280T72 278H58Q52 284 52 289Z"></path></g><g data-mml-node="mo" transform="translate(13798.9,0)"><path data-c="28" d="M94 250Q94 319 104 381T127 488T164 576T202 643T244 695T277 729T302 750H315H319Q333 750 333 741Q333 738 316 720T275 667T226 581T184 443T167 250T184 58T225 -81T274 -167T316 -220T333 -241Q333 -250 318 -250H315H302L274 -226Q180 -141 137 -14T94 250Z"></path></g><g data-mml-node="mfrac" transform="translate(14187.9,0)"><g data-mml-node="mrow" transform="translate(220,676)"><g data-mml-node="mi"><path data-c="1D444" d="M399 -80Q399 -47 400 -30T402 -11V-7L387 -11Q341 -22 303 -22Q208 -22 138 35T51 201Q50 209 50 244Q50 346 98 438T227 601Q351 704 476 704Q514 704 524 703Q621 689 680 617T740 435Q740 255 592 107Q529 47 461 16L444 8V3Q444 2 449 -24T470 -66T516 -82Q551 -82 583 -60T625 -3Q631 11 638 11Q647 11 649 2Q649 -6 639 -34T611 -100T557 -165T481 -194Q399 -194 399 -87V-80ZM636 468Q636 523 621 564T580 625T530 655T477 665Q429 665 379 640Q277 591 215 464T153 216Q153 110 207 59Q231 38 236 38V46Q236 86 269 120T347 155Q372 155 390 144T417 114T429 82T435 55L448 64Q512 108 557 185T619 334T636 468ZM314 18Q362 18 404 39L403 49Q399 104 366 115Q354 117 347 117Q344 117 341 117T337 118Q317 118 296 98T274 52Q274 18 314 18Z"></path></g><g data-mml-node="msup" transform="translate(791,0)"><g data-mml-node="mi"><path data-c="1D43E" d="M285 628Q285 635 228 637Q205 637 198 638T191 647Q191 649 193 661Q199 681 203 682Q205 683 214 683H219Q260 681 355 681Q389 681 418 681T463 682T483 682Q500 682 500 674Q500 669 497 660Q496 658 496 654T495 648T493 644T490 641T486 639T479 638T470 637T456 637Q416 636 405 634T387 623L306 305Q307 305 490 449T678 597Q692 611 692 620Q692 635 667 637Q651 637 651 648Q651 650 654 662T659 677Q662 682 676 682Q680 682 711 681T791 680Q814 680 839 681T869 682Q889 682 889 672Q889 650 881 642Q878 637 862 637Q787 632 726 586Q710 576 656 534T556 455L509 418L518 396Q527 374 546 329T581 244Q656 67 661 61Q663 59 666 57Q680 47 717 46H738Q744 38 744 37T741 19Q737 6 731 0H720Q680 3 625 3Q503 3 488 0H478Q472 6 472 9T474 27Q478 40 480 43T491 46H494Q544 46 544 71Q544 75 517 141T485 216L427 354L359 301L291 248L268 155Q245 63 245 58Q245 51 253 49T303 46H334Q340 37 340 35Q340 19 333 5Q328 0 317 0Q314 0 280 1T180 2Q118 2 85 2T49 1Q31 1 31 11Q31 13 34 25Q38 41 42 43T65 46Q92 46 125 49Q139 52 144 61Q147 65 216 339T285 628Z"></path></g><g data-mml-node="mi" transform="translate(974,363) scale(0.707)"><path data-c="1D447" d="M40 437Q21 437 21 445Q21 450 37 501T71 602L88 651Q93 669 101 677H569H659Q691 677 697 676T704 667Q704 661 687 553T668 444Q668 437 649 437Q640 437 637 437T631 442L629 445Q629 451 635 490T641 551Q641 586 628 604T573 629Q568 630 515 631Q469 631 457 630T439 622Q438 621 368 343T298 60Q298 48 386 46Q418 46 427 45T436 36Q436 31 433 22Q429 4 424 1L422 0Q419 0 415 0Q410 0 363 1T228 2Q99 2 64 0H49Q43 6 43 9T45 27Q49 40 55 46H83H94Q174 46 189 55Q190 56 191 56Q196 59 201 76T241 233Q258 301 269 344Q339 619 339 625Q339 630 310 630H279Q212 630 191 624Q146 614 121 583T67 467Q60 445 57 441T43 437H40Z"></path></g></g></g><g data-mml-node="msqrt" transform="translate(464.2,-855.6)"><g transform="translate(853,0)"><g data-mml-node="msub"><g data-mml-node="mi"><path data-c="1D451" d="M366 683Q367 683 438 688T511 694Q523 694 523 686Q523 679 450 384T375 83T374 68Q374 26 402 26Q411 27 422 35Q443 55 463 131Q469 151 473 152Q475 153 483 153H487H491Q506 153 506 145Q506 140 503 129Q490 79 473 48T445 8T417 -8Q409 -10 393 -10Q359 -10 336 5T306 36L300 51Q299 52 296 50Q294 48 292 46Q233 -10 172 -10Q117 -10 75 30T33 157Q33 205 53 255T101 341Q148 398 195 420T280 442Q336 442 364 400Q369 394 369 396Q370 400 396 505T424 616Q424 629 417 632T378 637H357Q351 643 351 645T353 664Q358 683 366 683ZM352 326Q329 405 277 405Q242 405 210 374T160 293Q131 214 119 129Q119 126 119 118T118 106Q118 61 136 44T179 26Q233 26 290 98L298 109L352 326Z"></path></g><g data-mml-node="mi" transform="translate(553,-150) scale(0.707)"><path data-c="1D458" d="M121 647Q121 657 125 670T137 683Q138 683 209 688T282 694Q294 694 294 686Q294 679 244 477Q194 279 194 272Q213 282 223 291Q247 309 292 354T362 415Q402 442 438 442Q468 442 485 423T503 369Q503 344 496 327T477 302T456 291T438 288Q418 288 406 299T394 328Q394 353 410 369T442 390L458 393Q446 405 434 405H430Q398 402 367 380T294 316T228 255Q230 254 243 252T267 246T293 238T320 224T342 206T359 180T365 147Q365 130 360 106T354 66Q354 26 381 26Q429 26 459 145Q461 153 479 153H483Q499 153 499 144Q499 139 496 130Q455 -11 378 -11Q333 -11 305 15T277 90Q277 108 280 121T283 145Q283 167 269 183T234 206T200 217T182 220H180Q168 178 159 139T145 81T136 44T129 20T122 7T111 -2Q98 -11 83 -11Q66 -11 57 -1T48 16Q48 26 85 176T158 471L195 616Q196 629 188 632T149 637H144Q134 637 131 637T124 640T121 647Z"></path></g></g></g><g data-mml-node="mo" transform="translate(0,35.6)"><path data-c="221A" d="M95 178Q89 178 81 186T72 200T103 230T169 280T207 309Q209 311 212 311H213Q219 311 227 294T281 177Q300 134 312 108L397 -77Q398 -77 501 136T707 565T814 786Q820 800 834 800Q841 800 846 794T853 782V776L620 293L385 -193Q381 -200 366 -200Q357 -200 354 -197Q352 -195 256 15L160 225L144 214Q129 202 113 190T95 178Z"></path></g><rect width="971.4" height="60" x="853" y="775.6"></rect></g><rect width="2512.8" height="60" x="120" y="220"></rect></g><g data-mml-node="mo" transform="translate(16940.7,0)"><path data-c="29" d="M60 749L64 750Q69 750 74 750H86L114 726Q208 641 251 514T294 250Q294 182 284 119T261 12T224 -76T186 -143T145 -194T113 -227T90 -246Q87 -249 86 -250H74Q66 -250 63 -250T58 -247T55 -238Q56 -237 66 -225Q221 -64 221 250T66 725Q56 737 55 738Q55 746 60 749Z"></path></g><g data-mml-node="mi" transform="translate(17329.7,0)"><path data-c="1D449" d="M52 648Q52 670 65 683H76Q118 680 181 680Q299 680 320 683H330Q336 677 336 674T334 656Q329 641 325 637H304Q282 635 274 635Q245 630 242 620Q242 618 271 369T301 118L374 235Q447 352 520 471T595 594Q599 601 599 609Q599 633 555 637Q537 637 537 648Q537 649 539 661Q542 675 545 679T558 683Q560 683 570 683T604 682T668 681Q737 681 755 683H762Q769 676 769 672Q769 655 760 640Q757 637 743 637Q730 636 719 635T698 630T682 623T670 615T660 608T652 599T645 592L452 282Q272 -9 266 -16Q263 -18 259 -21L241 -22H234Q216 -22 216 -15Q213 -9 177 305Q139 623 138 626Q133 637 76 637H59Q52 642 52 648Z"></path></g></g></g></svg></mjx-container></span></p></li>
<li><p>除以<span class="math inline"><mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.372ex;" xmlns="http://www.w3.org/2000/svg" width="4.128ex" height="2.398ex" role="img" focusable="false" viewBox="0 -895.6 1824.4 1060"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="msqrt"><g transform="translate(853,0)"><g data-mml-node="msub"><g data-mml-node="mi"><path data-c="1D451" d="M366 683Q367 683 438 688T511 694Q523 694 523 686Q523 679 450 384T375 83T374 68Q374 26 402 26Q411 27 422 35Q443 55 463 131Q469 151 473 152Q475 153 483 153H487H491Q506 153 506 145Q506 140 503 129Q490 79 473 48T445 8T417 -8Q409 -10 393 -10Q359 -10 336 5T306 36L300 51Q299 52 296 50Q294 48 292 46Q233 -10 172 -10Q117 -10 75 30T33 157Q33 205 53 255T101 341Q148 398 195 420T280 442Q336 442 364 400Q369 394 369 396Q370 400 396 505T424 616Q424 629 417 632T378 637H357Q351 643 351 645T353 664Q358 683 366 683ZM352 326Q329 405 277 405Q242 405 210 374T160 293Q131 214 119 129Q119 126 119 118T118 106Q118 61 136 44T179 26Q233 26 290 98L298 109L352 326Z"></path></g><g data-mml-node="mi" transform="translate(553,-150) scale(0.707)"><path data-c="1D458" d="M121 647Q121 657 125 670T137 683Q138 683 209 688T282 694Q294 694 294 686Q294 679 244 477Q194 279 194 272Q213 282 223 291Q247 309 292 354T362 415Q402 442 438 442Q468 442 485 423T503 369Q503 344 496 327T477 302T456 291T438 288Q418 288 406 299T394 328Q394 353 410 369T442 390L458 393Q446 405 434 405H430Q398 402 367 380T294 316T228 255Q230 254 243 252T267 246T293 238T320 224T342 206T359 180T365 147Q365 130 360 106T354 66Q354 26 381 26Q429 26 459 145Q461 153 479 153H483Q499 153 499 144Q499 139 496 130Q455 -11 378 -11Q333 -11 305 15T277 90Q277 108 280 121T283 145Q283 167 269 183T234 206T200 217T182 220H180Q168 178 159 139T145 81T136 44T129 20T122 7T111 -2Q98 -11 83 -11Q66 -11 57 -1T48 16Q48 26 85 176T158 471L195 616Q196 629 188 632T149 637H144Q134 637 131 637T124 640T121 647Z"></path></g></g></g><g data-mml-node="mo" transform="translate(0,35.6)"><path data-c="221A" d="M95 178Q89 178 81 186T72 200T103 230T169 280T207 309Q209 311 212 311H213Q219 311 227 294T281 177Q300 134 312 108L397 -77Q398 -77 501 136T707 565T814 786Q820 800 834 800Q841 800 846 794T853 782V776L620 293L385 -193Q381 -200 366 -200Q357 -200 354 -197Q352 -195 256 15L160 225L144 214Q129 202 113 190T95 178Z"></path></g><rect width="971.4" height="60" x="853" y="775.6"></rect></g></g></g></svg></mjx-container></span>的原因</p>
<ul>
<li>矩阵计算导致的元素值整体偏大，从而引发梯度消失</li>
<li>计算后的数据平方差为<span class="math inline"><mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.357ex;" xmlns="http://www.w3.org/2000/svg" width="2.198ex" height="1.927ex" role="img" focusable="false" viewBox="0 -694 971.4 851.8"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="msub"><g data-mml-node="mi"><path data-c="1D451" d="M366 683Q367 683 438 688T511 694Q523 694 523 686Q523 679 450 384T375 83T374 68Q374 26 402 26Q411 27 422 35Q443 55 463 131Q469 151 473 152Q475 153 483 153H487H491Q506 153 506 145Q506 140 503 129Q490 79 473 48T445 8T417 -8Q409 -10 393 -10Q359 -10 336 5T306 36L300 51Q299 52 296 50Q294 48 292 46Q233 -10 172 -10Q117 -10 75 30T33 157Q33 205 53 255T101 341Q148 398 195 420T280 442Q336 442 364 400Q369 394 369 396Q370 400 396 505T424 616Q424 629 417 632T378 637H357Q351 643 351 645T353 664Q358 683 366 683ZM352 326Q329 405 277 405Q242 405 210 374T160 293Q131 214 119 129Q119 126 119 118T118 106Q118 61 136 44T179 26Q233 26 290 98L298 109L352 326Z"></path></g><g data-mml-node="mi" transform="translate(553,-150) scale(0.707)"><path data-c="1D458" d="M121 647Q121 657 125 670T137 683Q138 683 209 688T282 694Q294 694 294 686Q294 679 244 477Q194 279 194 272Q213 282 223 291Q247 309 292 354T362 415Q402 442 438 442Q468 442 485 423T503 369Q503 344 496 327T477 302T456 291T438 288Q418 288 406 299T394 328Q394 353 410 369T442 390L458 393Q446 405 434 405H430Q398 402 367 380T294 316T228 255Q230 254 243 252T267 246T293 238T320 224T342 206T359 180T365 147Q365 130 360 106T354 66Q354 26 381 26Q429 26 459 145Q461 153 479 153H483Q499 153 499 144Q499 139 496 130Q455 -11 378 -11Q333 -11 305 15T277 90Q277 108 280 121T283 145Q283 167 269 183T234 206T200 217T182 220H180Q168 178 159 139T145 81T136 44T129 20T122 7T111 -2Q98 -11 83 -11Q66 -11 57 -1T48 16Q48 26 85 176T158 471L195 616Q196 629 188 632T149 637H144Q134 637 131 637T124 640T121 647Z"></path></g></g></g></g></svg></mjx-container></span>，除以<span class="math inline"><mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.372ex;" xmlns="http://www.w3.org/2000/svg" width="4.128ex" height="2.398ex" role="img" focusable="false" viewBox="0 -895.6 1824.4 1060"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="msqrt"><g transform="translate(853,0)"><g data-mml-node="msub"><g data-mml-node="mi"><path data-c="1D451" d="M366 683Q367 683 438 688T511 694Q523 694 523 686Q523 679 450 384T375 83T374 68Q374 26 402 26Q411 27 422 35Q443 55 463 131Q469 151 473 152Q475 153 483 153H487H491Q506 153 506 145Q506 140 503 129Q490 79 473 48T445 8T417 -8Q409 -10 393 -10Q359 -10 336 5T306 36L300 51Q299 52 296 50Q294 48 292 46Q233 -10 172 -10Q117 -10 75 30T33 157Q33 205 53 255T101 341Q148 398 195 420T280 442Q336 442 364 400Q369 394 369 396Q370 400 396 505T424 616Q424 629 417 632T378 637H357Q351 643 351 645T353 664Q358 683 366 683ZM352 326Q329 405 277 405Q242 405 210 374T160 293Q131 214 119 129Q119 126 119 118T118 106Q118 61 136 44T179 26Q233 26 290 98L298 109L352 326Z"></path></g><g data-mml-node="mi" transform="translate(553,-150) scale(0.707)"><path data-c="1D458" d="M121 647Q121 657 125 670T137 683Q138 683 209 688T282 694Q294 694 294 686Q294 679 244 477Q194 279 194 272Q213 282 223 291Q247 309 292 354T362 415Q402 442 438 442Q468 442 485 423T503 369Q503 344 496 327T477 302T456 291T438 288Q418 288 406 299T394 328Q394 353 410 369T442 390L458 393Q446 405 434 405H430Q398 402 367 380T294 316T228 255Q230 254 243 252T267 246T293 238T320 224T342 206T359 180T365 147Q365 130 360 106T354 66Q354 26 381 26Q429 26 459 145Q461 153 479 153H483Q499 153 499 144Q499 139 496 130Q455 -11 378 -11Q333 -11 305 15T277 90Q277 108 280 121T283 145Q283 167 269 183T234 206T200 217T182 220H180Q168 178 159 139T145 81T136 44T129 20T122 7T111 -2Q98 -11 83 -11Q66 -11 57 -1T48 16Q48 26 85 176T158 471L195 616Q196 629 188 632T149 637H144Q134 637 131 637T124 640T121 647Z"></path></g></g></g><g data-mml-node="mo" transform="translate(0,35.6)"><path data-c="221A" d="M95 178Q89 178 81 186T72 200T103 230T169 280T207 309Q209 311 212 311H213Q219 311 227 294T281 177Q300 134 312 108L397 -77Q398 -77 501 136T707 565T814 786Q820 800 834 800Q841 800 846 794T853 782V776L620 293L385 -193Q381 -200 366 -200Q357 -200 354 -197Q352 -195 256 15L160 225L144 214Q129 202 113 190T95 178Z"></path></g><rect width="971.4" height="60" x="853" y="775.6"></rect></g></g></g></svg></mjx-container></span>，将分布的方差纠正回接近 1</li>
</ul></li>
<li><p>并行化的体现</p>
<ul>
<li>序列计算多头注意力</li>
</ul></li>
<li><p>影响计算量的因素</p>
<ul>
<li>序列长度：点积、矩阵乘</li>
<li>头数量</li>
</ul></li>
<li><p>优势</p>
<ul>
<li>并行处理整个序列</li>
<li>长距离依赖</li>
</ul></li>
<li><p>缺点</p>
<ul>
<li>计算量大</li>
<li>超参调优</li>
<li>超长序列处理能力</li>
</ul></li>
</ul>
<h4 id="cross-attention">Cross-Attention</h4>
<h4 id="convolutional-attention">Convolutional Attention</h4>
<ul>
<li>SENet -CBAM</li>
</ul>
<h3 id="基础模型">基础模型</h3>
<h4 id="前馈神经网络">前馈神经网络</h4>
<h4 id="卷积神经网络cnn">卷积神经网络（CNN）</h4>
<h4 id="循环神经网络rnn">循环神经网络（RNN）</h4>
<h4 id="长短时记忆网络lstm">长短时记忆网络（LSTM）</h4>
<h3 id="模型调优">模型调优</h3>
<h4 id="模型优化">模型优化</h4>
<!--
细小裂缝检测与原型特征增强:
你是如何实现原型特征增强的？这种方法如何帮助模型捕捉细小裂缝的特征？
在提升对细小裂缝检测准确率方面，你采取了哪些评估指标？如何衡量改进效果？ -->
<!-- 图像预处理与重建技术:
详细解释一下Diffusion模型在隧道图像重建中的具体应用，它是如何减少噪声和补偿信息损失的？
低通滤波器的选择标准是什么？如何确定最佳的滤波参数？
光照嵌入是如何实现的？它如何帮助改善图像的亮度控制和质量？ -->
<h4 id="正则化">正则化</h4>
<ul>
<li>L1 正则化</li>
<li>L2 正则化</li>
</ul>
<h4 id="损失函数">损失函数</h4>
<p>已知 softmax 输出概率序列与实际分布概率序列，计算两者交叉熵</p>
<h4 id="超参数调整">超参数调整</h4>
<!--
如何进行模型的交叉验证？有没有进行过超参数调优？过程和结果如何？ -->
<p>在深度学习中，超参数（Hyperparameters）是指在训练开始前设置的模型参数，不是通过训练学习得到的。超参数的选择对模型性能有很大的影响，不同的超参数设置可能导致显著不同的训练结果。</p>
<h4 id="优化器选择">优化器选择</h4>
<ul>
<li>SGD</li>
<li>AdaGrad</li>
<li>RMSProp</li>
<li>Adam</li>
</ul>
<h4 id="学习率衰减">学习率衰减</h4>
<p>LR 中的连续值特征是如何处理的 为什么 LR 要先对数据进行归一化处理 LR
用了 sigmoid 函数，那么 LR 是线性模型还是非线性模型，为什么</p>
<ul>
<li>线性</li>
<li>分段</li>
<li>余弦</li>
<li>WarmUp</li>
<li>周期性</li>
</ul>
<h4 id="缩放法则-scaling-law">缩放法则 Scaling-Law</h4>
<p>在 AI
领域中，描述模型性能如何随着模型规模（如<strong>参数数量、训练数据量、计算资源</strong>等）变化而变化的一组经验法则</p>
<ul>
<li><p>应用</p>
<ol type="1">
<li><p>设计更大规模的模型</p>
<p>指导研究人员如何设计和训练更大规模的模型，以实现更高的性能</p></li>
<li><p>优化资源分配</p>
<p>如确定是否应增加模型参数数量、增加训练数据量，还是增加计算资源，以实现最优的性能提升</p></li>
<li><p>预测性能</p>
<p>根据现有模型的性能和缩放法则，可以预测更大规模模型的性能</p></li>
</ol></li>
</ul>
<h3 id="常见模型评估指标">常见模型评估指标</h3>
<!-- 你采用了哪些方法来验证模型性能？精度、召回率、F1分数等评价指标的具体数值是多少？ -->
<h4 id="准确率">准确率</h4>
<h4 id="召回率">召回率</h4>
<h4 id="f1-分数">F1 分数</h4>
<h4 id="浮点数运算次数-flops">浮点数运算次数 FLOPs</h4>
<h4 id="帧每秒-fps">帧每秒 FPS</h4>
<h2 id="深度学习框架">深度学习框架</h2>
<h3 id="训练范式">训练范式</h3>
<h3 id="pytorch">PyTorch</h3>
<h4 id="tensor">Tensor</h4>
<ul>
<li><p>存储</p>
<ul>
<li>头信息区（Tensor）：tensor
的形状（size）、步长（stride）、数据类型（type）等</li>
<li>存储区（Storage）：数据</li>
</ul></li>
<li><p>stride 属性</p>
<p>指定维度中一个元素到下一个元素的步长</p></li>
<li><p>view 方法</p>
<p>返回共享存储区的 tensor</p></li>
<li><p>计算图</p></li>
<li><p>环境搭建</p></li>
<li><p>数据加载</p></li>
<li><p>模型定义</p></li>
<li><p>训练</p></li>
<li><p>验证</p></li>
<li><p>保存</p></li>
<li><p>加载模型</p></li>
</ul>
<h3 id="pytorchlightning">PytorchLightning</h3>
<!-- 为什么选择PytorchLightning作为开发框架？它为你的项目带来了哪些便利？
请详细介绍你的模型架构，包括所使用的网络层、损失函数的选择以及优化器的配置。
在使用PytorchLightning时，如何进行模型的训练、验证和测试阶段的管理？ -->
<h2 id="逻辑思维与项目经验">逻辑思维与项目经验</h2>
<h3 id="逻辑思维">逻辑思维</h3>
<p>准备通过解决实际问题来展示你的逻辑思维能力和数据分析洞察力，可以是以往项目中的案例分析。</p>
<h3 id="团队合作与挑战接受度">团队合作与挑战接受度</h3>
<p>思考并准备实例说明你如何在团队中有效沟通、协作解决问题，以及面对技术挑战时的态度和解决策略。</p>
</div></article></div></div><div class="column column-left is-4-tablet is-4-desktop is-3-widescreen  order-1"><div class="card widget" data-type="profile"><div class="card-content"><nav class="level"><div class="level-item has-text-centered flex-shrink-1"><div><figure class="image is-128x128 mx-auto mb-2"><img class="avatar is-rounded" src="https://raw.githubusercontent.com/derolol/picgo/main/images/avatar.png" alt="Derolol"></figure><p class="title is-size-4 is-block" style="line-height:inherit;">Derolol</p><p class="is-size-6 is-block">Student</p><p class="is-size-6 is-flex justify-content-center"><i class="fas fa-map-marker-alt mr-1"></i><span>China</span></p></div></div></nav><nav class="level is-mobile"><div class="level-item has-text-centered is-marginless"><div><p class="heading">Posts</p><a href="/archives"><p class="title">52</p></a></div></div><div class="level-item has-text-centered is-marginless"><div><p class="heading">Categories</p><a href="/categories"><p class="title">6</p></a></div></div><div class="level-item has-text-centered is-marginless"><div><p class="heading">Tags</p><a href="/tags"><p class="title">45</p></a></div></div></nav><div class="level"><a class="level-item button is-primary is-rounded" href="https://github.com/derolol" target="_blank" rel="noopener">Follow</a></div><div class="level is-mobile is-multiline"><a class="level-item button is-transparent is-marginless" target="_blank" rel="noopener" title="Github" href="https://github.com/derolol"><i class="fab fa-github"></i></a><a class="level-item button is-transparent is-marginless" target="_blank" rel="noopener" title="Facebook" href="https://facebook.com"><i class="fab fa-facebook"></i></a><a class="level-item button is-transparent is-marginless" target="_blank" rel="noopener" title="Twitter" href="https://twitter.com"><i class="fab fa-twitter"></i></a><a class="level-item button is-transparent is-marginless" target="_blank" rel="noopener" title="Dribbble" href="https://dribbble.com"><i class="fab fa-dribbble"></i></a><a class="level-item button is-transparent is-marginless" target="_blank" rel="noopener" title="RSS" href="/"><i class="fas fa-rss"></i></a></div></div></div><div class="card widget" data-type="categories"><div class="card-content"><div class="menu"><h3 class="menu-label">Categories</h3><ul class="menu-list"><li><a class="level is-mobile" href="/categories/algorithm/"><span class="level-start"><span class="level-item">algorithm</span></span><span class="level-end"><span class="level-item tag">16</span></span></a></li><li><a class="level is-mobile" href="/categories/interview/"><span class="level-start"><span class="level-item">interview</span></span><span class="level-end"><span class="level-item tag">5</span></span></a></li><li><a class="level is-mobile" href="/categories/paper/"><span class="level-start"><span class="level-item">paper</span></span><span class="level-end"><span class="level-item tag">7</span></span></a></li><li><a class="level is-mobile" href="/categories/source-code/"><span class="level-start"><span class="level-item">source code</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/categories/%E8%AE%A1%E7%AE%97%E6%9C%BA%E5%9F%BA%E7%A1%80/"><span class="level-start"><span class="level-item">计算机基础</span></span><span class="level-end"><span class="level-item tag">2</span></span></a></li><li><a class="level is-mobile" href="/categories/%E9%9D%A2%E8%AF%95/"><span class="level-start"><span class="level-item">面试</span></span><span class="level-end"><span class="level-item tag">2</span></span></a></li></ul></div></div></div><div class="card widget" data-type="links"><div class="card-content"><div class="menu"><h3 class="menu-label">Links</h3><ul class="menu-list"><li><a class="level is-mobile" href="https://hexo.io" target="_blank" rel="noopener"><span class="level-left"><span class="level-item">Hexo</span></span><span class="level-right"><span class="level-item tag">hexo.io</span></span></a></li><li><a class="level is-mobile" href="https://bulma.io" target="_blank" rel="noopener"><span class="level-left"><span class="level-item">Bulma</span></span><span class="level-right"><span class="level-item tag">bulma.io</span></span></a></li><li><a class="level is-mobile" href="https://ayyha.github.io/" target="_blank" rel="noopener"><span class="level-left"><span class="level-item">AyyHA</span></span><span class="level-right"><span class="level-item tag">ayyha.github.io</span></span></a></li></ul></div></div></div><div class="card widget" data-type="tags"><div class="card-content"><div class="menu"><h3 class="menu-label">Tags</h3><div class="field is-grouped is-grouped-multiline"><div class="control"><a class="tags has-addons" href="/tags/CL/"><span class="tag">CL</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/CVPR2022/"><span class="tag">CVPR2022</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/CVPR2023/"><span class="tag">CVPR2023</span><span class="tag">3</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Data-Synthesis/"><span class="tag">Data Synthesis</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Data-enhancement/"><span class="tag">Data enhancement</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/GAN/"><span class="tag">GAN</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Multimodal/"><span class="tag">Multimodal</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/NLP/"><span class="tag">NLP</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Prototype/"><span class="tag">Prototype</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Transformer/"><span class="tag">Transformer</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/ZSL/"><span class="tag">ZSL</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/algorithm/"><span class="tag">algorithm</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/code/"><span class="tag">code</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/interview/"><span class="tag">interview</span><span class="tag">5</span></a></div><div class="control"><a class="tags has-addons" href="/tags/leetcode/"><span class="tag">leetcode</span><span class="tag">15</span></a></div><div class="control"><a class="tags has-addons" href="/tags/prompt/"><span class="tag">prompt</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/pytorch/"><span class="tag">pytorch</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/tree/"><span class="tag">tree</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/%E4%BA%8C%E5%88%86%E6%B3%95/"><span class="tag">二分法</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/%E4%BC%81%E4%B8%9A/"><span class="tag">企业</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/%E5%8A%A8%E6%80%81%E8%A7%84%E5%88%92/"><span class="tag">动态规划</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/%E5%8C%BA%E9%97%B4/"><span class="tag">区间</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/%E5%8C%BA%E9%97%B4%E5%90%88%E5%B9%B6/"><span class="tag">区间合并</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/%E5%8F%8C%E6%8C%87%E9%92%88/"><span class="tag">双指针</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/%E5%93%88%E5%B8%8C%E8%A1%A8/"><span class="tag">哈希表</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/%E5%A0%86/"><span class="tag">堆</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/%E5%A4%9A%E6%A8%A1%E6%80%81/"><span class="tag">多模态</span><span class="tag">2</span></a></div><div class="control"><a class="tags has-addons" href="/tags/%E5%A4%A7%E7%96%86/"><span class="tag">大疆</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/%E5%AD%97%E7%AC%A6%E4%B8%B2/"><span class="tag">字符串</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/%E6%89%A9%E6%95%A3%E6%A8%A1%E5%9E%8B/"><span class="tag">扩散模型</span><span class="tag">2</span></a></div><div class="control"><a class="tags has-addons" href="/tags/%E6%8E%92%E5%BA%8F%E8%BE%85%E5%8A%A9/"><span class="tag">排序辅助</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F/"><span class="tag">操作系统</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/%E6%95%B0%E5%AD%A6%E6%8E%A8%E7%90%86/"><span class="tag">数学推理</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/%E6%96%87%E7%94%9F%E5%9B%BE/"><span class="tag">文生图</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/%E6%A0%88/"><span class="tag">栈</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/%E6%A0%91/"><span class="tag">树</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/%E6%AD%A3%E5%88%99%E5%8C%B9%E9%85%8D/"><span class="tag">正则匹配</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/%E6%BB%91%E5%8A%A8%E7%AA%97%E5%8F%A3/"><span class="tag">滑动窗口</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/%E7%9F%A9%E9%98%B5%E5%A4%84%E7%90%86/"><span class="tag">矩阵处理</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/%E7%AC%94%E8%AF%95/"><span class="tag">笔试</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/%E7%BB%BC%E6%B5%8B/"><span class="tag">综测</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/%E8%AE%A1%E7%AE%97%E6%9C%BA%E5%9F%BA%E7%A1%80/"><span class="tag">计算机基础</span><span class="tag">2</span></a></div><div class="control"><a class="tags has-addons" href="/tags/%E8%AF%AD%E8%A8%80/"><span class="tag">语言</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/%E9%93%BE%E8%A1%A8/"><span class="tag">链表</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/%E9%A3%8E%E6%A0%BC%E8%BF%81%E7%A7%BB/"><span class="tag">风格迁移</span><span class="tag">1</span></a></div></div></div></div></div><div class="column-right-shadow is-hidden-widescreen is-sticky"></div></div><div class="column column-right is-4-tablet is-4-desktop is-3-widescreen is-hidden-touch is-hidden-desktop-only order-3 is-sticky"><!--!--><div class="card widget" data-type="recent-posts"><div class="card-content"><h3 class="menu-label">Recents</h3><article class="media"><div class="media-content"><p class="date"><time dateTime="2024-09-26T02:29:09.000Z">2024-09-26</time></p><p class="title"><a href="/2024/09/26/interview/experience/">面试经验</a></p><p class="categories"><a href="/categories/interview/">interview</a></p></div></article><article class="media"><div class="media-content"><p class="date"><time dateTime="2024-09-23T15:19:31.000Z">2024-09-23</time></p><p class="title"><a href="/2024/09/23/interview/shenxinfu/">深信服算法面试整理</a></p></div></article><article class="media"><div class="media-content"><p class="date"><time dateTime="2024-09-19T16:41:25.000Z">2024-09-20</time></p><p class="title"><a href="/2024/09/20/knowledge/image-process/">图像处理</a></p></div></article><article class="media"><div class="media-content"><p class="date"><time dateTime="2024-09-11T08:42:52.000Z">2024-09-11</time></p><p class="title"><a href="/2024/09/11/project/rocm/">AMD GPU MI210 深度学习疑难杂症</a></p></div></article><article class="media"><div class="media-content"><p class="date"><time dateTime="2024-09-05T15:09:24.000Z">2024-09-05</time></p><p class="title"><a href="/2024/09/05/algorithm/interval/">区间题目集合</a></p><p class="categories"><a href="/categories/algorithm/">algorithm</a></p></div></article></div></div></div></div></div></section><footer class="footer"><div class="container"><div class="level"><div class="level-start"><a class="footer-logo is-block mb-2" href="/"><img src="https://raw.githubusercontent.com/derolol/picgo/main/images/head.png" alt="Derolol Blog" height="28"></a><p class="is-size-7"><span>&copy; 2024 derolol</span>  Powered by <a href="https://hexo.io/" target="_blank" rel="noopener">Hexo</a> &amp; <a href="https://github.com/ppoffice/hexo-theme-icarus" target="_blank" rel="noopener">Icarus</a><br><span id="busuanzi_container_site_uv">Visited by <span id="busuanzi_value_site_uv">0</span> users</span></p><p class="is-size-7">© 2019</p></div><div class="level-end"><div class="field has-addons"><p class="control"><a class="button is-transparent is-large" target="_blank" rel="noopener" title="Creative Commons" href="https://creativecommons.org/"><i class="fab fa-creative-commons"></i></a></p><p class="control"><a class="button is-transparent is-large" target="_blank" rel="noopener" title="Attribution 4.0 International" href="https://creativecommons.org/licenses/by/4.0/"><i class="fab fa-creative-commons-by"></i></a></p><p class="control"><a class="button is-transparent is-large" target="_blank" rel="noopener" title="Download on GitHub" href="https://github.com/ppoffice/hexo-theme-icarus"><i class="fab fa-github"></i></a></p></div></div></div></div></footer><script src="https://cdn.jsdelivr.net/npm/jquery@3.3.1/dist/jquery.min.js"></script><script src="https://cdn.jsdelivr.net/npm/moment@2.22.2/min/moment-with-locales.min.js"></script><script src="https://cdn.jsdelivr.net/npm/clipboard@2.0.4/dist/clipboard.min.js" defer></script><script>moment.locale("en");</script><script>var IcarusThemeSettings = {
            article: {
                highlight: {
                    clipboard: true,
                    fold: 'unfolded'
                }
            }
        };</script><script src="/js/column.js"></script><script src="/js/animation.js"></script><a id="back-to-top" title="Back to top" href="javascript:;"><i class="fas fa-chevron-up"></i></a><script src="/js/back_to_top.js" defer></script><!--!--><!--!--><!--!--><!--!--><script src="https://cdn.jsdelivr.net/npm/cookieconsent@3.1.1/build/cookieconsent.min.js" defer></script><script>window.addEventListener("load", () => {
      window.cookieconsent.initialise({
        type: "info",
        theme: "edgeless",
        static: false,
        position: "bottom-left",
        content: {
          message: "This website uses cookies to improve your experience.",
          dismiss: "Got it!",
          allow: "Allow cookies",
          deny: "Decline",
          link: "Learn more",
          policy: "Cookie Policy",
          href: "https://www.cookiesandyou.com/",
        },
        palette: {
          popup: {
            background: "#edeff5",
            text: "#838391"
          },
          button: {
            background: "#4b81e8"
          },
        },
      });
    });</script><script src="https://cdn.jsdelivr.net/npm/lightgallery@1.10.0/dist/js/lightgallery.min.js" defer></script><script src="https://cdn.jsdelivr.net/npm/justifiedGallery@3.8.1/dist/js/jquery.justifiedGallery.min.js" defer></script><script>window.addEventListener("load", () => {
            if (typeof $.fn.lightGallery === 'function') {
                $('.article').lightGallery({ selector: '.gallery-item' });
            }
            if (typeof $.fn.justifiedGallery === 'function') {
                if ($('.justified-gallery > p > .gallery-item').length) {
                    $('.justified-gallery > p > .gallery-item').unwrap();
                }
                $('.justified-gallery').justifiedGallery();
            }
        });</script><!--!--><!--!--><!--!--><!--!--><!--!--><script src="/js/main.js" defer></script><div class="searchbox"><div class="searchbox-container"><div class="searchbox-header"><div class="searchbox-input-container"><input class="searchbox-input" type="text" placeholder="Type something..."></div><a class="searchbox-close" href="javascript:;">×</a></div><div class="searchbox-body"></div></div></div><script src="/js/insight.js" defer></script><script>document.addEventListener('DOMContentLoaded', function () {
            loadInsight({"contentUrl":"/content.json"}, {"hint":"Type something...","untitled":"(Untitled)","posts":"Posts","pages":"Pages","categories":"Categories","tags":"Tags"});
        });</script></body></html>