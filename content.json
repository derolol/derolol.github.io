{"posts":[{"title":"ANCL","text":"Auxiliary Network Continual Learning (ANCL) Paper: Achieving a Better Stability-Plasticity Trade-off via Auxiliary Networks in Continual Learning Authors: Sanghwan Kim ; Lorenzo Noci ; Antonio Orvieto ; Thomas Hofmann Code: https://github.com/kim-sanghwan/ANCL Framework: Continual Learning (CL) 持续学习 符号定义 PT：Previous Task CT：Current Task 含义 保留PT信息的同时，继续在CT中进行学习 难点：Catastrophic Forgetting 灾难性的遗忘 对于梯度更新学习的模型，在学习CT的过程中更倾向于覆盖PT学习的梯度 换而言之，Stability-Plasticity Dilemma Martial Mermillod, Aur ́ elia Bugaiska, and Patrick Bonin. The stability-plasticity dilemma: Investigating the continuum from catastrophic forgetting to age-limited learning effects, 2013. 1 Stability: 在PT具有较好的泛化能力 Plasticity: 在CT学习新概念 所以，如何平衡Stability和Plasticity是研究的重点 任务分类 类别增量学习(Class-Incremental Learning)的前世今生、开源工具包 Task Incremental Learning (TIL)：训练和测试阶段均为模型提供当前任务标识 Domain Incremental Learning (DIL)：测试阶段不提供当前任务标识 Class Incremental Learning (CIL)：测试阶段自动识别当前任务标识和分类 学习难度逐渐增加，ANCL在TIL和CIL设置中进行了评估 相关工作 增加Auxiliary Network或Extra Module Active Forgetting with synaptic Expansion-Convergence (AFEC) 超参控制新旧参数的融合 当前工作 框架化使用Auxiliary Network的CL，使得Auxiliary Network插件化 通过和调整正则化项 局限 不同方法依赖于不同的超参 参考 [1] 类别增量学习(Class-Incremental Learning)的前世今生、开源工具包","link":"/2024/06/20/ANCL/"},{"title":"Bi-VAEGAN","text":"Bi-VAEGAN Paper: Bi-directional Distribution Alignment for Transductive Zero-Shot Learning Authors: Zhicai Wang, Yanbin Hao, Tingting Mu, Ouxiang Li, Shuo Wang, Xiangnan He Code: https://github.com/Zhicaiwww/Bi-VAEGAN Framework: Zero-shot Learning (ZSL) Hugo Larochelle, Dumitru Erhan, and Yoshua Bengio. Zerodata learning of new tasks. In AAAI, volume 1, page 3, 2008. 1 目标 解决训练时缺少例子或标签的问题 Conventional ZSL / Inductive ZSL 核心挑战 在存在Class Relevance的条件下，使得分类器能从Seen Classes提取信息迁移到Unseen Classes当中 Class Relevance通常作为Auxiliary Data提供 Auxiliary Data可以为人工标注、文字描述、知识图谱或Formal Description of Knowledge（如嵌入向量） Domain Shift Problem 仅从Auxiliary Data学习容易导致Unseen Classes的真实分布与其建模分布之间存在差异 Proposed: Transductive ZSL (TZSL) 允许在训练中额外加入为目标类别收集的无标签示例 Generative Models 作用 Synthesize Examples 合成样本 Learn the Unseen Data Distribution 学习unseen数据分布 分类 Unconditional Generation Conditional Generation Auxiliary信息是信息量更丰富的类标签，通过Auxiliary信息作为Condition，可以学习到Data-Auxiliary联合分布，这连接了Visual空间和Auxiliary空间，使得生成器具有信息迁移的能力 难点 将seen classes所学迁移到unseen classes f-VAEGAN 提出方法 Transductive Regressor Normalization Class Prior Estimation (CPE) 架构 VAE编码器，得到维隐藏表征向量 条件生成器，以类别属性为条件，从正态分布采样维向量用于视觉特征生成 Wasserstein GAN（WGAN）的判别器，用于seen classes WGAN的判别器，用于unseen classes 映射视觉空间到特征空间的Regressor WGAN的判别器，用于特征判别 Workflow Alt text Level-1 和对抗性训练 Level-2 和、对抗性训练","link":"/2024/06/21/Bi-VAEGAN/"},{"title":"Code of Pixel-to-Prototype Constrast","text":"Generate CAMs Feature map Class feature map Score of class CAMs Pixel-to-Prototype Contrast Pseudo mask Pixel-wise projected feature Pixel-to-prototype contrast Prototype set Temperature Contrast 像素特征与原型的相似度 Prototype Estimation in Batch Top K pixels of class c CAM as confidences Estimate prototypes from pixel-wise feature embeddings that are with the top K confidences Prototype Loss Cross Prototype Contrast Cross CAM Contrast Intra-view Contrast Strategy to slove the matter of in accurate pseudo label [50] Semi-hard prototype mining Hard pixel sampling Code 归一化 归一化 作用 保证所有元素之和为1 将向量转换为概率分布 归一化 1234# 按通道执行L2归一化v = v / (torch.norm(v, dim=1, keepdim=True) + 1e-5)# orv = torch.nn.functional.normalize(v, dim=1) 作用 方向不变性：向量的方向不变，长度变为1，使得向量表示不再依赖于其大小 数值稳定性：将向量的大小规范在一个相对较小的区间 减小特征尺度的差异 便于执行相似性度量 Max归一化 归一化后向量的最大值为1 Max-Min归一化 归一化后向量值范围为[0, 1] Forward cam 1234# fea是最后一层输出的特征图self.fc8 = nn.Conv2d(4096, 21, 1, bias=False)cam = self.fc8(fea)cam = torch.nn.functional.interpolate(cam, (H, W), mode='bilinear', align_corners=True) cam_rv_down 清洗CAM 12345678910with torch.no_grad(): cam_d = torch.nn.functional.relu(cam.detach()) # max norm cam_d_max = torch.max(cam_d.view(n, c, -1), dim=-1)[0].view(n, c, 1, 1)+1e-5 cam_d_norm = torch.nn.functional.relu(cam_d - 1e-5) / cam_d_max # 计算保留概率值最大分类，反相为背景概率，其余分类置0 cam_d_norm[:, 0, :, :] = 1 - torch.max(cam_d_norm[:, 1:, :, :], dim=1)[0] cam_max = torch.max(cam_d_norm[:,1:,:,:], dim=1, keepdim=True)[0] cam_d_norm[:,1:,:,:][cam_d_norm[:,1:,:,:] &lt; cam_max] = 0 增强CAM 1234567891011121314151617181920# 根据像素相似度调整CAMcam_rv_down = self.PCM(cam_d_norm, f)# PCMdef PCM(self, cam, f): n,c,h,w = f.size() cam = torch.nn.functional.interpolate(cam, (h,w), mode='bilinear', align_corners=True).view(n,-1,h*w) # 多尺度特征融合 f = self.f9(f) f = f.view(n, -1, h*w) # 特征按通道L2归一化 f = f / (torch.norm(f, dim=1, keepdim=True) + 1e-5) # 计算像素相似度矩阵 aff = torch.nn.functional.relu(torch.matmul(f.transpose(1, 2), f), inplace=True) # 相似度矩阵L1归一化 aff = aff/(torch.sum(aff, dim=1, keepdim=True) + 1e-5) # CAM加权 cam_rv = torch.matmul(cam, aff).view(n, -1, h, w) return cam_rv cam_rv 1cam_rv = torch.nn.functional.interpolate(cam_rv_down, (H,W), mode='bilinear', align_corners=True) f_proj 12self.fc_proj = torch.nn.Conv2d(4096, 128, 1, bias=False)f_proj = torch.nn.functional.relu(self.fc_proj(fea), inplace=True) prototype 12345678910111213141516171819202122232425262728293031323334353637f_proj1 = torch.nn.functional.interpolate(f_proj1, size=(128 // 8, 128 // 8), mode='bilinear', align_corners=True)cam_rv1_down = torch.nn.functional.interpolate(cam_rv1_down, size=(128 // 8, 128 // 8), mode='bilinear', align_corners=True)cam_rv2_down = cam_rv2_downwith torch.no_grad(): fea1 = f_proj1.detach() c_fea1 = fea1.shape[1] cam_rv1_down = torch.nn.functional.relu(cam_rv1_down.detach()) # CAM Max-min归一化 n1, c1, h1, w1 = cam_rv1_down.shape max1 = torch.max(cam_rv1_down.view(n1, c1, -1), dim=-1)[0].view(n1, c1, 1, 1) min1 = torch.min(cam_rv1_down.view(n1, c1, -1), dim=-1)[0].view(n1, c1, 1, 1) cam_rv1_down[cam_rv1_down &lt; min1 + 1e-5] = 0. norm_cam1 = (cam_rv1_down - min1 - 1e-5) / (max1 - min1 + 1e-5) cam_rv1_down = norm_cam1 # 设置背景阈值 cam_rv1_down[:, 0, :, :] = args.bg_threshold # 根据图像级标签保留相应的类别 scores1 = torch.nn.functional.softmax(cam_rv1_down * label, dim=1) # 计算伪标签 pseudo_label1 = scores1.argmax(dim=1, keepdim=True) n_sc1, c_sc1, h_sc1, w_sc1 = scores1.shape scores1 = scores1.transpose(0, 1) fea1 = fea1.permute(0, 2, 3, 1).reshape(-1, c_fea1) # 获取各个分类CAM值最高的值与索引 top_values, top_indices = torch.topk(cam_rv1_down.transpose(0, 1).reshape(c_sc1, -1), k=h_sc1 * w_sc1 // 8, dim=-1) prototypes1 = torch.zeros(c_sc1, c_fea1).cuda() # [21, 128] # 遍历各个分类 for i in range(c_sc1): # 获取k个像素对应的特征 top_fea = fea1[top_indices[i]] # CAM值加权平均k个特征得到分类原型 prototypes1[i] = torch.sum(top_values[i].unsqueeze(-1) * top_fea, dim=0) / torch.sum(top_values[i]) # 各个原型L2归一化 prototypes1 = torch.nn.functional.normalize(prototypes1, dim=-1) prototype similarity 12345678910111213141516171819202122232425n_f, c_f, h_f, w_f = f_proj1.shape# [N, H, W, C] -&gt; [N x H x W, C]f_proj1 = f_proj1.permute(0, 2, 3, 1).reshape(n_f * h_f * w_f, c_f)# 特征L2归一化f_proj1 = torch.nn.functional.normalize(f_proj1, dim=-1)pseudo_label1 = pseudo_label1.reshape(-1)positives1 = prototypes2[pseudo_label1]negitives1 = prototypes2# for targetn_f, c_f, h_f, w_f = f_proj2.shapef_proj2 = f_proj2.permute(0, 2, 3, 1).reshape(n_f * h_f * w_f, c_f)f_proj2 = torch.nn.functional.normalize(f_proj2, dim=-1)pseudo_label2 = pseudo_label2.reshape(-1)positives2 = prototypes1[pseudo_label2]negitives2 = prototypes1A1 = torch.exp(torch.sum(f_proj1 * positives1, dim=-1) / 0.1)A2 = torch.sum(torch.exp(torch.matmul(f_proj1, negitives1.transpose(0, 1)) / 0.1), dim=-1)loss_nce1 = torch.mean(-1 * torch.log(A1 / A2))A3 = torch.exp(torch.sum(f_proj2 * positives2, dim=-1) / 0.1)A4 = torch.sum(torch.exp(torch.matmul(f_proj2, negitives2.transpose(0, 1)) / 0.1), dim=-1)loss_nce2 = torch.mean(-1 * torch.log(A3 / A4))loss_cross_nce = 0.1 * (loss_nce1 + loss_nce2) / 2","link":"/2023/11/14/Code-of-Pixel-to-Prototype-Constrast/"},{"title":"BiFormer","text":"Paper: BiFormer: Vision Transformer with Bi-Level Routing Attention Authors: Lei Zhu, Xinjiang Wang, Zhanghan Ke, Wayne Zhang, Rynson Lau Code: https://github.com/rayleizhu/BiFormer Framework: Dynamic Sparse Attention via bi-level routing","link":"/2024/06/26/biformer/"},{"title":"平衡二叉搜索树（BST）","text":"性质 若其左子树不为NULL，则左子树上所有节点的值都＜根节点的值 若其右子树不为NULL，则右子树上所有节点的值都＞根节点的值 其左右子树也分别是二叉搜索树 相关题目 leetcode108：将有序数组转换为二叉搜索树","link":"/2024/06/24/binary-sort-tree/"},{"title":"leetcode1823 找出游戏的获胜者","text":"思路 单链表模拟 取余推测 https://leetcode.cn/problems/find-the-winner-of-the-circular-game/solutions/2362052/1823-zhao-chu-you-xi-de-huo-sheng-zhe-yu-tkst","link":"/2024/06/25/circular-game/"},{"title":"computer-network","text":"http中的get、post区别，是怎么做的","link":"/2024/06/26/computer-network/"},{"title":"Computer Vision","text":"【三年面试五年模拟】算法工程师的求职面试秘籍 从ReLU到GELU，一文概览神经网络的激活函数 https://github.com/DWCTOD/interview/blob/master/detail/%E4%BD%9C%E4%B8%9A%E5%B8%AE%20%E8%A7%86%E8%A7%89%E7%AE%97%E6%B3%95%E5%B7%A5%E7%A8%8B%E5%B8%88%20%E9%9D%A2%E7%BB%8F%EF%BC%882020%E5%B1%8A%EF%BC%89.md https://github.com/GYee/CV_interviews_Q-A 图像处理与计算机视觉基础 基本概念 原理 图像预处理 特征提取 对象检测 图像分类 图像分割 OpenCV库 读写图像 图像滤波 几何变换 特征检测与描述 深度学习基础 梯度下降 滑动平均 模型微调（Fine-tuning） 基础模块 池化层 Pooling Layer 归一化层 Normalization Layer BN，Batch Normalization IN，Instance Normalization LN，Layer Normalization GN，Group Normalization BN是怎么做，作用是什么 激活层 常见激活函数 Sigmoid Tanh ReLU LeakyReLU SoftPlus ELU SELU，自归一化 Swish，类Sigmoid作为开关 GELU GLU 特性 梯度消失 存在偏导过小 梯度爆炸 偏导累乘过大 梯度裁剪 输出均值为0能避免每次权重只能往单一反向变化 ReLU计算复杂度低 ReLU的负半轴为输出值增加稀疏性，减少计算量，但同时会让一些神经元不能更新 SoftPlus，ReLU的平滑 全连接层 Linear 嵌入层 Embedding 优化模块 空间金字塔池化（Spatial Pyramid Pooling，SPP） 空洞空间金字塔池化（Atrous Spatial Pyramid Pooling，ASPP） HDC 基础模型 前馈神经网络 卷积神经网络（CNN） 循环神经网络（RNN） 长短时记忆网络（LSTM） Inception 数据集建立 模型设计与调优 模型优化 正则化 L1正则化 L2正则化 损失函数 已知softmax输出概率序列与实际分布概率序列，计算两者交叉熵 超参数调整 在深度学习中，超参数（Hyperparameters）是指在训练开始前设置的模型参数，不是通过训练学习得到的。超参数的选择对模型性能有很大的影响，不同的超参数设置可能导致显著不同的训练结果。 优化器选择 SGD AdaGrad RMSProp Adam 学习率衰减 LR中的连续值特征是如何处理的 为什么LR要先对数据进行归一化处理 LR用了sigmoid函数，那么LR是线性模型还是非线性模型，为什么 线性 分段 余弦 WarmUp 周期性 常见模型评估指标 准确率 召回率 F1分数 浮点数运算次数 FLOPs 帧每秒 FPS 深度学习框架 训练范式 PyTorch 环境搭建 数据加载 模型定义 训练 验证 保存 加载模型 PytorchLightning 逻辑思维与项目经验 逻辑思维 准备通过解决实际问题来展示你的逻辑思维能力和数据分析洞察力，可以是以往项目中的案例分析。 团队合作与挑战接受度 思考并准备实例说明你如何在团队中有效沟通、协作解决问题，以及面对技术挑战时的态度和解决策略。","link":"/2024/06/26/cv/"},{"title":"data-structure","text":"多线程 异常处理 算法 排序 查找 动态规划 链表 树 二叉搜索树 Binary Search Tree 又称二叉排序树、二叉查找树 性质 一棵二叉树，可以为空；如果不为空，满足以下性质： 非空左子树的所有键值小于其根结点的键值。 非空右子树的所有键值大于其根结点的键值。 左、右子树都是二叉搜索树。 图","link":"/2024/06/26/data-structure/"},{"title":"database","text":"数据库 left join和right join、union是怎么操作的","link":"/2024/06/26/database/"},{"title":"deep-learning","text":"【三年面试五年模拟】算法工程师的求职面试秘籍 分类 Classification 将输入数据划分到预定义的有限标签中，输出为预测的类别标签 常用评价指标 准确率 精确率 召回率 F1分数 应用 花卉图像分类 垃圾邮件拦截 回归 Regression 建立数值型随机自变量的模型并进行连续的因变量预测，输出为数值 常用评价指标 均方误差 R2分数 应用 股票价格预测 房价预测 聚类 Clustering 将无标签的数据分成多个类（簇），确保类内样本相似，类间样本相异，其输出是聚类结果（簇划分，簇标签，簇中心等） 常用评价指标 样本紧密度 样本分隔度 应用 用户分群 异常检测 决策 Decision making 通过神经网络理解给定目标，约束条件和可用信息，预测出最佳或满意的动作决策，其输出是一连串的动作 常用评价指标 最终回报 平均奖励 应用 游戏AI 自动驾驶 概率密度估计 Probability density estimation 使用深度神经网络来估计一个随机变量或一组随机变量的概率密度函数，其输出是数据的概率分布 常用评价指标衡量分布差异 对数似然损失 KL散度 应用 数据生成 样本采样","link":"/2024/06/27/deep-learning/"},{"title":"leetcode29 两数相除","text":"思路 特殊情况处理 除数左移位，与被除数比较","link":"/2024/06/26/divide-two-integers/"},{"title":"Hello World","text":"Welcome to Hexo! This is your very first post. Check documentation for more info. If you get any problems when using Hexo, you can find the answer in troubleshooting or you can ask me on GitHub. Quick Start Create a new post 1$ hexo new \"My New Post\" More info: Writing Run server 1$ hexo server More info: Server Generate static files 1$ hexo generate More info: Generating Deploy to remote sites 1$ hexo deploy More info: Deployment","link":"/2024/06/24/hello-world/"},{"title":"hpc","text":"GPU编程与性能优化 CUDA与cuDNN GPU编程基本原理 特别是如何使用CUDA进行并行计算，以及cuDNN库在加速深度学习中的应用 内存管理 核函数设计 性能监控与调优 性能优化 掌握一些基本的性能分析工具和方法，比如使用nvprof或TensorFlow Profiler分析模型运行瓶颈，并实施相应的优化措施。","link":"/2024/06/26/hpc/"},{"title":"leetcode-host","text":"编程题： 逆时针打印数组 （剑指offer 和 leetcode54都有的常见题，常为顺时针打印数组） 给先序遍历重构二叉树 （例如输入为124XXX3XX，X表示空，无叶子节点） 有随机数0-2 0-3 0-4构建100的随机数 （使用0-3和0-4构建20与0-4构建的5形成100的随机数） 智力题： 49个人中至少几个人生日是同一月 两个人只握一次手，一共握了45次，问一共几个人（10人） 编程题： 数组合并（leetcode88）【简单】 区间合并，也叫线段合并（leetcode56）【中等】 以上内容+能否完全覆盖，题目为： 单个线段[2,6]可称为完全覆盖[4,6]，现有两组线段AB，每组中有一定数目的线段，判断A组能否完全覆盖B组 例如： [[1, 3], [2, 6]] [[1, 4], [4, 5]] True [[1, 2], [4, 7]] [[2, 5], [6, 7]] False 非递归中序遍历 重建二叉树 根据前序和中序遍历，返回后序遍历 用两个队列实现一个栈 解法：一个队列放入，一个队列输出。因为栈是后入先出，所以把q1的元素依次删除并插入q2，再删除最后一个元素。然后q1赋值为q2，q2初始化为空，这样才能不断删除。 问题1：交叉熵公式 解答：交叉熵公式如下： 这里公式定义，x、y都是表示概率分布。其中x是正确的概率分布，而y是我们预测出来的概率分布，这个公式算出来的结果，表示y与正确答案x之间的错误程度（即：y错得有多离谱），结果值越小，表示y越准确，与x越接近。 问题3：对后验概率估计的思考 解答：对于很多条件概率问题，可以等价于求后验概率问题。 问题4：针对归一化问题的数据线性排序思考 解答：基数排序是一种针对该问题很好的解决方式，往往因为其平均复杂度为被忽略其线性。 问题5：带有容错的最长公共子串如何实现（动态规划问题） 解答： 暂时还没想到。 问题6：剑指Offer原题，螺旋遍历 解答：主要找规律找出循环条件：并且。 https://github.com/DWCTOD/interview/blob/master/detail/%E4%BD%9C%E4%B8%9A%E5%B8%AE%20%E8%A7%86%E8%A7%89%E7%AE%97%E6%B3%95%E5%B7%A5%E7%A8%8B%E5%B8%88%20%E9%9D%A2%E7%BB%8F%EF%BC%882020%E5%B1%8A%EF%BC%89.md 第一题 leetcode29题 第二题： 给定n，用1到n作为二叉搜索树的节点值，返回n个点所能组成的二叉搜索树的个数 如 n=3 大数乘法 https://www.nowcoder.com/discuss/353156289771544576","link":"/2024/06/25/leetcode-host/"},{"title":"leetcode873 最长的斐波那契子序列的长度","text":"思路 状态定义 其中表示以位置i和j结尾的子序列 12345678910111213141516171819202122232425class Solution {public: int lenLongestFibSubseq(vector&lt;int&gt;&amp; arr) { int n = arr.size(); if (n &lt; 3) return 0; if (n == 3) { if (arr[0] + arr[1] == arr[2]) return 3; return 0; } int maxLen = 0; vector&lt;vector&lt;int&gt;&gt; m(n, vector&lt;int&gt;(n, 0)); unordered_map&lt;int, int&gt; diff; for (int i = 1; i &lt; n - 1; i ++) { diff[arr[i - 1]] = i - 1; for (int j = i + 1; j &lt; n; j ++) { int interval = arr[j] - arr[i]; if (diff.count(interval)) { m[i][j] = m[diff[interval]][i] + 1; maxLen = max(maxLen, m[i][j]); } } } return maxLen &gt; 0 ? maxLen + 2 : 0; }};","link":"/2024/06/25/longest-fibonacci-subsequence/"},{"title":"machine-learning","text":"决策树对连续值和离散值特征是否会重复利用作为分割特征 svm为什么要转成对偶问题进行求解，为什么对偶问题的解是原问题的解 svm如何进行多分类，多分类hinge loss什么形式","link":"/2024/06/26/machine-learning/"},{"title":"leetcode48 旋转图像","text":"思路 1/4矩阵旋转","link":"/2024/06/25/matrix-rotate/"},{"title":"leetcode86 分隔链表","text":"思路 遍历链表，分两个链表存储","link":"/2024/06/25/partition-list/"},{"title":"Pytorch Source Code","text":"init 处理逻辑 判断当前运行环境，加载必须库文件 Define basic utilities 定义基本工具 typename; is_tensor; ... Define numeric constants 定义数值常量 e; inf; nan; pi Define Storage and Tensor classes 定义Storage和Tensor类 ctypes库 一个可以在python中调用由C、C++编写并导出的dll动态链接库的包 ctypes.CDLL('vcruntime140.dll') 加载使用C、C++编写的vcruntime140.dll文件 .pyi文件 python中的类型提示文件，也被叫做存根文件stub file 用于提供代码的静态类型信息，也可以用来表示公共的接口 .pyi文件给出变量或函数的静态类型，实现了python和C、C++的绑定 参考 [1] Pytorch底层源码解读（一）概览","link":"/2024/06/20/torch/"},{"title":"leetcode94 树的中序遍历","text":"中序遍历 使用栈数据结构控制遍历节点顺序，类比递归调用的栈 12345678910111213141516171819202122232425262728293031323334/** * Definition for a binary tree node. * struct TreeNode { * int val; * TreeNode *left; * TreeNode *right; * TreeNode() : val(0), left(nullptr), right(nullptr) {} * TreeNode(int x) : val(x), left(nullptr), right(nullptr) {} * TreeNode(int x, TreeNode *left, TreeNode *right) : val(x), left(left), right(right) {} * }; */class Solution {public: vector&lt;int&gt; inorderTraversal(TreeNode* root) { vector&lt;int&gt; ret; if (root == nullptr) return ret; stack&lt;TreeNode*&gt; tree; tree.push(root); while (! tree.empty()) { TreeNode* top = tree.top(); if (top-&gt;left != nullptr) { tree.push(top-&gt;left); top-&gt;left = nullptr; continue; } ret.push_back(top-&gt;val); tree.pop(); if (top-&gt;right != nullptr) { tree.push(top-&gt;right); } } return ret; }};","link":"/2024/06/24/tree-inorder-traversal/"}],"tags":[{"name":"CVPR2023","slug":"CVPR2023","link":"/tags/CVPR2023/"},{"name":"CL","slug":"CL","link":"/tags/CL/"},{"name":"ZSL","slug":"ZSL","link":"/tags/ZSL/"},{"name":"GAN","slug":"GAN","link":"/tags/GAN/"},{"name":"CVPR2022","slug":"CVPR2022","link":"/tags/CVPR2022/"},{"name":"Prototype","slug":"Prototype","link":"/tags/Prototype/"},{"name":"Self-Attention","slug":"Self-Attention","link":"/tags/Self-Attention/"},{"name":"leetcode","slug":"leetcode","link":"/tags/leetcode/"},{"name":"algorithm","slug":"algorithm","link":"/tags/algorithm/"},{"name":"tree","slug":"tree","link":"/tags/tree/"},{"name":"dp","slug":"dp","link":"/tags/dp/"},{"name":"singly-linked list","slug":"singly-linked-list","link":"/tags/singly-linked-list/"},{"name":"interview","slug":"interview","link":"/tags/interview/"},{"name":"bit operation","slug":"bit-operation","link":"/tags/bit-operation/"},{"name":"matrix","slug":"matrix","link":"/tags/matrix/"},{"name":"code","slug":"code","link":"/tags/code/"},{"name":"pytorch","slug":"pytorch","link":"/tags/pytorch/"}],"categories":[{"name":"paper","slug":"paper","link":"/categories/paper/"},{"name":"algorithm","slug":"algorithm","link":"/categories/algorithm/"},{"name":"interview","slug":"interview","link":"/categories/interview/"},{"name":"source code","slug":"source-code","link":"/categories/source-code/"}],"pages":[]}