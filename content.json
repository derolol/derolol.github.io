{"posts":[{"title":"leetcode1823 找出游戏的获胜者","text":"思路 单链表模拟 取余推测 https://leetcode.cn/problems/find-the-winner-of-the-circular-game/solutions/2362052/1823-zhao-chu-you-xi-de-huo-sheng-zhe-yu-tkst","link":"/2024/06/25/algorithm/circular-game/"},{"title":"平衡二叉搜索树（BST）","text":"相关题目 leetcode108：将有序数组转换为二叉搜索树","link":"/2024/06/24/algorithm/binary-sort-tree/"},{"title":"leetcode29 两数相除","text":"思路 特殊情况处理 除数左移位，与被除数比较","link":"/2024/06/26/algorithm/divide-two-integers/"},{"title":"双指针算法题目集合","text":"思路 指针从两端收缩 遍历解空间为上三角，参考解析 leetcode11 盛最多水的容器 给定一个长度为 n 的整数数组 height 。有 n 条垂线，第 i 条线的两个端点是 (i, 0) 和 (i, height[i]) 。 找出其中的两条线，使得它们与 x 轴共同构成的容器可以容纳最多的水。 返回容器可以储存的最大水量。 说明：你不能倾斜容器 题解 两端逼近，优先短边收缩 leetcode167. 两数之和 II - 输入有序数组 给你一个下标从 1 开始的整数数组 numbers ，该数组已按 非递减顺序排列 ，请你从数组中找出满足相加之和等于目标数 target 的两个数。如果设这两个数分别是 numbers[] 和 numbers[] ，则 1 &lt;= index1 &lt; index2 &lt;= numbers.length 。 以长度为 2 的整数数组 [, ] 的形式返回这两个整数的下标 和 。 你可以假设每个输入只对应唯一的答案 ，而且你不可以重复使用相同的元素。 你所设计的解决方案必须只使用常量级的额外空间。 题解 两端逼近，&lt; target 收缩左值，&gt; target 收缩右值","link":"/2024/07/11/algorithm/double-pointer/"},{"title":"leetcode873 最长的斐波那契子序列的长度","text":"思路 状态定义 其中表示以位置 i 和 j 结尾的子序列 12345678910111213141516171819202122232425class Solution {public: int lenLongestFibSubseq(vector&lt;int&gt;&amp; arr) { int n = arr.size(); if (n &lt; 3) return 0; if (n == 3) { if (arr[0] + arr[1] == arr[2]) return 3; return 0; } int maxLen = 0; vector&lt;vector&lt;int&gt;&gt; m(n, vector&lt;int&gt;(n, 0)); unordered_map&lt;int, int&gt; diff; for (int i = 1; i &lt; n - 1; i ++) { diff[arr[i - 1]] = i - 1; for (int j = i + 1; j &lt; n; j ++) { int interval = arr[j] - arr[i]; if (diff.count(interval)) { m[i][j] = m[diff[interval]][i] + 1; maxLen = max(maxLen, m[i][j]); } } } return maxLen &gt; 0 ? maxLen + 2 : 0; }};","link":"/2024/06/25/algorithm/longest-fibonacci-subsequence/"},{"title":"哈希表算法题目集合","text":"leetcode128 最长连续序列 给定一个未排序的整数数组 nums ，找出数字连续的最长序列（不要求序列元素在原数组中连续）的长度。 请你设计并实现时间复杂度为 O(n) 的算法解决此问题。 题解 unordered_set 去重 遍历找到各个连续子序列的头 123456789101112131415161718class Solution {public: int longestConsecutive(vector&lt;int&gt;&amp; nums) { unordered_set&lt;int&gt; num_set; for (auto n : nums) num_set.insert(n); int max_len = 0; for (auto n : num_set) { if (!num_set.count(n - 1)) { int cur_num = n; while (num_set.count(cur_num + 1)) { cur_num ++; } max_len = max(cur_num - n + 1, max_len); } } return max_len; }};","link":"/2024/07/12/algorithm/hash-table/"},{"title":"leetcode48 旋转图像","text":"思路 1/4 矩阵旋转","link":"/2024/06/25/algorithm/matrix-rotate/"},{"title":"leetcode209 长度最小的子数组","text":"题目 给定一个含有 n 个正整数的数组和一个正整数 target 。 找出该数组中满足其总和大于等于 target 的长度最小的子数组[, , ..., , ] ，并返回其长度。如果不存在符合条件的子数组，返回 0 。 题解 确定左值，向右扩展直到满足 target，记录当前窗口大小，移动左值，重复上述动作，直到右值不能再拓展 123456789101112131415161718192021222324class Solution {public: int minSubArrayLen(int target, vector&lt;int&gt;&amp; nums) { int size = nums.size(); int left = 0; int right = 0; int sum = nums[0]; int minLen = size + 1; while (left &lt; size &amp;&amp; right &lt; size) { if (sum &gt;= target) { minLen = min(minLen, right - left + 1); sum -= nums[left]; left ++; continue; } if (right == size - 1) break; right ++; sum += nums[right]; } if (minLen == size + 1) return 0; return minLen; }};","link":"/2024/07/11/algorithm/min-sub-sum/"},{"title":"leetcode86 分隔链表","text":"思路 遍历链表，分两个链表存储","link":"/2024/06/25/algorithm/partition-list/"},{"title":"leetcode238 除自身以外数组的乘积","text":"题目 给你一个整数数组 nums，返回 数组 answer ，其中 answer[i] 等于 nums 中除 nums[i] 之外其余各元素的乘积 。 题目数据 保证 数组 nums 之中任意元素的全部前缀元素和后缀的乘积都在 32 位 整数范围内。 请不要使用除法，且在 O(n) 时间复杂度内完成此题。 题解 正反向前缀积，使用中间变量记录累积 123456789101112131415161718class Solution {public: vector&lt;int&gt; productExceptSelf(vector&lt;int&gt;&amp; nums) { vector&lt;int&gt; ret(nums.size()); int mul = nums[0]; ret[0] = 1; for (int i = 1; i &lt; nums.size(); i ++) { ret[i] = mul; mul *= nums[i]; } mul = nums[nums.size() - 1]; for (int i = nums.size() - 2; i &gt;= 0; i --) { ret[i] *= mul; mul *= nums[i]; } return ret; }};","link":"/2024/07/11/algorithm/product-except-self/"},{"title":"栈算法题目集合","text":"leetcode71 简化路径 给你一个字符串 path ，表示指向某一文件或目录的 Unix 风格 绝对路径 （以 '/' 开头），请你将其转化为更加简洁的规范路径。 在 Unix 风格的文件系统中，一个点（.）表示当前目录本身；此外，两个点 （..） 表示将目录切换到上一级（指向父目录）；两者都可以是复杂相对路径的组成部分。任意多个连续的斜杠（即，'//'）都被视为单个斜杠 '/' 。 对于此问题，任何其他格式的点（例如，'...'）均被视为文件/目录名称。 请注意，返回的 规范路径 必须遵循下述格式： 始终以斜杠 '/' 开头。 两个目录名之间必须只有一个斜杠 '/' 。 最后一个目录名（如果存在）不能 以 '/' 结尾。 此外，路径仅包含从根目录到目标文件或目录的路径上的目录（即，不含 '.' 或 '..'）。 返回简化后得到的规范路径。 题解 参考题解","link":"/2024/07/12/algorithm/stack/"},{"title":"reorganize-string","text":"https://leetcode.cn/problems/reorganize-string/solutions/","link":"/2024/06/28/algorithm/reorganize-string/"},{"title":"leetcode94 树的中序遍历","text":"中序遍历 使用栈数据结构控制遍历节点顺序，类比递归调用的栈 12345678910111213141516171819202122232425262728293031323334/** * Definition for a binary tree node. * struct TreeNode { * int val; * TreeNode *left; * TreeNode *right; * TreeNode() : val(0), left(nullptr), right(nullptr) {} * TreeNode(int x) : val(x), left(nullptr), right(nullptr) {} * TreeNode(int x, TreeNode *left, TreeNode *right) : val(x), left(left), right(right) {} * }; */class Solution {public: vector&lt;int&gt; inorderTraversal(TreeNode* root) { vector&lt;int&gt; ret; if (root == nullptr) return ret; stack&lt;TreeNode*&gt; tree; tree.push(root); while (! tree.empty()) { TreeNode* top = tree.top(); if (top-&gt;left != nullptr) { tree.push(top-&gt;left); top-&gt;left = nullptr; continue; } ret.push_back(top-&gt;val); tree.pop(); if (top-&gt;right != nullptr) { tree.push(top-&gt;right); } } return ret; }};","link":"/2024/06/24/algorithm/tree-inorder-traversal/"},{"title":"Hexo操作","text":"Hexo在子目录中新建文章 1hexo new [layout] -p path/&lt;title&gt;","link":"/2024/07/08/hexo/basic/"},{"title":"人工智能代理（Artificial Intelligence Agent）","text":"AI Agent深度解析：潜力与挑战并存的智能新世界 定义 感知环境、进行决策和执行动作的智能实体 应用 ABI/GBI生成式BI或是数据分析 Code Agent代码助手 基于RAG技术的知识问答 Coding Agent aiXcoder 代码大模型在企业的应用实践 PPT 语言模型发展历程 深度神经网络 预训练模型 大语言模型 论文 aiXcoder构建 训练数据收集与筛选 Github数据爬取与过滤 删除低质量项目 相似算法去重 去除敏感数据 去除注释数据 删除语法错误代码 删除静态分析缺陷代码 代码文件顺序重排 训练 任务 随机Mask抽象语法树节点 存在问题 项目级代码生成下的长序列依赖 测试 更符合真实场景的测评集 基于CodeFuse的下一代研发探索 PPT Github 数据来源 Github 预训练&amp;微调 推理加速&amp;部署 发展 仓库级代码 大模型落地到代码助手场景的探索实践 PPT 百度大模型驱动下的智能代码助手提效实践 https://qcon.infoq.cn/2023/shanghai/presentation/5679","link":"/2024/07/09/knowledge/ai-agent/"},{"title":"计算机基础 computer-fundamental","text":"数据库 left join 和 right join、union 是怎么操作的","link":"/2024/06/27/knowledge/computer-fundamental/"},{"title":"computer-network","text":"http 中的 get、post 区别，是怎么做的","link":"/2024/06/26/knowledge/computer-network/"},{"title":"计算机视觉 Computer Vision","text":"【三年面试五年模拟】算法工程师的求职面试秘籍 &gt; 从 ReLU 到 GELU，一文概览神经网络的激活函数 https://github.com/DWCTOD/interview/blob/master/detail/%E4%BD%9C%E4%B8%9A%E5%B8%AE%20%E8%A7%86%E8%A7%89%E7%AE%97%E6%B3%95%E5%B7%A5%E7%A8%8B%E5%B8%88%20%E9%9D%A2%E7%BB%8F%EF%BC%882020%E5%B1%8A%EF%BC%89.md https://github.com/GYee/CV_interviews_Q-A 图像处理与计算机视觉基础 基本概念 原理 图像预处理 特征提取 对象检测 图像分类 图像分割 OpenCV 库 读写图像 图像滤波 几何变换 特征检测与描述 深度学习基础 梯度下降 滑动平均 模型微调（Fine-tuning） 基础模块 池化层 Pooling Layer 归一化层 Normalization Layer BN，Batch Normalization IN，Instance Normalization LN，Layer Normalization GN，Group Normalization BN 是怎么做，作用是什么 激活层 常见激活函数 Sigmoid Tanh ReLU LeakyReLU SoftPlus ELU SELU，自归一化 Swish，类 Sigmoid 作为开关 GELU GLU 特性 梯度消失 存在偏导过小 梯度爆炸 偏导累乘过大 梯度裁剪 输出均值为 0 能避免每次权重只能往单一反向变化 ReLU 计算复杂度低 ReLU 的负半轴为输出值增加稀疏性，减少计算量，但同时会让一些神经元不能更新 SoftPlus，ReLU 的平滑 全连接层 Linear 嵌入层 Embedding 卷积层 Convolution 特征 局部感知、权值共享、平移不变、多核 1×1 卷积 特征增强 特征融合 改变通道数 分类 空洞卷积 分组卷积 转置卷积层 Transpose Convolution 优化模块 残差结构 Residual Connection 将输入与层输出相加 优势 缓解梯度消失，增加网络深度 保留信息，特征重用 空间金字塔池化（Spatial Pyramid Pooling，SPP） 空洞空间金字塔池化（Atrous Spatial Pyramid Pooling，ASPP） HDC 可变形卷积 Deformable Convolution 可分离卷积 Separable Convolution Transformer 模块结构 多头自注意力机制（Multi-Head Self-Attention Mechanism） 前馈神经网络（Feed-Forward Neural Network） 层归一化（Layer Normalization） 残差连接（Residual Connections） Self-Attention 除以的原因 矩阵计算导致的元素值整体偏大，从而引发梯度消失 计算后的数据平方差为，除以，将分布的方差纠正回接近 1 并行化的体现 序列计算多头注意力 影响计算量的因素 序列长度：点积、矩阵乘 头数量 优势 并行处理整个序列 长距离依赖 缺点 计算量大 超参调优 超长序列处理能力 Cross-Attention Convolutional Attention SENet -CBAM 基础模型 前馈神经网络 卷积神经网络（CNN） 循环神经网络（RNN） 长短时记忆网络（LSTM） Inception 模型调优 模型优化 正则化 L1 正则化 L2 正则化 损失函数 已知 softmax 输出概率序列与实际分布概率序列，计算两者交叉熵 超参数调整 在深度学习中，超参数（Hyperparameters）是指在训练开始前设置的模型参数，不是通过训练学习得到的。超参数的选择对模型性能有很大的影响，不同的超参数设置可能导致显著不同的训练结果。 优化器选择 SGD AdaGrad RMSProp Adam 学习率衰减 LR 中的连续值特征是如何处理的 为什么 LR 要先对数据进行归一化处理 LR 用了 sigmoid 函数，那么 LR 是线性模型还是非线性模型，为什么 线性 分段 余弦 WarmUp 周期性 缩放法则 Scaling-Law 在 AI 领域中，描述模型性能如何随着模型规模（如参数数量、训练数据量、计算资源等）变化而变化的一组经验法则 应用 设计更大规模的模型 指导研究人员如何设计和训练更大规模的模型，以实现更高的性能 优化资源分配 如确定是否应增加模型参数数量、增加训练数据量，还是增加计算资源，以实现最优的性能提升 预测性能 根据现有模型的性能和缩放法则，可以预测更大规模模型的性能 常见模型评估指标 准确率 召回率 F1 分数 浮点数运算次数 FLOPs 帧每秒 FPS 深度学习框架 训练范式 123456789101112131415161718192021222324252627# 加载数据dataloader = DataLoader()# 加载模型model = MyModel()# 损失函数criterion = nn.CrossEntropyLoss()# 优化器optimizer = optim.Adam(model.parameters(), lr=0.001)# 学习率衰减策略# 训练num_epochsfor epoch in range(num_epochs): # 遍历完整数据集 for inputs, labels in dataloader: # 梯度置零 optimizer.zero_grad() # 模型推理 outputs = model(inputs) # 计算损失 loss = criterion(outputs, labels) # 累加梯度 loss.backward() # 梯度更新 optimizer.step() 评估范式 1234567# 设置模型评估模式model.eval()# 取消梯度更新with torch.no_grad(): for inputs, labels in test_dataloader: outputs = model(inputs) # 计算准确率或其他指标 PyTorch Tensor 存储 头信息区（Tensor）：tensor 的形状（size）、步长（stride）、数据类型（type）等 存储区（Storage）：数据 stride 属性 指定维度中一个元素到下一个元素的步长 维度变换 类型 方法 描述 维度顺序 permute 指定维度重排，返回共享存储区的 tensor transpose 交换维度，返回共享存储区的 tensor 形状变换 view 返回共享存储区 tensor，要求存储连续，否则调用 contiguous contiguous 开辟新的存储区构建连续 tensor reshape 若连续则返回原 tensor，否则创建新 tensor 广播 broadcast_to 冗余维度 squeeze 压缩维度 unsqueeze 展开维度 扩展维度 expand 扩展大小为 1 的维度 repeat 按照指定维度重复 tensor 展平维度 flatten ravel 维度剪裁 narrow 维度展开 unfold 张量乘法 方法 应用 torch.matmul() 多维矩阵相乘 torch.mm() 2 维矩阵相乘 torch.bmm() 批矩阵相乘 torch.dot() 点积 torch.mv() 矩阵向量相乘 torch.einsum() 复杂张量运算 torch.einsum('ij,jk-&gt;ik', a, b) 张量合并与拆分 stack 扩展维度拼接 cat 根据维度拼接 split 按大小分 chunk 按块分 nn.Module 模块基类 nn.Sequential 线性模块容器 计算图 环境搭建 数据加载 模型定义 训练 验证 保存 加载模型 PytorchLightning 逻辑思维与项目经验 逻辑思维 准备通过解决实际问题来展示你的逻辑思维能力和数据分析洞察力，可以是以往项目中的案例分析。 团队合作与挑战接受度 思考并准备实例说明你如何在团队中有效沟通、协作解决问题，以及面对技术挑战时的态度和解决策略。","link":"/2024/06/26/knowledge/computer-vision/"},{"title":"计算机视觉 Computer Vision","text":"【三年面试五年模拟】算法工程师的求职面试秘籍 &gt; 从 ReLU 到 GELU，一文概览神经网络的激活函数 https://github.com/DWCTOD/interview/blob/master/detail/%E4%BD%9C%E4%B8%9A%E5%B8%AE%20%E8%A7%86%E8%A7%89%E7%AE%97%E6%B3%95%E5%B7%A5%E7%A8%8B%E5%B8%88%20%E9%9D%A2%E7%BB%8F%EF%BC%882020%E5%B1%8A%EF%BC%89.md https://github.com/GYee/CV_interviews_Q-A 图像处理与计算机视觉基础 基本概念 原理 图像预处理 特征提取 对象检测 图像分类 图像分割 OpenCV 库 读写图像 图像滤波 几何变换 特征检测与描述 深度学习基础 梯度下降 滑动平均 模型微调（Fine-tuning） 基础模块 池化层 Pooling Layer 归一化层 Normalization Layer BN，Batch Normalization IN，Instance Normalization LN，Layer Normalization GN，Group Normalization BN 是怎么做，作用是什么 激活层 常见激活函数 Sigmoid Tanh ReLU LeakyReLU SoftPlus ELU SELU，自归一化 Swish，类 Sigmoid 作为开关 GELU GLU 特性 梯度消失 存在偏导过小 梯度爆炸 偏导累乘过大 梯度裁剪 输出均值为 0 能避免每次权重只能往单一反向变化 ReLU 计算复杂度低 ReLU 的负半轴为输出值增加稀疏性，减少计算量，但同时会让一些神经元不能更新 SoftPlus，ReLU 的平滑 全连接层 Linear 嵌入层 Embedding 卷积层 Convolution 特征 局部感知、权值共享、平移不变、多核 1×1 卷积 特征增强 特征融合 改变通道数 分类 空洞卷积 分组卷积 转置卷积层 Transpose Convolution 上采样 插值 反卷积 PixelShuffle 优化模块 残差结构 Residual Connection 将输入与层输出相加 优势 缓解梯度消失，增加网络深度 保留信息，特征重用 空间金字塔池化（Spatial Pyramid Pooling，SPP） 空洞空间金字塔池化（Atrous Spatial Pyramid Pooling，ASPP） HDC 可变形卷积 Deformable Convolution 可分离卷积 Separable Convolution Transformer 模块结构 多头自注意力机制（Multi-Head Self-Attention Mechanism） 前馈神经网络（Feed-Forward Neural Network） 层归一化（Layer Normalization） 残差连接（Residual Connections） Self-Attention 除以的原因 矩阵计算导致的元素值整体偏大，从而引发梯度消失 计算后的数据平方差为，除以，将分布的方差纠正回接近 1 并行化的体现 序列计算多头注意力 影响计算量的因素 序列长度：点积、矩阵乘 头数量 优势 并行处理整个序列 长距离依赖 缺点 计算量大 超参调优 超长序列处理能力 Cross-Attention Convolutional Attention SENet -CBAM 基础模型 前馈神经网络 卷积神经网络（CNN） 循环神经网络（RNN） 长短时记忆网络（LSTM） 模型调优 模型优化 正则化 L1 正则化 L2 正则化 损失函数 已知 softmax 输出概率序列与实际分布概率序列，计算两者交叉熵 超参数调整 在深度学习中，超参数（Hyperparameters）是指在训练开始前设置的模型参数，不是通过训练学习得到的。超参数的选择对模型性能有很大的影响，不同的超参数设置可能导致显著不同的训练结果。 优化器选择 SGD AdaGrad RMSProp Adam 学习率衰减 LR 中的连续值特征是如何处理的 为什么 LR 要先对数据进行归一化处理 LR 用了 sigmoid 函数，那么 LR 是线性模型还是非线性模型，为什么 线性 分段 余弦 WarmUp 周期性 缩放法则 Scaling-Law 在 AI 领域中，描述模型性能如何随着模型规模（如参数数量、训练数据量、计算资源等）变化而变化的一组经验法则 应用 设计更大规模的模型 指导研究人员如何设计和训练更大规模的模型，以实现更高的性能 优化资源分配 如确定是否应增加模型参数数量、增加训练数据量，还是增加计算资源，以实现最优的性能提升 预测性能 根据现有模型的性能和缩放法则，可以预测更大规模模型的性能 常见模型评估指标 准确率 召回率 F1 分数 浮点数运算次数 FLOPs 帧每秒 FPS 深度学习框架 训练范式 PyTorch Tensor 存储 头信息区（Tensor）：tensor 的形状（size）、步长（stride）、数据类型（type）等 存储区（Storage）：数据 stride 属性 指定维度中一个元素到下一个元素的步长 view 方法 返回共享存储区的 tensor 计算图 环境搭建 数据加载 模型定义 训练 验证 保存 加载模型 PytorchLightning 逻辑思维与项目经验 逻辑思维 准备通过解决实际问题来展示你的逻辑思维能力和数据分析洞察力，可以是以往项目中的案例分析。 团队合作与挑战接受度 思考并准备实例说明你如何在团队中有效沟通、协作解决问题，以及面对技术挑战时的态度和解决策略。","link":"/2024/06/26/knowledge/cv/"},{"title":"数据结构 Data Structure","text":"数组 用连续内存存储数据 读写操作复杂度 O(1) 字符串 用连续内存存储字符 链表 由指针把若干个节点连接成链状结构 树 节点之间用指针链接 除根节点之外每个节点只有一个父节点，根节点没有父节点 叶子节点没有子节点 二叉树 每个节点最多只能有两个子节点 二叉搜索树 若其左子树不为NULL，则左子树上所有节点的值都＜根节点的值 若其右子树不为NULL，则右子树上所有节点的值都＞根节点的值 其左右子树也分别是二叉搜索树 查询复杂度 堆 最大堆：根节点的值最大 最小堆：根节点的值最小 栈 先进后出 队列 先进先出 图 多线程 异常处理 算法 排序 算法 时间复杂度 稳定性 冒泡排序 √ 选择排序 × 归并排序 √ 快速排序 × 堆排序 × 查找 二分查找 动态规划","link":"/2024/06/26/knowledge/data-structure/"},{"title":"图像数据集 Image Dataset","text":"数据集建立 数据增强 AutoAugment：搜索最优图像处理操作组合 RandAugment：已有图像处理集合，确定操作次数 N 和操作幅度 M 分类任务 Mixup：按比例混合图像和标签 Cutout：图像掩码 CutMix：图像掩码混合 目标检测任务 Mosaic：图像合成 语义分割任务 Copy-Paste：分割结果粘贴到另一张图 数据预处理 数据增强 旋转、平移、缩放、翻转、随机色调(H)、饱和度(S)、明度(V)调整、等 数据归一化 Normalization Min-Max 归一化 Z-Score 归一化 RobustScaler 归一化 One-Hot 编码 数据类别不平衡问题 采样比例 数据生成","link":"/2024/06/26/knowledge/dataset/"},{"title":"deep-learning","text":"【三年面试五年模拟】算法工程师的求职面试秘籍 分类 Classification 将输入数据划分到预定义的有限标签中，输出为预测的类别标签 常用评价指标 准确率 精确率 召回率 F1 分数 应用 花卉图像分类 垃圾邮件拦截 回归 Regression 建立数值型随机自变量的模型并进行连续的因变量预测，输出为数值 常用评价指标 均方误差 R2 分数 应用 股票价格预测 房价预测 聚类 Clustering 将无标签的数据分成多个类（簇），确保类内样本相似，类间样本相异，其输出是聚类结果（簇划分，簇标签，簇中心等） 常用评价指标 样本紧密度 样本分隔度 应用 用户分群 异常检测 决策 Decision making 通过神经网络理解给定目标，约束条件和可用信息，预测出最佳或满意的动作决策，其输出是一连串的动作 常用评价指标 最终回报 平均奖励 应用 游戏 AI 自动驾驶 概率密度估计 Probability density estimation 使用深度神经网络来估计一个随机变量或一组随机变量的概率密度函数，其输出是数据的概率分布 常用评价指标衡量分布差异 对数似然损失 KL 散度 应用 数据生成 样本采样","link":"/2024/06/27/knowledge/deep-learning/"},{"title":"generative-model","text":"VAE AE VAE VAE GAN Diffusion GAN DDPM 生成过程被设计成一系列的马尔可夫步骤，其中每一个步骤只依赖于前一步的状态 DDIM 采样 DDIM 在每一步都会预测并尝试直接达到最终的清晰状态，而不是仅仅依赖于当前的模糊状态 DDPM 通过马尔科夫链推导下一状态，DDIM 通过减少推理步数加速推理 Stable Diffusion ControlNet RAG DIT 指标 MSE 计算公式 PSNR 单位：信噪比 计算公式 SSIM 通过均值和协方差衡量两张图像的 Luminance（亮度）、Contrast（对比度）和 Structure（结构） 计算公式 取值范围：[-1, 1]","link":"/2024/07/01/knowledge/generative-model/"},{"title":"hpc","text":"GPU 编程与性能优化 CUDA 与 cuDNN GPU 编程基本原理 特别是如何使用 CUDA 进行并行计算，以及 cuDNN 库在加速深度学习中的应用 内存管理 核函数设计 性能监控与调优 性能优化 掌握一些基本的性能分析工具和方法，比如使用 nvprof 或 TensorFlow Profiler 分析模型运行瓶颈，并实施相应的优化措施。","link":"/2024/06/26/knowledge/hpc/"},{"title":"leetcode-host","text":"编程题： 逆时针打印数组 （剑指 offer 和 leetcode54 都有的常见题，常为顺时针打印数组） 给先序遍历重构二叉树 （例如输入为 124XXX3XX，X 表示空，无叶子节点） 有随机数 0-2 0-3 0-4 构建 100 的随机数 （使用 0-3 和 0-4 构建 20 与 0-4 构建的 5 形成 100 的随机数） 智力题： 49 个人中至少几个人生日是同一月 两个人只握一次手，一共握了 45 次，问一共几个人（10 人） 编程题： 数组合并（leetcode88）【简单】 区间合并，也叫线段合并（leetcode56）【中等】 以上内容+能否完全覆盖，题目为： 单个线段[2,6]可称为完全覆盖[4,6]，现有两组线段 AB，每组中有一定数目的线段，判断 A 组能否完全覆盖 B 组 例如： [[1, 3], [2, 6]] [[1, 4], [4, 5]] True [[1, 2], [4, 7]] [[2, 5], [6, 7]] False 非递归中序遍历 重建二叉树 根据前序和中序遍历，返回后序遍历 用两个队列实现一个栈 解法：一个队列放入，一个队列输出。因为栈是后入先出，所以把 q1 的元素依次删除并插入 q2，再删除最后一个元素。然后 q1 赋值为 q2，q2 初始化为空，这样才能不断删除。 问题 1：交叉熵公式 解答：交叉熵公式如下： 这里公式定义，x、y 都是表示概率分布。其中 x 是正确的概率分布，而 y 是我们预测出来的概率分布，这个公式算出来的结果，表示 y 与正确答案 x 之间的错误程度（即：y 错得有多离谱），结果值越小，表示 y 越准确，与 x 越接近。 问题 3：对后验概率估计的思考 解答：对于很多条件概率问题，可以等价于求后验概率问题。 问题 4：针对归一化问题的数据线性排序思考 解答：基数排序是一种针对该问题很好的解决方式，往往因为其平均复杂度为被忽略其线性。 问题 5：带有容错的最长公共子串如何实现（动态规划问题） 解答： 暂时还没想到。 问题 6：剑指 Offer 原题，螺旋遍历 解答：主要找规律找出循环条件：并且。 https://github.com/DWCTOD/interview/blob/master/detail/%E4%BD%9C%E4%B8%9A%E5%B8%AE%20%E8%A7%86%E8%A7%89%E7%AE%97%E6%B3%95%E5%B7%A5%E7%A8%8B%E5%B8%88%20%E9%9D%A2%E7%BB%8F%EF%BC%882020%E5%B1%8A%EF%BC%89.md 第一题 leetcode29 题 第二题： 给定 n，用 1 到 n 作为二叉搜索树的节点值，返回 n 个点所能组成的二叉搜索树的个数 如 n=3 大数乘法 https://www.nowcoder.com/discuss/353156289771544576","link":"/2024/06/25/knowledge/leetcode-host/"},{"title":"machine-learning","text":"决策树对连续值和离散值特征是否会重复利用作为分割特征 svm 为什么要转成对偶问题进行求解，为什么对偶问题的解是原问题的解 svm 如何进行多分类，多分类 hinge loss 什么形式 随机森林与 GBDT，解释、比较异同、哪种方法单棵决策树的深度更大","link":"/2024/06/26/knowledge/machine-learning/"},{"title":"数学基础","text":"微积分 求导 梯度 偏微分 积分 统计学，概率论 推断统计学和概率论的基本概念 高斯分布 贝叶斯公式 最小二乘法 线性回归 逻辑回归 线性代数 向量 向量空间 向量投影 矩阵运算 特征值分解 SVD分解","link":"/2024/07/08/knowledge/math/"},{"title":"opencv","text":"https://segmentfault.com/a/1190000044071469 cv2.imread(path, flag) flag： 12[return] type: numpy.ndarray size: (H, W, C), C -&gt; (BGR) ImageToTensor 12345image = cv2.imread(path)image_rgb = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)image = torch.from_numpy(image_rgb).transpose(1, 2).transpose(0, 1)image = image.float().div(255) cv2.imwrite(path, image) TensorToImage 123image = image.mul(255).byte()image = image.transpose(0, 1).transpose(1, 2)image = image.cpu().numpy() 通道处理 12b, g, r = cv2.split(image)image = cv2.merge((b, g, r)) 图像处理与计算机视觉基础 二值化 cv2.threshold() 边缘检测 cv2.Canny() 图像滤波 cv2.filter2D() cv2.blur() cv2.GaussianBlur() 图像形态学操作 cv2.erode() 腐蚀 cv2.dilate() 膨胀 cv2.morphologyEx() 开闭运算、形态学梯度、顶帽、黑帽等","link":"/2024/06/28/knowledge/opencv/"},{"title":"RAG技术","text":"深度解析RAG技术在大模型时代的原理与实践 概念","link":"/2024/07/09/knowledge/rag/"},{"title":"python","text":"Python修饰器（语法糖） 详解Python修饰器（语法糖） 123@修饰函数def 待修饰函数(): pass 在调用待修饰函数时，直接调用修饰函数 在实现修饰函数时，需要注意待修饰函数的参数和返回值","link":"/2024/07/01/knowledge/python/"},{"title":"semantic-segmentation","text":"语义分割模型：从 FCN 到 DeepLab V3+的全面解析 2015 FCN（全卷积网络） 2015 年提出，首次将卷积神经网络应用于像素级分类，开创了端到端语义分割的先河。FCN 将最后几层全连接层替换为卷积层，允许任意大小的输入图像，并通过上采样（反卷积）恢复分割图的分辨率。 U-Net 特别适用于医学图像分割，其特征是编码器-解码器结构，解码器层与编码器层之间有跳跃连接，用以恢复细节信息。 2016 SegNet 基于 VGG 网络，改进了 FCN 的上采样部分，使用了编码器-解码器结构，其中解码器的上采样层使用了编码器的池化索引来恢复细节。 DeepLab V1 DeepLab 系列模型是语义分割领域的另一个重要里程碑。DeepLab V1 首次引入了 Atrous Convolution 模块，弥补了删除池化模块后感受野大小的影响。 2017 PSPNet（金字塔场景解析网络） 引入了金字塔池化模块（Pyramid Pooling Module），捕获不同尺度的信息，增强了模型对不同大小物体的分割能力。 DeepLab V2 DeepLab V2 在 ResNet 的基础上引入了 ASPP（Atrous Spatial Pyramid Pooling）模块，进一步扩大了感受野并提高了分割精度。 MobileNet V1 深度可分离卷积的应用 2018 DeepLab V3 而 DeepLab V3 通过增加 ASPP 模块的宽度，进一步提升了模型的性能。DeepLab V3+是 DeepLab 系列的最新版本，它增加了网络深度，将 Xception 网络作为主干，使用深度可分离卷积 Depthwise Separable Convolution， 结合了解码器模块，以恢复细节信息，进一步提升了多尺度处理能力。 ENet 专为实时应用设计，使用了高效的编码-解码结构和跳过连接。 MobileNet V2 Inverted Residuals 和 Linear Bottleneck ICNet 多尺度级联输入，深度监督 BiSeNet 结合了两个分支，Spatial 分支和 Context 分支。 2019 HRNet 维持高分辨率流以捕获更多细节，同时进行多尺度融合。 DANet Spatial Attention 和 Channel Attention Fast-SCNN：Fast Semantic Segmentation Network 共享下采样权重双分支网络 OCRNet（Object Contextual Representations） 引入了对象上下文表示，使用注意力机制来建模像素间的关系，增强对局部和全局上下文的理解。 CCNet 提出了交叉注意机制（Criss-Cross Attention），允许模型以较低的计算成本建模长距离依赖 MobileNet V3 SE 模块和 Swish 2021 SegFormer SETR 图像块编码，输入纯 Transformer 网络提取特征，reshape 特征后卷积上采样 MaskFormer 将分割统一为 Mask 分类任务 2023 Segment Anything Model (SAM) 一种通用的分割模型，能够处理多种类型的分割任务，包括语义分割、实例分割和全景分割。","link":"/2024/07/01/knowledge/semantic-segmentation/"},{"title":"Pytorch Source Code","text":"init 处理逻辑 判断当前运行环境，加载必须库文件 Define basFic utilities 定义基本工具 typename; is_tensor; ... Define numeric constants 定义数值常量 e; inf; nan; pi Define Storage and Tensor classes 定义 Storage 和 Tensor 类 ctypes 库 一个可以在 python 中调用由 C、C++编写并导出的 dll 动态链接库的包 ctypes.CDLL('vcruntime140.dll') 加载使用 C、C++编写的vcruntime140.dll文件 .pyi 文件 python 中的类型提示文件，也被叫做存根文件 stub file 用于提供代码的静态类型信息，也可以用来表示公共的接口 .pyi 文件给出变量或函数的静态类型，实现了 python 和 C、C++的绑定 参考 [1] Pytorch 底层源码解读（一）概览","link":"/2024/06/20/knowledge/torch/"},{"title":"ANCL","text":"Auxiliary Network Continual Learning (ANCL) Paper: Achieving a Better Stability-Plasticity Trade-off via Auxiliary Networks in Continual Learning Authors: Sanghwan Kim ; Lorenzo Noci ; Antonio Orvieto ; Thomas Hofmann Code: https://github.com/kim-sanghwan/ANCL Framework: Continual Learning (CL) 持续学习 符号定义 PT：Previous Task CT：Current Task 含义 保留 PT 信息的同时，继续在 CT 中进行学习 难点：Catastrophic Forgetting 灾难性的遗忘 对于梯度更新学习的模型，在学习 CT 的过程中更倾向于覆盖 PT 学习的梯度 换而言之，Stability-Plasticity Dilemma Martial Mermillod, Aur ́ elia Bugaiska, and Patrick Bonin. The stability-plasticity dilemma: Investigating the continuum from catastrophic forgetting to age-limited learning effects, 2013. 1 Stability: 在 PT 具有较好的泛化能力 Plasticity: 在 CT 学习新概念 所以，如何平衡 Stability 和 Plasticity是研究的重点 任务分类 类别增量学习(Class-Incremental Learning)的前世今生、开源工具包 Task Incremental Learning (TIL)：训练和测试阶段均为模型提供当前任务标识 Domain Incremental Learning (DIL)：测试阶段不提供当前任务标识 Class Incremental Learning (CIL)：测试阶段自动识别当前任务标识和分类 学习难度逐渐增加，ANCL 在 TIL 和 CIL 设置中进行了评估 相关工作 增加 Auxiliary Network 或 Extra Module Active Forgetting with synaptic Expansion-Convergence (AFEC) 超参控制新旧参数的融合 当前工作 框架化使用 Auxiliary Network 的 CL，使得 Auxiliary Network 插件化 通过和调整正则化项 局限 不同方法依赖于不同的超参 参考 [1] 类别增量学习(Class-Incremental Learning)的前世今生、开源工具包","link":"/2024/06/20/paper/ANCL/"},{"title":"Bi-VAEGAN","text":"Paper: Bi-directional Distribution Alignment for Transductive Zero-Shot Learning Authors: Zhicai Wang, Yanbin Hao, Tingting Mu, Ouxiang Li, Shuo Wang, Xiangnan He Code: GitHub Framework: Zero-shot Learning (ZSL) Hugo Larochelle, Dumitru Erhan, and Yoshua Bengio. Zerodata learning of new tasks. In AAAI, volume 1, page 3, 2008. 1 目标 解决训练时缺少例子或标签的问题 Conventional ZSL / Inductive ZSL 核心挑战 在存在Class Relevance的条件下，使得分类器能从 Seen Classes 提取信息迁移到 Unseen Classes 当中 Class Relevance 通常作为 Auxiliary Data 提供 Auxiliary Data 可以为人工标注、文字描述、知识图谱或 Formal Description of Knowledge（如嵌入向量） Domain Shift Problem 仅从 Auxiliary Data 学习容易导致 Unseen Classes 的真实分布与其建模分布之间存在差异 Proposed: Transductive ZSL (TZSL) 允许在训练中额外加入为目标类别收集的无标签示例 Generative Models 作用 Synthesize Examples 合成样本 Learn the Unseen Data Distribution 学习 unseen 数据分布 分类 Unconditional Generation Conditional Generation Auxiliary 信息是信息量更丰富的类标签，通过 Auxiliary 信息作为 Condition，可以学习到 Data-Auxiliary 联合分布，这连接了 Visual 空间和 Auxiliary 空间，使得生成器具有信息迁移的能力 难点 将 seen classes 所学迁移到 unseen classes f-VAEGAN 提出方法 Transductive Regressor Normalization Class Prior Estimation (CPE) 架构 VAE 编码器，得到维隐藏表征向量 条件生成器，以类别属性为条件，从正态分布采样维向量用于视觉特征生成 Wasserstein GAN（WGAN）的判别器，用于 seen classes WGAN 的判别器，用于 unseen classes 映射视觉空间到特征空间的 Regressor WGAN 的判别器，用于特征判别 Workflow Alt text Level-1 和对抗性训练 Level-2 和、对抗性训练","link":"/2024/06/21/paper/Bi-VAEGAN/"},{"title":"Code of Pixel-to-Prototype Constrast","text":"Generate CAMs Feature map Class feature map Score of class CAMs Pixel-to-Prototype Contrast Pseudo mask Pixel-wise projected feature Pixel-to-prototype contrast Prototype set Temperature Contrast 像素特征与原型的相似度 Prototype Estimation in Batch Top K pixels of class c CAM as confidences Estimate prototypes from pixel-wise feature embeddings that are with the top K confidences Prototype Loss Cross Prototype Contrast Cross CAM Contrast Intra-view Contrast Strategy to slove the matter of in accurate pseudo label [50] Semi-hard prototype mining Hard pixel sampling Code 归一化 归一化 作用 保证所有元素之和为 1 将向量转换为概率分布 归一化 1234# 按通道执行L2归一化v = v / (torch.norm(v, dim=1, keepdim=True) + 1e-5)# orv = torch.nn.functional.normalize(v, dim=1) 作用 方向不变性：向量的方向不变，长度变为 1，使得向量表示不再依赖于其大小 数值稳定性：将向量的大小规范在一个相对较小的区间 减小特征尺度的差异 便于执行相似性度量 Max 归一化 归一化后向量的最大值为 1 Max-Min 归一化 归一化后向量值范围为[0, 1] Forward cam 1234# fea是最后一层输出的特征图self.fc8 = nn.Conv2d(4096, 21, 1, bias=False)cam = self.fc8(fea)cam = torch.nn.functional.interpolate(cam, (H, W), mode='bilinear', align_corners=True) cam_rv_down 清洗 CAM 12345678910with torch.no_grad(): cam_d = torch.nn.functional.relu(cam.detach()) # max norm cam_d_max = torch.max(cam_d.view(n, c, -1), dim=-1)[0].view(n, c, 1, 1)+1e-5 cam_d_norm = torch.nn.functional.relu(cam_d - 1e-5) / cam_d_max # 计算保留概率值最大分类，反相为背景概率，其余分类置0 cam_d_norm[:, 0, :, :] = 1 - torch.max(cam_d_norm[:, 1:, :, :], dim=1)[0] cam_max = torch.max(cam_d_norm[:,1:,:,:], dim=1, keepdim=True)[0] cam_d_norm[:,1:,:,:][cam_d_norm[:,1:,:,:] &lt; cam_max] = 0 增强 CAM 1234567891011121314151617181920# 根据像素相似度调整CAMcam_rv_down = self.PCM(cam_d_norm, f)# PCMdef PCM(self, cam, f): n,c,h,w = f.size() cam = torch.nn.functional.interpolate(cam, (h,w), mode='bilinear', align_corners=True).view(n,-1,h*w) # 多尺度特征融合 f = self.f9(f) f = f.view(n, -1, h*w) # 特征按通道L2归一化 f = f / (torch.norm(f, dim=1, keepdim=True) + 1e-5) # 计算像素相似度矩阵 aff = torch.nn.functional.relu(torch.matmul(f.transpose(1, 2), f), inplace=True) # 相似度矩阵L1归一化 aff = aff/(torch.sum(aff, dim=1, keepdim=True) + 1e-5) # CAM加权 cam_rv = torch.matmul(cam, aff).view(n, -1, h, w) return cam_rv cam_rv 1cam_rv = torch.nn.functional.interpolate(cam_rv_down, (H,W), mode='bilinear', align_corners=True) f_proj 12self.fc_proj = torch.nn.Conv2d(4096, 128, 1, bias=False)f_proj = torch.nn.functional.relu(self.fc_proj(fea), inplace=True) prototype 12345678910111213141516171819202122232425262728293031323334353637f_proj1 = torch.nn.functional.interpolate(f_proj1, size=(128 // 8, 128 // 8), mode='bilinear', align_corners=True)cam_rv1_down = torch.nn.functional.interpolate(cam_rv1_down, size=(128 // 8, 128 // 8), mode='bilinear', align_corners=True)cam_rv2_down = cam_rv2_downwith torch.no_grad(): fea1 = f_proj1.detach() c_fea1 = fea1.shape[1] cam_rv1_down = torch.nn.functional.relu(cam_rv1_down.detach()) # CAM Max-min归一化 n1, c1, h1, w1 = cam_rv1_down.shape max1 = torch.max(cam_rv1_down.view(n1, c1, -1), dim=-1)[0].view(n1, c1, 1, 1) min1 = torch.min(cam_rv1_down.view(n1, c1, -1), dim=-1)[0].view(n1, c1, 1, 1) cam_rv1_down[cam_rv1_down &lt; min1 + 1e-5] = 0. norm_cam1 = (cam_rv1_down - min1 - 1e-5) / (max1 - min1 + 1e-5) cam_rv1_down = norm_cam1 # 设置背景阈值 cam_rv1_down[:, 0, :, :] = args.bg_threshold # 根据图像级标签保留相应的类别 scores1 = torch.nn.functional.softmax(cam_rv1_down * label, dim=1) # 计算伪标签 pseudo_label1 = scores1.argmax(dim=1, keepdim=True) n_sc1, c_sc1, h_sc1, w_sc1 = scores1.shape scores1 = scores1.transpose(0, 1) fea1 = fea1.permute(0, 2, 3, 1).reshape(-1, c_fea1) # 获取各个分类CAM值最高的值与索引 top_values, top_indices = torch.topk(cam_rv1_down.transpose(0, 1).reshape(c_sc1, -1), k=h_sc1 * w_sc1 // 8, dim=-1) prototypes1 = torch.zeros(c_sc1, c_fea1).cuda() # [21, 128] # 遍历各个分类 for i in range(c_sc1): # 获取k个像素对应的特征 top_fea = fea1[top_indices[i]] # CAM值加权平均k个特征得到分类原型 prototypes1[i] = torch.sum(top_values[i].unsqueeze(-1) * top_fea, dim=0) / torch.sum(top_values[i]) # 各个原型L2归一化 prototypes1 = torch.nn.functional.normalize(prototypes1, dim=-1) prototype similarity 12345678910111213141516171819202122232425n_f, c_f, h_f, w_f = f_proj1.shape# [N, H, W, C] -&gt; [N x H x W, C]f_proj1 = f_proj1.permute(0, 2, 3, 1).reshape(n_f * h_f * w_f, c_f)# 特征L2归一化f_proj1 = torch.nn.functional.normalize(f_proj1, dim=-1)pseudo_label1 = pseudo_label1.reshape(-1)positives1 = prototypes2[pseudo_label1]negitives1 = prototypes2# for targetn_f, c_f, h_f, w_f = f_proj2.shapef_proj2 = f_proj2.permute(0, 2, 3, 1).reshape(n_f * h_f * w_f, c_f)f_proj2 = torch.nn.functional.normalize(f_proj2, dim=-1)pseudo_label2 = pseudo_label2.reshape(-1)positives2 = prototypes1[pseudo_label2]negitives2 = prototypes1A1 = torch.exp(torch.sum(f_proj1 * positives1, dim=-1) / 0.1)A2 = torch.sum(torch.exp(torch.matmul(f_proj1, negitives1.transpose(0, 1)) / 0.1), dim=-1)loss_nce1 = torch.mean(-1 * torch.log(A1 / A2))A3 = torch.exp(torch.sum(f_proj2 * positives2, dim=-1) / 0.1)A4 = torch.sum(torch.exp(torch.matmul(f_proj2, negitives2.transpose(0, 1)) / 0.1), dim=-1)loss_nce2 = torch.mean(-1 * torch.log(A3 / A4))loss_cross_nce = 0.1 * (loss_nce1 + loss_nce2) / 2","link":"/2023/11/14/paper/Code-of-Pixel-to-Prototype-Constrast/"},{"title":"BiFormer","text":"Paper: BiFormer: Vision Transformer with Bi-Level Routing Attention Authors: Lei Zhu, Xinjiang Wang, Zhanghan Ke, Wayne Zhang, Rynson Lau Code: GitHub Framework: Transformer 优势 long-range dependency inductive-bias-free high parallelism 劣势 计算量大 内存占用大 现有方案：引入稀疏性 局部窗口 轴向注意力 空洞注意力 存在问题 筛选 key/value 时没有区分 query Bi-level Routing Attention (BRA) Sparsity 利用稀疏性来节省计算量和内存，同时只包含 GPU 友好的稠密矩阵乘法 Query-aware 为各个 Query 筛选语义最相关的 Key-Value 对 伪代码 1234567891011121314151617181920212223242526272829# input: features (H, W, C). Assume H==W.# output: features (H, W, C).# S: square root of number of regions.# k: number of regions to attend.# patchify input (H, W, C) -&gt; (Sˆ2, HW/Sˆ2, C)x = patchify(input, patch_size=H//S)# linear projection of query, key, valuequery, key, value = linear_qkv(x).chunk(3, dim=-1)# regional query and key (Sˆ2, C)query_r, key_r = query.mean(dim=1), key.mean(dim=1)# adjacency matrix for regional graph (Sˆ2, Sˆ2)A_r = mm(query_r, key_r.transpose(-1, -2))# compute index matrix of routed regions (Sˆ2, K)I_r = topk(A_r, k).index# gather key-value pairskey_g = gather(key, I_r)# (Sˆ2, kHW/Sˆ2, C)value_g = gather(value, I_r)# (Sˆ2, kHW/Sˆ2, C)# token-to-token attentionA = bmm(query, key_g.transpose(-2, -1))A = softmax(A, dim=-1)output = bmm(A, value_g) + dwconv(value)# recover to (H, W, C) shapeoutput = unpatchify(output, patch_size=H//S)","link":"/2024/06/26/paper/biformer/"}],"tags":[{"name":"leetcode","slug":"leetcode","link":"/tags/leetcode/"},{"name":"dp","slug":"dp","link":"/tags/dp/"},{"name":"singly-linked list","slug":"singly-linked-list","link":"/tags/singly-linked-list/"},{"name":"algorithm","slug":"algorithm","link":"/tags/algorithm/"},{"name":"tree","slug":"tree","link":"/tags/tree/"},{"name":"bit operation","slug":"bit-operation","link":"/tags/bit-operation/"},{"name":"双指针","slug":"双指针","link":"/tags/%E5%8F%8C%E6%8C%87%E9%92%88/"},{"name":"哈希表","slug":"哈希表","link":"/tags/%E5%93%88%E5%B8%8C%E8%A1%A8/"},{"name":"matrix","slug":"matrix","link":"/tags/matrix/"},{"name":"滑动窗口","slug":"滑动窗口","link":"/tags/%E6%BB%91%E5%8A%A8%E7%AA%97%E5%8F%A3/"},{"name":"suffix-product","slug":"suffix-product","link":"/tags/suffix-product/"},{"name":"栈","slug":"栈","link":"/tags/%E6%A0%88/"},{"name":"interview","slug":"interview","link":"/tags/interview/"},{"name":"code","slug":"code","link":"/tags/code/"},{"name":"pytorch","slug":"pytorch","link":"/tags/pytorch/"},{"name":"CVPR2023","slug":"CVPR2023","link":"/tags/CVPR2023/"},{"name":"CL","slug":"CL","link":"/tags/CL/"},{"name":"ZSL","slug":"ZSL","link":"/tags/ZSL/"},{"name":"GAN","slug":"GAN","link":"/tags/GAN/"},{"name":"CVPR2022","slug":"CVPR2022","link":"/tags/CVPR2022/"},{"name":"Prototype","slug":"Prototype","link":"/tags/Prototype/"},{"name":"Transformer","slug":"Transformer","link":"/tags/Transformer/"}],"categories":[{"name":"algorithm","slug":"algorithm","link":"/categories/algorithm/"},{"name":"interview","slug":"interview","link":"/categories/interview/"},{"name":"source code","slug":"source-code","link":"/categories/source-code/"},{"name":"paper","slug":"paper","link":"/categories/paper/"}],"pages":[]}