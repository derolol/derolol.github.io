{"posts":[{"title":"二分法算法题目集合","text":"leetcode300 最长递增子序列 给你一个整数数组 nums ，找到其中最长严格递增子序列的长度。 子序列 是由数组派生而来的序列，删除（或不删除）数组中的元素而不改变其余元素的顺序。例如，[3,6,2,7] 是数组 [0,3,1,6,2,2,7] 的子序列。 题解 常规做法是通过一维数组存储当前位置作为结尾的最大子序列长度。 而根据推理可知，若在从左向右遍历时，当前元素不能与已有元素构成更长的子序列，那么替换更小的元素将有更大的可能性构成更长子序列。 1234567891011int lengthOfLIS(vector&lt;int&gt;&amp; nums) { auto end = nums.begin(); for (int x : nums) { auto it = lower_bound(nums.begin(), end, x); *it = x; if (it == end) { end ++; } } return end - nums.begin();} 排序序列找区间或值 leetcode34 在排序数组中查找元素的第一个和最后一个位置 给你一个按照非递减顺序排列的整数数组 nums，和一个目标值 target。请你找出给定目标值在数组中的开始位置和结束位置。 如果数组中不存在目标值 target，返回 [-1, -1]。 你必须设计并实现时间复杂度为 O(log n) 的算法解决此问题。 思路 直接定位，向两个方向找到区间边缘 定位左边界，再定位右边界 leetcode74 搜索二维矩阵 给你一个满足下述两条属性的 m x n 整数矩阵： 每行中的整数从左到右按非严格递增顺序排列。 每行的第一个整数大于前一行的最后一个整数。 给你一个整数 target ，如果 target 在矩阵中，返回 true ；否则，返回 false 。 思路 二分定位行 二分定位元素","link":"/2024/08/03/algorithm/binary-search/"},{"title":"排序辅助算法题目集合","text":"leetcode274 H 指数 给你一个整数数组 citations ，其中 citations[i] 表示研究者的第 i 篇论文被引用的次数。计算并返回该研究者的 h 指数。 根据维基百科上 h 指数的定义：h 代表“高引用次数” ，一名科研人员的 h 指数 是指他（她）至少发表了 h 篇论文，并且 至少 有 h 篇论文被引用次数大于等于 h 。如果 h 有多种可能的值，h 指数 是其中最大的那个。 题解 12345678910int hIndex(vector&lt;int&gt;&amp; citations) { sort(citations.begin(), citations.end()); int h = 0, i = citations.size() - 1; while (i &gt;= 0 &amp;&amp; citations[i] &gt; h) { h++; i--; } return h; } 给定数组求组合 leetcode39 组合总和 给你一个 无重复元素 的整数数组 candidates 和一个目标整数 target ，找出 candidates 中可以使数字和为目标数 target 的 所有 不同组合 ，并以列表形式返回。你可以按 任意顺序 返回这些组合。 candidates 中的 同一个 数字可以 无限制重复被选取 。如果至少一个数字的被选数量不同，则两种组合是不同的。 对于给定的输入，保证和为 target 的不同组合数少于 150 个。 思路 题目要求得到所有组合，而不是值。所以考虑通过递归和回溯的方式得到不用的组合 组成元素和数量相同的两个组合为同一种组合，所以在递归的时候需要传递当前的候选值范围 由于有目标值的要求，可以通过排序剪枝不可能的组合 12345678910111213141516171819202122232425262728293031class Solution {private: vector&lt;vector&lt;int&gt;&gt; ret;public: void getSum(vector&lt;int&gt;&amp; candidates, int target, int start, int sum, vector&lt;int&gt;&amp; host) { // 若满足条件则记录 if (sum == target) { ret.push_back(host); return; } // 从取值起始点遍历候选值 for (int i = start, num = candidates.size(); i &lt; num; i ++) { // 如果加上最小值都超过目标条件，那么可以直接结束遍历 if (sum + candidates[i] &gt; target) break; // 加入当前值到组合中 host.push_back(candidates[i]); // 通过罗列自身及之后的候选值构建满足条件的组合 getSum(candidates, target, i, sum + candidates[i], host); // 回溯组合 host.pop_back(); } } vector&lt;vector&lt;int&gt;&gt; combinationSum(vector&lt;int&gt;&amp; candidates, int target) { sort(candidates.begin(), candidates.end()); vector&lt;int&gt; v; getSum(candidates, target, 0, 0, v); return ret; }};","link":"/2024/07/17/algorithm/after-sort/"},{"title":"平衡二叉搜索树（BST）","text":"相关题目 leetcode108：将有序数组转换为二叉搜索树","link":"/2024/06/24/algorithm/binary-sort-tree/"},{"title":"动态规划算法题目集合","text":"0/1 背包 准备多种大小的背包，当物品体积小于背包时，可尝试比较当前已放置物品价值，与当前物品价值和更小体积背包放置物品价值之和，取更大值 leetcode416. 分割等和子集 给你一个 只包含正整数 的 非空 数组 nums 。请你判断是否可以将这个数组分割成两个子集，使得两个子集的元素和相等。 123456789101112131415161718bool canPartition(vector&lt;int&gt;&amp; nums) { int sum = 0; for (auto n : nums) sum += n; // 若累加值为奇数，则不可能存在这样的两个子集 if (sum % 2 != 0) return false; sum &gt;&gt;= 1; int n = nums.size(); // 判断大小为sum的背包，是否能恰好装入价值为sum的物品 // 其中，各个物品的体积和价值均为nums[i] vector&lt;int&gt; maxPack(sum + 1); for (auto n : nums) { if (n &gt; sum) return false; for (int i = sum; i &gt;= n; i --) { maxPack[i] = max(maxPack[i], maxPack[i - n] + n); } } return maxPack[sum] == sum;} 一维动态规划 leetcode139 单词拆分 给你一个字符串 s 和一个字符串列表 wordDict 作为字典。如果可以利用字典中出现的一个或多个单词拼接出 s 则返回 true。 注意：不要求字典中出现的单词全部都使用，并且字典中的单词可以重复使用。 1234567891011121314151617bool wordBreak(string s, vector&lt;string&gt;&amp; wordDict) { vector&lt;bool&gt; dp(s.size() + 1, false); dp[0] = true; for (int i = 1, len = s.size(); i &lt;= len; i ++) { bool flag = false; for (string word : wordDict) { if (i &lt; word.size()) continue; int start = i - word.size(); if (dp[start] &amp;&amp; s.substr(start, word.size()) == word) { flag = true; break; } } dp[i] = flag; } return dp[s.size()];} 牛客 2021360 春招编程题(第一批) 火星人的宝藏 X 星人发现了一个藏宝图，在藏宝图中标注了 N 个宝库的位置。这 N 个宝库连成了一条直线，每个宝库都有若干枚金币。 X 星人决定乘坐热气球去收集金币，热气球每次最多只能飞行 M 千米（假设热气球在飞行过程中并不会发生故障）此外，由于设计上的缺陷，热气球最多只能启动 K 次。 X 星人带着热气球来到了第 1 个宝库（达到第 1 个宝库时热气球尚未启动），收集完第 1 个宝库的金币后将启动热气球前往下一个宝库。如果他决定收集某一个宝库的金币，必须停下热气球，收集完之后再重新启动热气球。当然，X 星人每到一个宝库是一定会拿走这个宝库所有金币的。 已知每一个宝库距离第 1 个宝库的距离（单位：千米）和宝库的金币数量。 请问 X 星人最多可以收集到多少枚金币？ 输入描述： 单组输入。 第 1 行输入三个正整数 N、M 和 K，分别表示宝库的数量、热气球每次最多能够飞行的距离（千米）和热气球最多可以启动的次数，三个正整数均不超过 100，相邻两个正整数之间用空格隔开。 接下来 N 行每行包含两个整数，分别表示第 1 个宝库到某一个宝库的距离（千米）和这个宝库的金币枚数。（因为初始位置为第 1 个宝库，因此第 1 个宝库所对应行的第 1 个值为 0。） 输入保证所有的宝库按照到第 1 个宝库的距离从近到远排列，初始位置为第 1 个宝库。 输出描述： 输出一个正整数，表示最多可以收集的金币数。 示例 1 输入例子： 5 10 2 0 5 8 6 10 8 18 12 22 15 输出例子： 25 例子说明： X 星人启动热气球两次，分别收集第 1 个、第 3 个和第 4 个宝库的金币，一共可以得到的金币总数为 5+8+12=25 枚。 题解 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354#include &lt;iostream&gt;#include &lt;vector&gt;using namespace std;int main() { int N, M, K; while (cin &gt;&gt; N &gt;&gt; M &gt;&gt; K) { // 注意 while 处理多个 case vector&lt;vector&lt;int&gt;&gt; ship(N, vector&lt;int&gt;(2, 0)); vector&lt;vector&lt;int&gt;&gt; dp(N, vector&lt;int&gt;(K + 1, 0)); int k, v; for (int i = 0; i &lt; N; i ++) { cin &gt;&gt; k &gt;&gt; v; ship[i][0] = k; ship[i][1] = v; if (i &gt; 0 &amp;&amp; k &lt;= M) { dp[i][1] = max(dp[i - 1][1], v); } } if (N - 1 &lt;= K) { long long ret = 0; for (int i = 1; i &lt; N; i ++) { if (ship[i][0] - ship[i - 1][0] &gt; M) break; ret += ship[i][1]; // cout &lt;&lt; ret &lt;&lt; endl; } cout &lt;&lt; ret + ship[0][1]; return 0; } for (int k = 2; k &lt;= K; k ++) { for (int n = k; n &lt; N; n ++) { int nn = n - 1; int maxV = 0; while (nn &gt;= k - 1) { if (dp[nn][k - 1] == 0) { nn --; continue; } if (ship[n][0] - ship[nn][0] &gt; M) break; maxV = max(maxV, dp[nn][k - 1] + ship[n][1]); nn --; } dp[n][k] = maxV; // if (k == K) cout &lt;&lt; \"n: \" &lt;&lt; n &lt;&lt; \" k: \" &lt;&lt; k &lt;&lt; \" dp:\" &lt;&lt; dp[n][k] &lt;&lt; endl; } } int ret = 0; for (int i = K; i &lt; N; i ++) { ret = max(ret, dp[i][K]); } cout &lt;&lt; ret + ship[0][1]; } return 0;}// 64 位输出请用 printf(\"%lld\") leetcode221 最大正方形 在一个由 '0' 和 '1' 组成的二维矩阵内，找到只包含 '1' 的最大正方形，并返回其面积。 1dp[i][j] = min(min(dp[i - 1][j - 1], dp[i - 1][j]), dp[i][j - 1]) leetcode873 最长的斐波那契子序列的长度 思路 状态定义 其中表示以位置 i 和 j 结尾的子序列 12345678910111213141516171819202122232425class Solution {public: int lenLongestFibSubseq(vector&lt;int&gt;&amp; arr) { int n = arr.size(); if (n &lt; 3) return 0; if (n == 3) { if (arr[0] + arr[1] == arr[2]) return 3; return 0; } int maxLen = 0; vector&lt;vector&lt;int&gt;&gt; m(n, vector&lt;int&gt;(n, 0)); unordered_map&lt;int, int&gt; diff; for (int i = 1; i &lt; n - 1; i ++) { diff[arr[i - 1]] = i - 1; for (int j = i + 1; j &lt; n; j ++) { int interval = arr[j] - arr[i]; if (diff.count(interval)) { m[i][j] = m[diff[interval]][i] + 1; maxLen = max(maxLen, m[i][j]); } } } return maxLen &gt; 0 ? maxLen + 2 : 0; }}; 最大子数组和 leetcode918 环形子数组的最大和 给定一个长度为 n 的环形整数数组 nums ，返回 nums 的非空 子数组 的最大可能和 。 环形数组 意味着数组的末端将会与开头相连呈环状。形式上， nums[i] 的下一个元素是 nums[(i + 1) % n] ， nums[i] 的前一个元素是 nums[(i - 1 + n) % n] 。 子数组 最多只能包含固定缓冲区 nums 中的每个元素一次。形式上，对于子数组 nums[i], nums[i + 1], ..., nums[j] ，不存在 i &lt;= k1, k2 &lt;= j 其中 k1 % n == k2 % n 。 思路 通常对于环形数组的最大子数组和有两种情形，一种是不需要首尾相连的子数组，另一种则是需要连接开头的子数组 对于子数组 1，其求解方式与传统求解方法一样； 对于子数组 2，可以通过逆向思维求解，由于子数组 2 为最大子数组，所以数组的和减去子数组 2 的和，得到的是最小子数组 3，子数组 3 的首位是不相连的 123456789101112131415161718192021int maxSubarraySumCircular(vector&lt;int&gt;&amp; nums) { int maxSum = nums[0], minSum = nums[0]; int preMaxSum = nums[0], preMinSum = nums[0]; int sum = nums[0]; for (int i = 1, len = nums.size(); i &lt; len; i ++) { // 对比局部最大与当前值 preMaxSum = max(preMaxSum + nums[i], nums[i]); // 对比最大值与局部最大 maxSum = max(preMaxSum, maxSum); // 对比局部最小与当前值 preMinSum = min(preMinSum + nums[i], nums[i]); // 对比最小值与局部最小 minSum = min(preMinSum, minSum); // 数组累加和 sum += nums[i]; } // 若此时最大值为负值，即所有值为负值，则不存在两个子数组 if (maxSum &lt; 0) return maxSum; // 对比两个子数组 return max(maxSum, sum - minSum);} 逆向动态规划 leetcode174. 地下城游戏 恶魔们抓住了公主并将她关在了地下城 dungeon 的 右下角 。地下城是由 m x n 个房间组成的二维网格。我们英勇的骑士最初被安置在 左上角 的房间里，他必须穿过地下城并通过对抗恶魔来拯救公主。 骑士的初始健康点数为一个正整数。如果他的健康点数在某一时刻降至 0 或以下，他会立即死亡。 有些房间由恶魔守卫，因此骑士在进入这些房间时会失去健康点数（若房间里的值为负整数，则表示骑士将损失健康点数）；其他房间要么是空的（房间里的值为 0），要么包含增加骑士健康点数的魔法球（若房间里的值为正整数，则表示骑士将增加健康点数）。 为了尽快解救公主，骑士决定每次只 向右 或 向下 移动一步。 返回确保骑士能够拯救到公主所需的最低初始健康点数。 注意：任何房间都可能对骑士的健康点数造成威胁，也可能增加骑士的健康点数，包括骑士进入的左上角房间以及公主被监禁的右下角房间。 思路 对于任意位置，其起始健康点数取决于后续经过的房间，所以尝试使用逆向 DP 解题 对比理解，若问题为到达终点的最大健康点数，对于任意位置，其最大健康点数只与已经经过的房间有关，可以使用正向 DP 解题 根据观察发现，当后续房间值为非负值时，对起始健康点数无影响；而当后续房间值为负值时，起始健康点数应为负值的相反数。由此可得到状态转移方程为： 其中，操作用于获得所需健康点数更小的后续房间值，操作则将正值置，保留负值 为了实现原地算法，将所需健康点数累加到当前位置，若累加后值为正，则代表加上当前房间值后，后续的路径勇士都有足够的健康点数，否则代表需要更大的健康点数 另外，由于勇士只有在健康点数为时才能存活，所以对于负值为的房间，需要保证起始健康点数为 时间复杂度: 空间复杂度: 123456789101112int calculateMinimumHP(vector&lt;vector&lt;int&gt;&gt;&amp; dungeon) { int m = dungeon.size(), n = dungeon[0].size(); for (int i = m - 1; i &gt;= 0; i --) { int j = (i == m - 1) ? n - 2 : n - 1; for (; j &gt;= 0; j --) { int right = (j + 1 == n) ? dungeon[i + 1][j] : dungeon[i][j + 1]; int bottom = (i + 1 == m) ? dungeon[i][j + 1] : dungeon[i + 1][j]; dungeon[i][j] += min(max(right, bottom), 0); } } return dungeon[0][0] &gt; 0 ? 1 : (- 1 * dungeon[0][0]) + 1;} leetcode213 打家劫舍 II 你是一个专业的小偷，计划偷窃沿街的房屋，每间房内都藏有一定的现金。这个地方所有的房屋都 围成一圈 ，这意味着第一个房屋和最后一个房屋是紧挨着的。同时，相邻的房屋装有相互连通的防盗系统，如果两间相邻的房屋在同一晚上被小偷闯入，系统会自动报警 。 给定一个代表每个房屋存放金额的非负整数数组，计算你 在不触动警报装置的情况下 ，今晚能够偷窃到的最高金额。 分类讨论动态规划 可考虑两种可能的情况，一种为小偷偷了第一家，最后一家就不能偷；第二种情况为小偷没有偷第一家","link":"/2024/07/20/algorithm/dynamic-programming/"},{"title":"双指针算法题目集合","text":"思路 指针从两端收缩 遍历解空间为上三角，参考解析 leetcode11 盛最多水的容器 给定一个长度为 n 的整数数组 height 。有 n 条垂线，第 i 条线的两个端点是 (i, 0) 和 (i, height[i]) 。 找出其中的两条线，使得它们与 x 轴共同构成的容器可以容纳最多的水。 返回容器可以储存的最大水量。 说明：你不能倾斜容器 题解 两端逼近，优先短边收缩 leetcode167. 两数之和 II - 输入有序数组 给你一个下标从 1 开始的整数数组 numbers ，该数组已按 非递减顺序排列 ，请你从数组中找出满足相加之和等于目标数 target 的两个数。如果设这两个数分别是 numbers[] 和 numbers[] ，则 1 &lt;= index1 &lt; index2 &lt;= numbers.length 。 以长度为 2 的整数数组 [, ] 的形式返回这两个整数的下标 和 。 你可以假设每个输入只对应唯一的答案 ，而且你不可以重复使用相同的元素。 你所设计的解决方案必须只使用常量级的额外空间。 题解 两端逼近，&lt; target 收缩左值，&gt; target 收缩右值","link":"/2024/07/11/algorithm/double-pointer/"},{"title":"贪心算法题目集合","text":"leetcode452 用最少数量的箭引爆气球 有一些球形气球贴在一堵用 XY 平面表示的墙面上。墙面上的气球记录在整数数组 points ，其中 points[i] = [, ] 表示水平直径在 和 之间的气球。你不知道气球的确切 y 坐标。 一支弓箭可以沿着 x 轴从不同点 完全垂直 地射出。在坐标 x 处射出一支箭，若有一个气球的直径的开始和结束坐标为 , 且满足 ≤ x ≤ ，则该气球会被 引爆 。可以射出的弓箭的数量 没有限制 。 弓箭一旦被射出之后，可以无限地前进。 给你一个数组 points ，返回引爆所有气球所必须射出的 最小弓箭数 。 题解 排序，记录最小右边界（箭所能移动的最远方向），若不在射程范围内，记录新的右边界，否则更新最小右边界","link":"/2024/07/17/algorithm/greedy/"},{"title":"堆题目集合","text":"最小堆 排列组合 TopK 问题 优先队列 priority_queue 123456789101112131415161718192021222324252627282930313233343536#include &lt;iostream&gt;#include &lt;queue&gt;using namespace std;int main(){ priority_queue&lt;int,vector&lt;int&gt;,less&lt;int&gt;&gt; p1; //默认大根堆 p1.push(2); p1.push(1); p1.push(3); //输出 3 2 1 while(p1.empty()== false){ cout&lt;&lt;p1.top()&lt;&lt;\" \"; p1.pop(); } priority_queue&lt;int,vector&lt;int&gt;,greater&lt;int&gt;&gt; p2; //小根堆 p2.push(2); p2.push(1); p2.push(3); //输出 1 2 3 while(p2.empty()== false){ cout&lt;&lt;p2.top()&lt;&lt;\" \"; p2.pop(); } // 基本数据类型-pair-pair类型默认先比较第一个元素，第一个相等比较第二个 priority_queue&lt;pair&lt;int, int&gt; &gt; a; a.push(pair&lt;int,int&gt;{1,2}); a.push(pair&lt;int,int&gt;{1,3}); a.push(pair&lt;int,int&gt;{2,5}); while (a.empty()== false){ cout&lt;&lt;a.top().first&lt;&lt;\" \"&lt;&lt; a.top().second&lt;&lt;endl; a.pop(); } //输出: //2 5 //1 3 //1 2 return 0;} 多路归并 每次迭代有 N 个选择，将 N 个选择放入堆得到排序后的值 题型 一维 根据题意队列初始化 1 个值 二维 根据题意队列按序初始化 1 个列表 初始化左上角值，set 判断是否重复插入 注意去重的问题 leetcode264 丑数 II（一维） 给你一个整数 n ，请你找出并返回第 n 个 丑数 。 丑数 就是质因子只包含 2、3 和 5 的正整数。 1234567891011121314151617181920int nthUglyNumber(int n) { long times_num[] = {2, 3, 5}; set&lt;long&gt; record; priority_queue&lt;long, vector&lt;long&gt;, greater&lt;long&gt;&gt; q; q.push(1); int index = 1; while (index &lt;= n) { int cur_num = q.top(); if (index == n) return cur_num; q.pop(); for (long t : times_num) { long new_num = cur_num * t; if (record.contains(new_num)) continue; q.push(new_num); record.insert(new_num); } index ++; } return 0;} leetcode373 查找和最小的 K 对数字（二维） 给定两个以 非递减顺序排列 的整数数组 和 , 以及一个整数 。 定义一对值 ，其中第一个元素来自 ，第二个元素来自 。 请找到和最小的 k 个数对 。 12345678910111213141516171819202122vector&lt;vector&lt;int&gt;&gt; kSmallestPairs(vector&lt;int&gt;&amp; nums1, vector&lt;int&gt;&amp; nums2, int k) { vector&lt;vector&lt;int&gt;&gt; result; int l1 = nums1.size(), l2 = nums2.size(); auto cmp = [&amp;](const auto&amp; a, const auto&amp; b) { return nums1[a.first] + nums2[a.second] &gt; nums1[b.first] + nums2[b.second]; }; priority_queue&lt;pair&lt;int, int&gt;, vector&lt;pair&lt;int, int&gt;&gt;, decltype(cmp)&gt; q(cmp); for (int i = 0; i &lt; l1; i ++) { q.push({i, 0}); } while (result.size() &lt; k) { auto [ia, ib] = q.top(); result.push_back({nums1[ia], nums2[ib]}); q.pop(); if (ib + 1 &lt; l2) q.push(pair&lt;int, int&gt;{ia, ib + 1}); } return result;}","link":"/2024/08/01/algorithm/heap/"},{"title":"哈希表算法题目集合","text":"leetcode380 O(1) 时间插入、删除和获取随机元素 实现 RandomizedSet 类： RandomizedSet() 初始化 RandomizedSet 对象 bool insert(int val) 当元素 val 不存在时，向集合中插入该项，并返回 true ；否则，返回 false 。 bool remove(int val) 当元素 val 存在时，从集合中移除该项，并返回 true ；否则，返回 false 。 int getRandom() 随机返回现有集合中的一项（测试用例保证调用此方法时集合中至少存在一个元素）。每个元素应该有 相同的概率 被返回。 你必须实现类的所有函数，并满足每个函数的 平均 时间复杂度为 O(1) 。 题解 STL 关联式容器中： set 和 map 的底层数据结构为红黑树，因为 map 和 set 要求是自动排序的，红黑树能够实现这一功能，并且各个操作的时间复杂度都较低，而 unordered_set 和 unordered_map 的底层数据结构为哈希表，查找时间复杂度为常数级 1234567891011121314151617181920212223242526272829303132333435363738394041class RandomizedSet {public: vector&lt;int&gt; data; unordered_map&lt;int, int&gt; record; RandomizedSet() { } bool insert(int val) { if (record.contains(val)) { return false; } data.push_back(val); record[val] = data.size() - 1; return true; } bool remove(int val) { if (! record.contains(val)) { return false; } data[record[val]] = data[data.size() - 1]; record[data[record[val]]] = record[val]; data.pop_back(); record.erase(val); return true; } int getRandom() { return data[rand() % data.size()]; }};/** * Your RandomizedSet object will be instantiated and called as such: * RandomizedSet* obj = new RandomizedSet(); * bool param_1 = obj-&gt;insert(val); * bool param_2 = obj-&gt;remove(val); * int param_3 = obj-&gt;getRandom(); */ leetcode128 最长连续序列 给定一个未排序的整数数组 nums ，找出数字连续的最长序列（不要求序列元素在原数组中连续）的长度。 请你设计并实现时间复杂度为 O(n) 的算法解决此问题。 题解 unordered_set 去重 遍历找到各个连续子序列的头 123456789101112131415161718class Solution {public: int longestConsecutive(vector&lt;int&gt;&amp; nums) { unordered_set&lt;int&gt; num_set; for (auto n : nums) num_set.insert(n); int max_len = 0; for (auto n : num_set) { if (!num_set.count(n - 1)) { int cur_num = n; while (num_set.count(cur_num + 1)) { cur_num ++; } max_len = max(cur_num - n + 1, max_len); } } return max_len; }};","link":"/2024/07/12/algorithm/hash-table/"},{"title":"区间题目集合","text":"leetcode 打气球 右端点排序，合并区间 美团种树 随着种树数量的增加，树总量单调递增，可以通过二分法查询种树量 美团流星 左端点和右端点分开排序，计算最大经过区间数量","link":"/2024/09/05/algorithm/interval/"},{"title":"数学推理算法题目集合","text":"2024 360 秋招提前批 最大曼哈顿距离 给定一个包含若干个点的集合，求其中任意两点之间的最大/最小曼哈顿距离。 题解 由上述式子可得： 整理合并可得： 所以，问题转换为比较点和点的横纵坐标和与差。 在遍历各个点的时候分别计算横纵坐标的和与差，且分别排序。 将最大值减去最小值得到两种情况下的最大曼哈顿距离，然后再从中选择最大的曼哈顿距离作为最终答案。 123456789vector&lt;int&gt; d1;vector&lt;int&gt; d2;for (int i = 0; i &lt; points.size(); i ++) d1.push_back(points[i][0] - points[i][1]) d2.push_back(points[i][0] + points[i][1])sort(d1.begin(), d1.end())sort(d2.begin(), d2.end())cout &lt;&lt; max(d1[d1.size() - 1] - d1[0], d2[d2.size() - 1] - d2[0])) 相似题型 1014. 最佳观光组合 leetcode172 阶乘后的零 给定一个整数 n ，返回 n! 结果中尾随零的数量。 提示 1234567891011int trailingZeroes(int n) { int times = 0; for (int i = 5; i &lt;= n; i += 5) { int count = i; while (count &amp;&amp; count % 5 == 0) { times ++; count /= 5; } } return times;} leetcode201 数字范围按位与 给你两个整数 left 和 right ，表示区间 [left, right] ，返回此区间内所有数字 按位与 的结果（包含 left 、right 端点）。 题解 相邻两个数相与，末尾的 0 将会被消除，通过移位找公共前缀得到区间相与结果 12345678910int rangeBitwiseAnd(int left, int right) { int shift = 0; // 找到公共前缀 while (left &lt; right) { left &gt;&gt;= 1; right &gt;&gt;= 1; ++shift; } return left &lt;&lt; shift;} leetcode1823 找出游戏的获胜者 思路 单链表模拟 取余推测 leetcode 题解 leetcode29 两数相除 思路 特殊情况处理 除数左移位，与被除数比较 leetcode238 除自身以外数组的乘积 题目 给你一个整数数组 nums，返回 数组 answer ，其中 answer[i] 等于 nums 中除 nums[i] 之外其余各元素的乘积 。 题目数据 保证 数组 nums 之中任意元素的全部前缀元素和后缀的乘积都在 32 位 整数范围内。 请不要使用除法，且在 O(n) 时间复杂度内完成此题。 题解 正反向前缀积，使用中间变量记录累积 123456789101112131415161718class Solution {public: vector&lt;int&gt; productExceptSelf(vector&lt;int&gt;&amp; nums) { vector&lt;int&gt; ret(nums.size()); int mul = nums[0]; ret[0] = 1; for (int i = 1; i &lt; nums.size(); i ++) { ret[i] = mul; mul *= nums[i]; } mul = nums[nums.size() - 1]; for (int i = nums.size() - 2; i &gt;= 0; i --) { ret[i] *= mul; mul *= nums[i]; } return ret; }}; leetcode69 x 的平方根 给你一个非负整数 x ，计算并返回 x 的 算术平方根 。 由于返回类型是整数，结果只保留 整数部分 ，小数部分将被 舍去 。 注意：不允许使用任何内置指数函数和算符，例如 pow(x, 0.5) 或者 x ** 0.5 。 题解 官方题解 乘方拆解 leetcode50 Pow(x, n) 实现 pow(x, n) ，即计算 x 的整数 n 次幂函数（即，）。 题解 将拆解为 其中，式子可转换为 12345678910111213141516171819202122232425262728293031323334double myPow(double x, int n) { // 特殊情况 if (n == 0) return 1; if (n == 1) return x; if (n == -1) return 1 / x; bool flag = n &lt; 0; long nn = n; if (flag) nn = -1 * nn; double temp = x; double res = 1; // 将 n 次累乘拆解为 logn 次 while (nn &gt; 1) { long times = 2; temp = x; while (true) { temp *= temp; if (times * 2 &lt;= nn) times *= 2; else break; } nn -= times; res *= temp; } for (long i = 0; i &lt; nn; i ++) res *= x; return flag ? 1 / res : res;} 二进制位计数 leetcode137 只出现一次的数字 II 给你一个整数数组 nums ，除某个元素仅出现 一次 外，其余每个元素都恰出现 三次 。请你找出并返回那个只出现了一次的元素。 你必须设计并实现线性时间复杂度的算法且使用常数级空间来解决此问题。 题解 统计每个二进制位 1 的出现次数，若只出现一次的数在该为为 0，那么统计次数应为 3 的倍数，否则有余数 1234567891011int singleNumber(vector&lt;int&gt;&amp; nums) { int ret = 0; for (int i = 0; i &lt; 32; i ++) { int count = 0; for (auto n : nums) { count += n &gt;&gt; i &amp; 1; } if (count % 3) ret += 1 &lt;&lt; i; } return ret;} 另一种方法通过状态机的思想，用一个两位二进制数统计当前二进制位的状态，00 -&gt; 01 -&gt; 10 -&gt; 00 12345678910111213int singleNumber(vector&lt;int&gt;&amp; nums) { // n1 表示二进制的低位 // n2 表示二进制的高位 int n1 = 0, n2 = 0; for (auto n : nums) { // 若高位 n2 为 0，则低位 n1 只与当前值 n 有关；否则计数已满 3，低位直接置 0 n1 = ~n2 &amp; n1 ^ n; // 若低位 n1 为 0，则高位 n2 只与当前值 n 有关，若 n 为 1，则表示要进位，否则表示计数已满；若低位为 1，那么高位只能为 0 n2 = ~n1 &amp; n2 ^ n; } // 若计数满 3，低位应为 0，若计数除以 3 余 1，则低位应为 1，所以返回低位 return n1;} leetcode169 多数元素 给定一个大小为 n 的数组 nums ，返回其中的多数元素。多数元素是指在数组中出现次数 大于 ⌊ n/2 ⌋ 的元素。 你可以假设数组是非空的，并且给定的数组总是存在多数元素。 排序后中值必包含多数元素 1234int majorityElement(vector&lt;int&gt;&amp; nums) { sort(nums.begin(), nums.end()); return nums[nums.size() / 2];} 摩尔投票法 参考题解推荐了这种方法，思考方向为若与当前假设的众数相同，则投赞成票 1，否则投否认票-1，当得票为 0 时，则继续进行假设，多数元素将获得更多投票将投票值稳定为正；另外，从代码上理解，多数元素总有更多数量的连续元素 12345678int majorityElement(vector&lt;int&gt;&amp; nums) { int x = 0, votes = 0; for (int num : nums){ if (votes == 0) x = num; votes += num == x ? 1 : -1; } return x;} 集合类题目 leetcode2306 公司命名","link":"/2024/07/21/algorithm/math/"},{"title":"链表题目集合","text":"leetcode86 分隔链表 思路 遍历链表，分两个链表存储 leetcode23 合并 K 个升序链表 给你一个链表数组，每个链表都已经按升序排列。 请你将所有链表合并到一个升序链表中，返回合并后的链表。 多路归并选择节点","link":"/2024/08/01/algorithm/linked-list/"},{"title":"矩阵处理算法题目集合","text":"leetcode48 旋转图像 思路 1/4 矩阵旋转 leetcode289 生命游戏 根据 百度百科 ， 生命游戏 ，简称为 生命 ，是英国数学家约翰·何顿·康威在 1970 年发明的细胞自动机。 给定一个包含 m × n 个格子的面板，每一个格子都可以看成是一个细胞。每个细胞都具有一个初始状态： 1 即为 活细胞 （live），或 0 即为 死细胞 （dead）。每个细胞与其八个相邻位置（水平，垂直，对角线）的细胞都遵循以下四条生存定律： 如果活细胞周围八个位置的活细胞数少于两个，则该位置活细胞死亡； 如果活细胞周围八个位置有两个或三个活细胞，则该位置活细胞仍然存活； 如果活细胞周围八个位置有超过三个活细胞，则该位置活细胞死亡； 如果死细胞周围正好有三个活细胞，则该位置死细胞复活； 下一个状态是通过将上述规则同时应用于当前状态下的每个细胞所形成的，其中细胞的出生和死亡是同时发生的。给你 m x n 网格面板 board 的当前状态，返回下一个状态。 题解 leecode 题解 使用二进制的第二位存储修改后的状态。 12345678910111213141516171819202122232425262728293031323334class Solution {public: void gameOfLife(vector&lt;vector&lt;int&gt;&gt;&amp; board) { int dx[] = {-1, 0, 1, -1, 1, -1, 0, 1}; int dy[] = {-1, -1, -1, 0, 0, 1, 1, 1}; for(int i = 0; i &lt; board.size(); i++) { for(int j = 0 ; j &lt; board[0].size(); j++) { int sum = 0; for(int k = 0; k &lt; 8; k++) { int nx = i + dx[k]; int ny = j + dy[k]; if(nx &gt;= 0 &amp;&amp; nx &lt; board.size() &amp;&amp; ny &gt;= 0 &amp;&amp; ny &lt; board[0].size()) { sum += (board[nx][ny]&amp;1); // 只累加最低位 } } if(board[i][j] == 1) { if(sum == 2 || sum == 3) { board[i][j] |= 2; // 使用第二个bit标记是否存活 } } else { if(sum == 3) { board[i][j] |= 2; // 使用第二个bit标记是否存活 } } } } for(int i = 0; i &lt; board.size(); i++) { for(int j = 0; j &lt; board[i].size(); j++) { board[i][j] &gt;&gt;= 1; //右移一位，用第二bit覆盖第一个bit。 } } }}; 矩阵哈希 leetcode36 有效的数独 请你判断一个 9 x 9 的数独是否有效。只需要 根据以下规则 ，验证已经填入的数字是否有效即可。 数字 1-9 在每一行只能出现一次。 数字 1-9 在每一列只能出现一次。 数字 1-9 在每一个以粗实线分隔的 3x3 宫内只能出现一次。（请参考示例图） 题解 分别从行、列和九宫格统计数字出现次数 1234567891011121314151617181920bool isValidSudoku(vector&lt;vector&lt;char&gt;&gt;&amp; board) { vector&lt;vector&lt;bool&gt;&gt; row(9, vector&lt;bool&gt;(9, false)); vector&lt;vector&lt;bool&gt;&gt; col(9, vector&lt;bool&gt;(9, false)); vector&lt;vector&lt;bool&gt;&gt; box(9, vector&lt;bool&gt;(9, false)); for (int i = 0; i &lt; 9; i ++) { for (int j = 0; j &lt; 9; j ++) { if (board[i][j] == '.') continue; int b = i / 3 * 3 + j / 3; int no = board[i][j] - '0' - 1; if (row[i][no] || col[j][no] || box[b][no]) return false; row[i][no] = true; col[j][no] = true; box[b][no] = true; } } return true;} leetcode240 搜索二维矩阵 II 编写一个高效的算法来搜索 m x n 矩阵 matrix 中的一个目标值 target 。该矩阵具有以下特性： 每行的元素从左到右升序排列。 每列的元素从上到下升序排列。 利用特性缩小检索范围 12345678910111213141516171819bool searchMatrix(vector&lt;vector&lt;int&gt;&gt;&amp; matrix, int target) { int m = matrix.size(), n = matrix[0].size(); int a = 0, b = 0; int i = m - 1, j = n - 1; while (i &gt;= 0 &amp;&amp; j &gt;= 0) { if (matrix[i][j] &lt; target) return false; while (b &lt; n - 1 &amp;&amp; matrix[i][b] &lt; target) { b ++; } if (matrix[i][b] == target) return true; i --; while (a &lt; m - 1 &amp;&amp; matrix[a][j] &lt; target) { a ++; } if (matrix[a][j] == target) return true; j --; } return false;} 转换为二叉搜索树 参考题解","link":"/2024/07/22/algorithm/matrix/"},{"title":"滑动窗口题目集合","text":"leetcode209 长度最小的子数组 题目 给定一个含有 n 个正整数的数组和一个正整数 target 。 找出该数组中满足其总和大于等于 target 的长度最小的子数组[, , ..., , ] ，并返回其长度。如果不存在符合条件的子数组，返回 0 。 题解 确定左值，向右扩展直到满足 target，记录当前窗口大小，移动左值，重复上述动作，直到右值不能再拓展 123456789101112131415161718192021222324class Solution {public: int minSubArrayLen(int target, vector&lt;int&gt;&amp; nums) { int size = nums.size(); int left = 0; int right = 0; int sum = nums[0]; int minLen = size + 1; while (left &lt; size &amp;&amp; right &lt; size) { if (sum &gt;= target) { minLen = min(minLen, right - left + 1); sum -= nums[left]; left ++; continue; } if (right == size - 1) break; right ++; sum += nums[right]; } if (minLen == size + 1) return 0; return minLen; }};","link":"/2024/08/01/algorithm/sliding-window/"},{"title":"reorganize-string","text":"重构字符串 中等 相关标签 相关企业 提示 给定一个字符串 s ，检查是否能重新排布其中的字母，使得两相邻的字符不同。 返回 s 的任意可能的重新排列。若不可行，返回空字符串 \"\" 。 https://leetcode.cn/problems/reorganize-string/solutions/","link":"/2024/06/28/algorithm/reorganize-string/"},{"title":"排序算法总结","text":"参考资料 排序方法 平均情况 最好情况 最坏情况 空间 稳定性 冒泡 O(n^2) O(n) O(n^2) O(1) 稳定 简单选择排序 O(n^2) O(n^2) O(n^2) O(1) 不稳定 直接插入排序 O(n^2) O(n) O(n^2) O(1) 稳定 希尔排序 O(nlogn) ~ O(n^2) O(n1.3) O(n^2) O(1) 不稳定 快速排序 O(nlogn) O(nlogn) O(n^2) O(logn)~O(n) 不稳定 堆排序 O(nlogn) O(nlogn) O(nlogn) O(1) 不稳定 归并排序 O(nlogn) O(nlogn) O(nlogn) O(n) 稳定 冒泡排序：两层循环，外循环确定起始比较位置，内循环不断移动，与后一位比较判断是否交换 选择排序：两层循环，外循环控制选择次数，内循环遍历取得最值 插入排序：两层循环，外循环控制选择插入的对象，内循环向前遍历插入位置 希尔排序：不断迭代缩小步距的插入排序，通过大步距减少插入排序中向前遍历的次数 快速排序：确定序列中的关键值，根据关键值大小划分序列，子序列继续快排 归并排序：迭代到两两排序，再逐层合并排序结果 桶排序 基数排序 计数排序","link":"/2024/08/23/algorithm/sort/"},{"title":"栈算法题目集合","text":"leetcode71 简化路径 给你一个字符串 path ，表示指向某一文件或目录的 Unix 风格 绝对路径 （以 '/' 开头），请你将其转化为更加简洁的规范路径。 在 Unix 风格的文件系统中，一个点（.）表示当前目录本身；此外，两个点 （..） 表示将目录切换到上一级（指向父目录）；两者都可以是复杂相对路径的组成部分。任意多个连续的斜杠（即，'//'）都被视为单个斜杠 '/' 。 对于此问题，任何其他格式的点（例如，'...'）均被视为文件/目录名称。 请注意，返回的 规范路径 必须遵循下述格式： 始终以斜杠 '/' 开头。 两个目录名之间必须只有一个斜杠 '/' 。 最后一个目录名（如果存在）不能 以 '/' 结尾。 此外，路径仅包含从根目录到目标文件或目录的路径上的目录（即，不含 '.' 或 '..'）。 返回简化后得到的规范路径。 题解 参考题解 leetcode150 逆波兰表达式求值 给你一个字符串数组 tokens ，表示一个根据 逆波兰表示法 表示的算术表达式。 请你计算该表达式。返回一个表示表达式值的整数。 注意： 有效的算符为 '+'、'-'、'*' 和 '/' 。 每个操作数（运算对象）都可以是一个整数或者另一个表达式。 两个整数之间的除法总是 向零截断 。 表达式中不含除零运算。 输入是一个根据逆波兰表示法表示的算术表达式。 答案及所有中间计算结果可以用 32 位 整数表示。 题解 从左向右遍历，用栈存储数字，遇到符号出栈两个数字 12345678910111213141516171819202122232425262728class Solution {public: int evalRPN(vector&lt;string&gt;&amp; tokens) { long result = 0; stack&lt;int&gt; nums; for (string token : tokens) { if (token != \"+\" &amp;&amp; token != \"-\" &amp;&amp; token != \"*\" &amp;&amp; token != \"/\") { nums.push(stoi(token)); continue; } int n2 = nums.top();nums.pop(); int n1 = nums.top();nums.pop(); if (token == \"+\") { nums.push(n1 + n2); } else if (token == \"-\") { nums.push(n1 - n2); } else if (token == \"*\") { nums.push(n1 * n2); } else if (token == \"/\") { nums.push(n1 / n2); } } return nums.top(); }}; leetcode84 柱状图中最大的矩形 给定 n 个非负整数，用来表示柱状图中各个柱子的高度。每个柱子彼此相邻，且宽度为 1 。 求在该柱状图中，能够勾勒出来的矩形的最大面积。 单调栈获取两端更小值的索引 刚接触到题目以为与最大盛水量是相同解法，这题的不同之处在于面积取决于区间内的最小值，而不是两端的最小值 由此，对于柱高上升的区间，起始柱总能取其高计算面积，若新加入的柱高小于栈顶高，则表示已到达栈顶对应开区间的右端点，将栈顶弹出，新的栈顶表示开区间的左端点 1234567891011121314151617181920int largestRectangleArea(vector&lt;int&gt;&amp; heights) { int maxS = 0; // 方便计算开头与末尾的柱面积 heights.insert(heights.begin(), 0); heights.emplace_back(0); int n = heights.size(); // 单调栈 stack&lt;int&gt; rise; rise.emplace(0); for (int i = 1; i &lt; n; i ++) { // 触及右边界，出栈直到栈再次单调或为空 while (! rise.empty() &amp;&amp; heights[rise.top()] &gt; heights[i]) { int h = heights[rise.top()]; rise.pop(); int w = i - rise.top() - 1; maxS = max(maxS, h * w); } rise.emplace(i); } return maxS;} leetcode85 最大矩形 给定一个仅包含 0 和 1 、大小为 rows x cols 的二维二进制矩阵，找出只包含 1 的最大矩形，并返回其面积。 逐行求解最大矩形 12345678910111213141516171819202122232425262728int maximalRectangle(vector&lt;vector&lt;char&gt;&gt;&amp; matrix) { int maxS = 0; int m = matrix.size(), n = matrix[0].size(); vector&lt;int&gt; heights(n + 2, 0); stack&lt;int&gt; rise; // 遍历每一行 for (int i = 0; i &lt; m; i ++) { rise.emplace(0); // 计算当前高度 for (int j = 0; j &lt; n + 1; j ++) { if (j &lt; n) { // 若为0则高度置0 if (matrix[i][j] - '0') heights[j + 1] += 1; // 若为1高度加1 else heights[j + 1] = 0; } // 区间判断 while (! rise.empty() &amp;&amp; heights[rise.top()] &gt; heights[j + 1]) { int h = heights[rise.top()]; rise.pop(); int w = (j + 1) - rise.top() - 1; maxS = max(maxS, h * w); } rise.emplace(j + 1); } while (! rise.empty()) rise.pop(); } return maxS;} leetcode739 每日温度 给定一个整数数组 temperatures ，表示每天的温度，返回一个数组 answer ，其中 answer[i] 是指对于第 i 天，下一个更高温度出现在几天后。如果气温在这之后都不会升高，请在该位置用 0 来代替。 单调递减栈 若当前温度比当前栈顶高，则必为最近一个高温，否则记录在栈中待找寻下一个高温 1234567891011121314vector&lt;int&gt; dailyTemperatures(vector&lt;int&gt;&amp; temperatures) { int n = temperatures.size(); vector&lt;int&gt; ret(n, 0); stack&lt;int&gt; down; down.emplace(0); for (int i = 1; i &lt; n; i ++) { while (! down.empty() &amp;&amp; temperatures[down.top()] &lt; temperatures[i]) { ret[down.top()] = i - down.top(); down.pop(); } down.emplace(i); } return ret;}","link":"/2024/07/12/algorithm/stack/"},{"title":"字符串题目集合","text":"字符串输入流 库 sstream 类 stringstream leetcode58 最后一个单词的长度 给你一个字符串 s，由若干单词组成，单词前后用一些空格字符隔开。返回字符串中 最后一个 单词的长度。 单词 是指仅由字母组成、不包含任何空格字符的最大子字符串。 123456int lengthOfLastWord(string s) { stringstream ss(s); string word; while (ss &gt;&gt; word) ; return word.size();} KMP 算法 在匹配之前需要计算匹配字符串各个位置的最长重合前后缀。初始化第一个位置为 0，后缀末尾位置从 1 开始，若当前前缀的末尾与后缀末尾相同，则更新长度；否则根据当前前缀寻找新的重合前缀，这个阶段是最难理解的部分，如下图所示。 kmp leetcode28 找出字符串中第一个匹配项的下标 给你两个字符串 haystack 和 needle ，请你在 haystack 字符串中找出 needle 字符串的第一个匹配项的下标（下标从 0 开始）。如果 needle 不是 haystack 的一部分，则返回 -1 。 1234567891011121314151617181920212223int strStr(string haystack, string needle) { // next数组表示到该位置的字串的最大相同前后缀的长度 vector&lt;int&gt; next(needle.size()); int j = 0; next[0] = 0; for (int i = 1, len = needle.size(); i &lt; len; i ++) { // 末尾不相同则查找以当前前缀的最大重复长度 while (j &amp;&amp; needle[i] != needle[j]) j = next[j - 1]; // 末尾相同则记录长度 if (needle[i] == needle[j]) j ++; next[i] = j; } // 通过回溯重合位置减少遍历次数 for (int i = 0, j = 0, len = haystack.size(); i &lt; len; i ++) { // 若当前位置不匹配则变换比对位置 while (j &amp;&amp; haystack[i] != needle[j]) j = next[j - 1]; // 若比对成功则向后比对 if (haystack[i] == needle[j]) j ++; // 判断是否已完成比对 if (j == needle.size()) return i - needle.size() + 1; } return -1;} leetcode767 重构字符串 返回 s 的任意可能的重新排列。若不可行，返回空字符串 \"\" 。 题解 参考题解 leetcode228 汇总区间 给定一个 无重复元素 的 有序 整数数组 nums 。 返回 恰好覆盖数组中所有数字 的 最小有序 区间范围列表 。也就是说，nums 的每个元素都恰好被某个区间范围所覆盖，并且不存在属于某个范围但不属于 nums 的数字 x 。 列表中的每个区间范围 [a,b] 应该按如下格式输出： \"a-&gt;b\" ，如果 a != b \"a\" ，如果 a == b 思路 参考leetcode 题解 to_string 方法快速将数字转换为字符串 使用 i 和 j 表示区间的端点 在遍历的过程中，若位置 j 与位置 j+1 的值相差 1，则向右拓展区间，从而找到完整区间 123456789101112131415161718192021class Solution {public: vector&lt;string&gt; summaryRanges(vector&lt;int&gt;&amp; nums) { vector&lt;string&gt; ans; // Lambda 函数构建字符串 auto f = [&amp;](int i, int j) { return i == j ? to_string(nums[i]) : to_string(nums[i]) + \"-&gt;\" + to_string(nums[j]); }; // i 在迭代时为上一个区间结尾 + 1 for (int i = 0, j, n = nums.size(); i &lt; n; i = j + 1) { j = i; // 若差为 1，右端点移动 while (j + 1 &lt; n &amp;&amp; nums[j + 1] == nums[j] + 1) { ++ j; } // 添加区间 ans.emplace_back(f(i, j)); } return ans; }}; leetcode820 单词的压缩编码 单词数组 words 的 有效编码 由任意助记字符串 s 和下标数组 indices 组成，且满足： words.length == indices.length 助记字符串 s 以 '#' 字符结尾 对于每个下标 indices[i] ，s 的一个从 indices[i] 开始、到下一个 '#' 字符结束（但不包括 '#'）的 子字符串 恰好与 words[i] 相等 给你一个单词数组 words ，返回成功对 words 进行编码的最小助记字符串 s 的长度 。 Trie快速判断末尾重合字串 若字符串s1完全与字符串s2的末尾部分或整体重合，则字符串s1可作为同一部分助记字符串 因此，可以使用Trie构建字符串树 为了正确计算包含关系，需要优先按照字符串长度倒序排序字符串 123456789101112131415161718192021222324252627282930313233343536373839404142434445class Solution {private: // 使用长度为26的数组存储下一个字符 struct Node { vector&lt;Node*&gt; next; };public: int minimumLengthEncoding(vector&lt;string&gt;&amp; words) { int n = words.size(); // 初始化根节点 Node* root = new Node(); root-&gt;next.resize(26, nullptr); Node* p = nullptr; // 按字符串长度排序 sort(words.begin(), words.end(), [&amp;](const string&amp; a, const string&amp; b) { return a.size() &gt; b.size(); }); int count = 0; // 遍历各个字符串 for (int i = 0; i &lt; n; i ++) { bool include = true; string&amp; w = words[i]; p = root; // 从字符串尾部开始遍历 for (int j = w.size() - 1; j &gt;= 0; j --) { int charIndex = w[j] - 'a'; // 若为非空指针则表示已有该字符序列 if (p-&gt;next[charIndex]) { p = p-&gt;next[charIndex]; } // 创建新的字符 else { include = false; // 标识为新字符串 Node* newNode = new Node(); newNode-&gt;next.resize(26, nullptr); p-&gt;next[charIndex] = newNode; p = newNode; } } // 若为新字符串则加上当前字符串长度以及\"#\"的长度 if (! include) count += w.size() + 1; } return count; }};","link":"/2024/08/01/algorithm/string/"},{"title":"Hexo操作","text":"Hexo 在子目录中新建文章 1hexo new [layout] -p path/&lt;title&gt;","link":"/2024/07/08/hexo/basic/"},{"title":"树算法题目集合","text":"leetcode199 二叉树的右视图 给定一个二叉树的根节点 root，想象自己站在它的右侧，按照从顶部到底部的顺序，返回从右侧所能看到的节点值。 leetcode230 二叉搜索树中第 K 小的元素 给定一个二叉搜索树的根节点 root ，和一个整数 k ，请你设计一个算法查找其中第 k 小的元素（从 1 开始计数）。 题解 root-&gt;left-&gt;val &lt; root-&gt;val &lt; root-&gt;right-&gt;val 12345678910111213141516171819202122232425int kthSmallest(TreeNode* root, int k) { int count = 0; stack&lt;TreeNode*&gt; treeStack; treeStack.push(root); while (treeStack.size() &gt; 0) { TreeNode* base = treeStack.top(); if (base-&gt;left == nullptr) { count ++; treeStack.pop(); if (count == k) { return base-&gt;val; } if (base-&gt;right != nullptr) { treeStack.push(base-&gt;right); continue; } } else { treeStack.push(base-&gt;left); base-&gt;left = nullptr; continue; } } return 0;} leetcode148 排序链表 给你链表的头结点 head ，请将其按升序排列并返回排序后的链表 题解 归并排序链表，递归分治，子链表按序合并 树和算式 Leetcode 题解 各种表达式没有本质区别，他们其实是同一个语法树，只是遍历方式不同而得到的不同式子；是一个事物的一体多面，只不过是从不同角度观察罢了。 中缀表达式是其对应的语法树的中序遍历； 后缀表达式是其对应的语法树的后序遍历； 前缀表达式是其对应的语法树的前序遍历； leetcode105 从前序与中序遍历序列构造二叉树 给定两个整数数组 preorder 和 inorder ，其中 preorder 是二叉树的先序遍历， inorder 是同一棵树的中序遍历，请构造二叉树并返回其根节点。 题解 前序遍历提供根节点，中序遍历提供子树结构 Leetcode 题解 1234567891011121314151617181920class Solution {public: TreeNode* buildTree(vector&lt;int&gt;&amp; preorder, vector&lt;int&gt;&amp; inorder) { this-&gt;preorder = preorder; for(int i = 0; i &lt; inorder.size(); i++) dic[inorder[i]] = i; return recur(0, 0, inorder.size() - 1); }private: vector&lt;int&gt; preorder; unordered_map&lt;int, int&gt; dic; TreeNode* recur(int root, int left, int right) { if (left &gt; right) return nullptr; // 递归终止 TreeNode* node = new TreeNode(preorder[root]); // 建立根节点 int i = dic[preorder[root]]; // 划分根节点、左子树、右子树 node-&gt;left = recur(root + 1, left, i - 1); // 开启左子树递归 node-&gt;right = recur(root + i - left + 1, i + 1, right); // 开启右子树递归 return node; // 回溯返回根节点 }}; leetcode94 树的中序遍历 中序遍历 使用栈数据结构控制遍历节点顺序，类比递归调用的栈 12345678910111213141516171819202122232425262728293031323334/** * Definition for a binary tree node. * struct TreeNode { * int val; * TreeNode *left; * TreeNode *right; * TreeNode() : val(0), left(nullptr), right(nullptr) {} * TreeNode(int x) : val(x), left(nullptr), right(nullptr) {} * TreeNode(int x, TreeNode *left, TreeNode *right) : val(x), left(left), right(right) {} * }; */class Solution {public: vector&lt;int&gt; inorderTraversal(TreeNode* root) { vector&lt;int&gt; ret; if (root == nullptr) return ret; stack&lt;TreeNode*&gt; tree; tree.push(root); while (! tree.empty()) { TreeNode* top = tree.top(); if (top-&gt;left != nullptr) { tree.push(top-&gt;left); top-&gt;left = nullptr; continue; } ret.push_back(top-&gt;val); tree.pop(); if (top-&gt;right != nullptr) { tree.push(top-&gt;right); } } return ret; }}; 通过递归重复根节点操作 leetcode124 二叉树中的最大路径和 路径 被定义为一条从树中任意节点出发，沿父节点-子节点连接，达到任意节点的序列。同一个节点在一条路径序列中 至多出现一次 。该路径 至少包含一个 节点，且不一定经过根节点。 路径和 是路径中各节点值的总和。 给定一个二叉树的根节点 root ，返回其 最大路径和，即所有路径上节点值之和的最大值。 思路 共有两种路径，一种是左右节点跨越根的环形路径，一种为父子节点的直线路径 在递归的过程中，若子树的值为负，对最大和无贡献，可置为 0，计算左右子树和根节点的和，更新当前最大值 注意递归的返回值需包含根节点，已构成完整路径 12345678910111213141516171819class Solution {private: int maxSum = 0;public: int getMax(TreeNode_ root) { if (! root) return 0; int left = max(0, getMax(root-&gt;left)); int right = max(0, getMax(root-&gt;right)); maxSum = max(maxSum, root-&gt;val + left + right); return root-&gt;val + max(left, right); } int maxPathSum(TreeNode* root) { maxSum = INT_MIN; getMax(root); return maxSum; }}; 完全二叉树 完全二叉树的定义如下：在完全二叉树中，除了最底层节点可能没填满外，其余每层节点数都达到最大值，并且最下面一层的节点都集中在该层最左边的若干位置。若最底层为第 h 层，则该层包含 1~ 2^h 个节点。 leetcode222 完全二叉树的节点个数 给你一棵 完全二叉树 的根节点 root ，求出该树的节点个数。 比较有趣的思路：参考 比较左右子树高度，若相等则左子树必为满树，若不相等则右子树必为满树 123456789101112131415161718192021222324class Solution {public: // 计算子树高度 int getHeight(TreeNode* root) { if (! root) return 0; return 1 + max(getHeight(root-&gt;left), getHeight(root-&gt;right)); } // 计算子树节点数 int countNodes(TreeNode* root) { if (! root) return 0; int leftHeight = getHeight(root-&gt;left); int rightHeight = getHeight(root-&gt;right); // 比较子树高度 if (leftHeight == rightHeight) { // 递归计算子树节点数，累加完全树节点数 return countNodes(root-&gt;right) + (1 &lt;&lt; leftHeight); } else { return countNodes(root-&gt;left) + (1 &lt;&lt; rightHeight); } }};","link":"/2024/07/16/algorithm/tree/"},{"title":"大疆笔试","text":"基础知识考察 题型分析 20230806 题型：10 单选 + 10 多选 + 10 判断 2025 题型：一个半小时，20 选择 + 10 多选 + 10 判断 算法 数据结构 常见排序算法 编程基础 C++ Python static 的作用 数学原理 对求导 导数： 分子布局： 的列向量形式 人工智能神经网络 CNN SVM BN 池化 卷积过程中的乘法次数 一个 卷积（step=1,dilation=1） + 2 步长 池化加 卷积（step=1,dilation=1），问等价于步长：8？ 模型训练 Triplet loss 是一种常用于度量学习（metric learning）中的损失函数 Hard triplet 是指那些最难区分的样本对，即正例对之间的距离较大，负例对之间的距离较小。使用全部的 hard triplets 可以帮助模型更有效地学习样本之间的距离关系，从而提高模型的泛化能力和分类性能。在实际应用中，需要注意的是，筛选 hard triplets 会增加计算复杂度，因此在大规模数据集上可能需要使用更高效的筛选策略。 SLAM（Simultaneous Localization and Mapping） 主要任务：定位和构图（机器人所在位置和场景） LiDAR SLAM：使用激光雷达（LiDAR）数据进行环境感知 LiDAR SLAM 的点线匹配自由度：3 二维空间中的点线匹配，通常考虑的自由度包括平移（x 方向和 y 方向）和旋转（θ），因此总共有 3 个自由度。而在三维空间中，则会增加一个维度的平移和平面外的一个旋转，共6个自由度（三个平移自由度和三个旋转自由度） LiDAR SLAM 去运动畸变时：利用其他传感器（如 IMU）的同步数据；利用连续几帧点云数据 如何提升重复纹理环境感知能力：（网友答案）我写的是用时域方法以及用更精确的匹配方法 Numpy a=np.random.randn(3，3)，b=np.random.randn(3,1)，c=a*b，问c等于：a形状为(3, 3)，b形状为(3, 1)，NumPy 的广播机制将 b 扩展到 (3, 3) 形状，然后对a和b做逐元素乘法 感受野 感受野是指网络中某一层的单个神经元所能够接收到的输入图像区域的大小。在卷积神经网络（CNNs）中，感受野的大小主要由卷积核的大小、步长（stride）、填充（padding）以及网络的深度决定。例如，较大的卷积核或更深的网络会导致更大的感受野，因为每一层都会扩大其对输入图像的感受范围 ReLu、BN 和 Dropout 不能改变感受野 图像处理基础知识 相机知识 颜色 插值 滤波 特征提取 gamma 变换的函数表达式 给定一个序列 X 的分布(G(0, 1))，Y 和 X 的关系式，求序列 Y 的方差 中值滤波 常见边缘提取算子 一阶：Sobel 算子、Prewitt 算子、Roberts 算子 二阶：Laplacian 算子、Canny 算法 光流估计方法 L-K 光流（Lucas-Kanade 光流） 基本假设：相邻帧之间的像素灰度值保持不变（亮度恒定假设）/ 时空和亮度不变性假设 相机 图像分辨率缩小一半，内参数发生的变化：除了畸变系数，焦距 fx，fy，和光心 cx，cy 全部减半 曝光不变、静止多帧平均是一种技术，用于通过合并多帧相同曝光条件下的图像来减少图像中的噪声。这种方法不能改变图像的动态范围，但它确实有助于提高图像质量，特别是减少噪声 编程考察 题型分析 2025 两道编程（第二题二选一） 高频题目 Leetcode LCR 051. 二叉树中的最大路径和 Leetcode 174. 地下城游戏 LCP 35. 电动车游城市","link":"/2024/08/16/interview/dajiang/"},{"title":"深信服算法面试","text":"深度学习书籍 手撕 最长递增子序列 给你一个整数数组 nums ，找到其中最长严格递增子序列的长度。 子序列 是由数组派生而来的序列，删除（或不删除）数组中的元素而不改变其余元素的顺序。例如，[3,6,2,7] 是数组 [0,3,1,6,2,2,7] 的子序列。 排序链表 给你链表的头结点 head ，请将其按 升序 排列并返回 排序后的链表 基础知识 Transformer 为什么需要 SoftMax 手撕 Attention mutihead-attention qkv 矩阵 位置编码 介绍熟悉领域的经典模型演变以及优缺点 Loss 函数 手撕模块实现 手撕 BN，NMS，随机梯度下降，简单卷积操作，简单池化操作，IOU 计算（单个和 batch）","link":"/2024/09/23/interview/shenxinfu/"},{"title":"人工智能代理（Artificial Intelligence Agent）","text":"AI Agent深度解析：潜力与挑战并存的智能新世界 定义 感知环境、进行决策和执行动作的智能实体 应用 ABI/GBI生成式BI或是数据分析 Code Agent代码助手 基于RAG技术的知识问答 Coding Agent aiXcoder 代码大模型在企业的应用实践 PPT 语言模型发展历程 深度神经网络 预训练模型 大语言模型 论文 aiXcoder构建 训练数据收集与筛选 Github数据爬取与过滤 删除低质量项目 相似算法去重 去除敏感数据 去除注释数据 删除语法错误代码 删除静态分析缺陷代码 代码文件顺序重排 训练 任务 随机Mask抽象语法树节点 存在问题 项目级代码生成下的长序列依赖 测试 更符合真实场景的测评集 基于CodeFuse的下一代研发探索 PPT Github 数据来源 Github 预训练&amp;微调 推理加速&amp;部署 发展 仓库级代码 大模型落地到代码助手场景的探索实践 PPT 提示工程 基本原理 单个、定义明确的任务或问题上 说明明确且详细，最好附带示例 简明扼要 百度大模型驱动下的智能代码助手提效实践 PPT","link":"/2024/07/09/knowledge/ai-agent/"},{"title":"C++语言基础知识","text":"STL 容器（AI辅助生成、待检错） 1. vector 头文件: &lt;vector&gt; 描述: 动态数组，能够自动调整大小 底层结构: 数组 插入操作: emplace_back, insert 访问操作: front, back, operator[], at 删除操作: pop_back, erase 应用场景: 当需要快速随机访问元素且不需要频繁在中间位置插入或删除元素时 string，头文件，使用类 vector 的数据结构 2. list 头文件: &lt;list&gt; 描述: 双向链表 底层结构: 双向链表 插入操作: emplace_front, emplace_back, insert 访问操作: front, back 删除操作: pop_front, pop_back, remove, erase 应用场景: 需要频繁地在列表的任意位置进行插入和删除操作时 3. queue 头文件: &lt;queue&gt; 描述: 队列，先进先出(FIFO)的数据结构。 底层结构: 通常由适配器实现，底层使用其他容器如 deque 或 list 插入操作: push 访问操作: front, back 删除操作: pop 应用场景: 实现 FIFO 操作逻辑，例如任务调度。 4. priority_queue 头文件: &lt;queue&gt; 描述: 优先级队列，总是弹出具有最高优先级的元素。 底层结构: 通常由适配器实现，底层使用堆数据结构，如 std::vector 作为基础容器。 插入操作: push 访问操作: top 删除操作: pop 应用场景: 需要快速获取最高优先级元素的场景。 5. deque (double-ended queue) 头文件: &lt;deque&gt; 描述: 双端队列，可以在两端进行高效插入和删除 底层结构: 分配的连续内存块组成的数组 插入操作: emplace_front, emplace_back, push_front, push_back, insert 访问操作: front, back, operator[], at 删除操作: pop_front, pop_back, erase 应用场景: 当需要从两端高效地添加或移除元素时 6. stack 头文件: &lt;stack&gt; 描述: 栈，后进先出(LIFO)的数据结构。 底层结构: 通常由适配器实现，底层使用其他容器如 vector 或 deque 插入操作: push 访问操作: top 删除操作: pop 应用场景: 实现 LIFO 操作逻辑，例如表达式求值。 7. set 头文件: &lt;set&gt; 描述: 关联容器，存储唯一键值，自动排序 底层结构: 平衡二叉树 (通常为红黑树) 插入操作: emplace, insert 访问操作: find, lower_bound, upper_bound 删除操作: erase 应用场景: 当需要维护一个有序集合且不允许重复元素时 8. map 头文件: &lt;map&gt; 描述: 关联容器，存储键值对，键值唯一并自动排序 底层结构: 平衡二叉树 (通常为红黑树) 插入操作: emplace, insert 访问操作: find, lower_bound, upper_bound 删除操作: erase 应用场景: 当需要通过键来高效查找、插入和删除数据时 9. unordered_set 头文件: &lt;unordered_set&gt; 描述: 哈希表实现的容器，存储唯一键值 底层结构: 哈希表 插入操作: emplace, insert 访问操作: find, count 删除操作: erase 应用场景: 当需要快速查找和插入操作而不关心顺序时 10. unordered_map 头文件: &lt;unordered_map&gt; 描述: 哈希表实现的容器，存储键值对，键值唯一 底层结构: 哈希表 插入操作: emplace, insert 访问操作: find, count 删除操作: erase 应用场景: 当需要通过键来快速查找数据而不关心顺序时 11. pair 头文件: &lt;utility&gt; 描述: 存储两个元素的简单容器 底层结构: 两个成员变量 插入操作: 构造函数初始化 访问操作: first, second 删除操作: N/A (不可删除) 应用场景: 当需要同时处理两个相关数据时 12. bitset 头文件: &lt;bitset&gt; 描述: 存储位序列 底层结构: 内部实现细节依赖于实现 插入操作: 构造函数初始化, set 访问操作: test, operator[] 删除操作: reset 应用场景: 高效处理位操作，例如用于压缩算法、位图等 13. array 头文件: &lt;array&gt; 描述: 固定大小的数组容器。 底层结构: 数组 插入操作: N/A (不支持插入操作) 访问操作: operator[], at, front, back 删除操作: N/A (不支持删除操作) 应用场景: 当需要固定大小的数组且不需要动态调整大小时。 无符号和有符号 12unsigned short int i=(unsigned short int)(-1234);printf(\"%hu\\n\",i);","link":"/2024/08/21/knowledge/c++/"},{"title":"计算机基础知识汇总","text":"操作系统 进程状态 进程状态及转换 二进制存储 参考博客讲的很清楚 原码 反码 补码 移码 正则匹配 表达式 含义 [^0-9] 不包含字符 0-9 (a | b)* 若子表达式存在则只包含字符 a 或 b c+ 匹配至少一个字符 c","link":"/2024/06/27/knowledge/computer-fundamental/"},{"title":"计算机视觉 Computer Vision","text":"【三年面试五年模拟】算法工程师的求职面试秘籍 &gt; 从 ReLU 到 GELU，一文概览神经网络的激活函数 https://github.com/DWCTOD/interview/blob/master/detail/%E4%BD%9C%E4%B8%9A%E5%B8%AE%20%E8%A7%86%E8%A7%89%E7%AE%97%E6%B3%95%E5%B7%A5%E7%A8%8B%E5%B8%88%20%E9%9D%A2%E7%BB%8F%EF%BC%882020%E5%B1%8A%EF%BC%89.md https://github.com/GYee/CV_interviews_Q-A 图像处理与计算机视觉基础 基本概念 原理 图像预处理 特征提取 对象检测 图像分类 图像分割 OpenCV 库 读写图像 图像滤波 几何变换 特征检测与描述 深度学习基础 梯度下降 滑动平均 模型微调（Fine-tuning） 基础模块 池化层 Pooling Layer 归一化层 Normalization Layer BN，Batch Normalization IN，Instance Normalization LN，Layer Normalization GN，Group Normalization BN 是怎么做，作用是什么 激活层 常见激活函数 Sigmoid Tanh ReLU LeakyReLU SoftPlus ELU SELU，自归一化 Swish，类 Sigmoid 作为开关 GELU GLU 特性 梯度消失 存在偏导过小 梯度爆炸 偏导累乘过大 梯度裁剪 输出均值为 0 能避免每次权重只能往单一反向变化 ReLU 计算复杂度低 ReLU 的负半轴为输出值增加稀疏性，减少计算量，但同时会让一些神经元不能更新 SoftPlus，ReLU 的平滑 全连接层 Linear 嵌入层 Embedding 卷积层 Convolution 特征 局部感知、权值共享、平移不变、多核 1×1 卷积 特征增强 特征融合 改变通道数 分类 空洞卷积 分组卷积 转置卷积层 Transpose Convolution 优化模块 残差结构 Residual Connection 将输入与层输出相加 优势 缓解梯度消失，增加网络深度 保留信息，特征重用 空间金字塔池化（Spatial Pyramid Pooling，SPP） 空洞空间金字塔池化（Atrous Spatial Pyramid Pooling，ASPP） HDC 可变形卷积 Deformable Convolution 可分离卷积 Separable Convolution Transformer 模块结构 多头自注意力机制（Multi-Head Self-Attention Mechanism） 前馈神经网络（Feed-Forward Neural Network） 层归一化（Layer Normalization） 残差连接（Residual Connections） Self-Attention 除以的原因 矩阵计算导致的元素值整体偏大，从而引发梯度消失 计算后的数据平方差为，除以，将分布的方差纠正回接近 1 并行化的体现 序列计算多头注意力 影响计算量的因素 序列长度：点积、矩阵乘 头数量 优势 并行处理整个序列 长距离依赖 缺点 计算量大 超参调优 超长序列处理能力 Cross-Attention Convolutional Attention SENet -CBAM 基础模型 前馈神经网络 卷积神经网络（CNN） 循环神经网络（RNN） 长短时记忆网络（LSTM） Inception 模型调优 模型优化 正则化 L1 正则化 L2 正则化 损失函数 已知 softmax 输出概率序列与实际分布概率序列，计算两者交叉熵 超参数调整 在深度学习中，超参数（Hyperparameters）是指在训练开始前设置的模型参数，不是通过训练学习得到的。超参数的选择对模型性能有很大的影响，不同的超参数设置可能导致显著不同的训练结果。 优化器选择 SGD AdaGrad RMSProp Adam 学习率衰减 LR 中的连续值特征是如何处理的 为什么 LR 要先对数据进行归一化处理 LR 用了 sigmoid 函数，那么 LR 是线性模型还是非线性模型，为什么 线性 分段 余弦 WarmUp 周期性 缩放法则 Scaling-Law 在 AI 领域中，描述模型性能如何随着模型规模（如参数数量、训练数据量、计算资源等）变化而变化的一组经验法则 应用 设计更大规模的模型 指导研究人员如何设计和训练更大规模的模型，以实现更高的性能 优化资源分配 如确定是否应增加模型参数数量、增加训练数据量，还是增加计算资源，以实现最优的性能提升 预测性能 根据现有模型的性能和缩放法则，可以预测更大规模模型的性能 常见模型评估指标 准确率 召回率 F1 分数 浮点数运算次数 FLOPs 帧每秒 FPS 深度学习框架 训练范式 123456789101112131415161718192021222324252627# 加载数据dataloader = DataLoader()# 加载模型model = MyModel()# 损失函数criterion = nn.CrossEntropyLoss()# 优化器optimizer = optim.Adam(model.parameters(), lr=0.001)# 学习率衰减策略# 训练num_epochsfor epoch in range(num_epochs): # 遍历完整数据集 for inputs, labels in dataloader: # 梯度置零 optimizer.zero_grad() # 模型推理 outputs = model(inputs) # 计算损失 loss = criterion(outputs, labels) # 累加梯度 loss.backward() # 梯度更新 optimizer.step() 评估范式 1234567# 设置模型评估模式model.eval()# 取消梯度更新with torch.no_grad(): for inputs, labels in test_dataloader: outputs = model(inputs) # 计算准确率或其他指标 PyTorch Tensor 存储 头信息区（Tensor）：tensor 的形状（size）、步长（stride）、数据类型（type）等 存储区（Storage）：数据 stride 属性 指定维度中一个元素到下一个元素的步长 维度变换 类型 方法 描述 维度顺序 permute 指定维度重排，返回共享存储区的 tensor transpose 交换维度，返回共享存储区的 tensor 形状变换 view 返回共享存储区 tensor，要求存储连续，否则调用 contiguous contiguous 开辟新的存储区构建连续 tensor reshape 若连续则返回原 tensor，否则创建新 tensor 广播 broadcast_to 冗余维度 squeeze 压缩维度 unsqueeze 展开维度 扩展维度 expand 扩展大小为 1 的维度 repeat 按照指定维度重复 tensor 展平维度 flatten ravel 维度剪裁 narrow 维度展开 unfold 张量乘法 方法 应用 torch.matmul() 多维矩阵相乘 torch.mm() 2 维矩阵相乘 torch.bmm() 批矩阵相乘 torch.dot() 点积 torch.mv() 矩阵向量相乘 torch.einsum() 复杂张量运算 torch.einsum('ij,jk-&gt;ik', a, b) 张量合并与拆分 stack 扩展维度拼接 cat 根据维度拼接 split 按大小分 chunk 按块分 nn.Module 模块基类 nn.Sequential 线性模块容器 计算图 环境搭建 数据加载 模型定义 训练 验证 保存 加载模型 PytorchLightning 逻辑思维与项目经验 逻辑思维 准备通过解决实际问题来展示你的逻辑思维能力和数据分析洞察力，可以是以往项目中的案例分析。 团队合作与挑战接受度 思考并准备实例说明你如何在团队中有效沟通、协作解决问题，以及面对技术挑战时的态度和解决策略。","link":"/2024/06/26/knowledge/computer-vision/"},{"title":"computer-network","text":"网络字节流 参考连接 网络字节流是指网络传输时先后到达的字节 网络字节序编码是大端序（高位在低地址） 数值0x01020304在大端机器内存中地址排列为：[0x01,0x02,0x03,0x04] HTTP http 中的 get、post 区别，是怎么做的","link":"/2024/06/26/knowledge/computer-network/"},{"title":"计算机视觉 Computer Vision","text":"【三年面试五年模拟】算法工程师的求职面试秘籍 &gt; 从 ReLU 到 GELU，一文概览神经网络的激活函数 https://github.com/DWCTOD/interview/blob/master/detail/%E4%BD%9C%E4%B8%9A%E5%B8%AE%20%E8%A7%86%E8%A7%89%E7%AE%97%E6%B3%95%E5%B7%A5%E7%A8%8B%E5%B8%88%20%E9%9D%A2%E7%BB%8F%EF%BC%882020%E5%B1%8A%EF%BC%89.md https://github.com/GYee/CV_interviews_Q-A 图像处理与计算机视觉基础 基本概念 原理 图像预处理 特征提取 对象检测 图像分类 图像分割 OpenCV 库 读写图像 图像滤波 几何变换 特征检测与描述 深度学习基础 梯度下降 滑动平均 模型微调（Fine-tuning） 基础模块 池化层 Pooling Layer 归一化层 Normalization Layer BN，Batch Normalization IN，Instance Normalization LN，Layer Normalization GN，Group Normalization BN 是怎么做，作用是什么 激活层 常见激活函数 Sigmoid Tanh ReLU LeakyReLU SoftPlus ELU SELU，自归一化 Swish，类 Sigmoid 作为开关 GELU GLU 特性 梯度消失 存在偏导过小 梯度爆炸 偏导累乘过大 梯度裁剪 输出均值为 0 能避免每次权重只能往单一反向变化 ReLU 计算复杂度低 ReLU 的负半轴为输出值增加稀疏性，减少计算量，但同时会让一些神经元不能更新 SoftPlus，ReLU 的平滑 全连接层 Linear 嵌入层 Embedding 卷积层 Convolution 特征 局部感知、权值共享、平移不变、多核 1×1 卷积 特征增强 特征融合 改变通道数 分类 空洞卷积 分组卷积 转置卷积层 Transpose Convolution 上采样 插值 反卷积 PixelShuffle 优化模块 残差结构 Residual Connection 将输入与层输出相加 优势 缓解梯度消失，增加网络深度 保留信息，特征重用 空间金字塔池化（Spatial Pyramid Pooling，SPP） 空洞空间金字塔池化（Atrous Spatial Pyramid Pooling，ASPP） HDC 可变形卷积 Deformable Convolution 可分离卷积 Separable Convolution Transformer 模块结构 多头自注意力机制（Multi-Head Self-Attention Mechanism） 前馈神经网络（Feed-Forward Neural Network） 层归一化（Layer Normalization） 残差连接（Residual Connections） Self-Attention 除以的原因 矩阵计算导致的元素值整体偏大，从而引发梯度消失 计算后的数据平方差为，除以，将分布的方差纠正回接近 1 并行化的体现 序列计算多头注意力 影响计算量的因素 序列长度：点积、矩阵乘 头数量 优势 并行处理整个序列 长距离依赖 缺点 计算量大 超参调优 超长序列处理能力 Cross-Attention Convolutional Attention SENet -CBAM 基础模型 前馈神经网络 卷积神经网络（CNN） 循环神经网络（RNN） 长短时记忆网络（LSTM） 模型调优 模型优化 正则化 L1 正则化 L2 正则化 损失函数 已知 softmax 输出概率序列与实际分布概率序列，计算两者交叉熵 超参数调整 在深度学习中，超参数（Hyperparameters）是指在训练开始前设置的模型参数，不是通过训练学习得到的。超参数的选择对模型性能有很大的影响，不同的超参数设置可能导致显著不同的训练结果。 优化器选择 SGD AdaGrad RMSProp Adam 学习率衰减 LR 中的连续值特征是如何处理的 为什么 LR 要先对数据进行归一化处理 LR 用了 sigmoid 函数，那么 LR 是线性模型还是非线性模型，为什么 线性 分段 余弦 WarmUp 周期性 缩放法则 Scaling-Law 在 AI 领域中，描述模型性能如何随着模型规模（如参数数量、训练数据量、计算资源等）变化而变化的一组经验法则 应用 设计更大规模的模型 指导研究人员如何设计和训练更大规模的模型，以实现更高的性能 优化资源分配 如确定是否应增加模型参数数量、增加训练数据量，还是增加计算资源，以实现最优的性能提升 预测性能 根据现有模型的性能和缩放法则，可以预测更大规模模型的性能 常见模型评估指标 准确率 召回率 F1 分数 浮点数运算次数 FLOPs 帧每秒 FPS 深度学习框架 训练范式 PyTorch Tensor 存储 头信息区（Tensor）：tensor 的形状（size）、步长（stride）、数据类型（type）等 存储区（Storage）：数据 stride 属性 指定维度中一个元素到下一个元素的步长 view 方法 返回共享存储区的 tensor 计算图 环境搭建 数据加载 模型定义 训练 验证 保存 加载模型 PytorchLightning 逻辑思维与项目经验 逻辑思维 准备通过解决实际问题来展示你的逻辑思维能力和数据分析洞察力，可以是以往项目中的案例分析。 团队合作与挑战接受度 思考并准备实例说明你如何在团队中有效沟通、协作解决问题，以及面对技术挑战时的态度和解决策略。","link":"/2024/06/26/knowledge/cv/"},{"title":"数据结构 Data Structure","text":"数组 用连续内存存储数据 读写操作复杂度 O(1) 字符串 用连续内存存储字符 链表 由指针把若干个节点连接成链状结构 树 节点之间用指针链接 除根节点之外每个节点只有一个父节点，根节点没有父节点 叶子节点没有子节点 二叉树 每个节点最多只能有两个子节点 二叉搜索树 若其左子树不为NULL，则左子树上所有节点的值都＜根节点的值 若其右子树不为NULL，则右子树上所有节点的值都＞根节点的值 其左右子树也分别是二叉搜索树 查询复杂度 堆 最大堆：根节点的值最大 最小堆：根节点的值最小 栈 先进后出 队列 先进先出 图 多线程 异常处理 算法 排序 算法 时间复杂度 稳定性 冒泡排序 √ 选择排序 × 归并排序 √ 快速排序 × 堆排序 × 查找 二分查找 动态规划","link":"/2024/06/26/knowledge/data-structure/"},{"title":"图像数据集 Image Dataset","text":"数据集建立 数据增强 AutoAugment：搜索最优图像处理操作组合 RandAugment：已有图像处理集合，确定操作次数 N 和操作幅度 M 分类任务 Mixup：按比例混合图像和标签 Cutout：图像掩码 CutMix：图像掩码混合 目标检测任务 Mosaic：图像合成 语义分割任务 Copy-Paste：分割结果粘贴到另一张图 数据预处理 数据增强 旋转、平移、缩放、翻转、随机色调(H)、饱和度(S)、明度(V)调整、等 数据归一化 Normalization Min-Max 归一化 Z-Score 归一化 RobustScaler 归一化 One-Hot 编码 数据类别不平衡问题 采样比例 数据生成","link":"/2024/06/26/knowledge/dataset/"},{"title":"deep-learning","text":"【三年面试五年模拟】算法工程师的求职面试秘籍 分类 Classification 将输入数据划分到预定义的有限标签中，输出为预测的类别标签 常用评价指标 准确率 精确率 召回率 F1 分数 应用 花卉图像分类 垃圾邮件拦截 回归 Regression 建立数值型随机自变量的模型并进行连续的因变量预测，输出为数值 常用评价指标 均方误差 R2 分数 应用 股票价格预测 房价预测 聚类 Clustering 将无标签的数据分成多个类（簇），确保类内样本相似，类间样本相异，其输出是聚类结果（簇划分，簇标签，簇中心等） 常用评价指标 样本紧密度 样本分隔度 应用 用户分群 异常检测 决策 Decision making 通过神经网络理解给定目标，约束条件和可用信息，预测出最佳或满意的动作决策，其输出是一连串的动作 常用评价指标 最终回报 平均奖励 应用 游戏 AI 自动驾驶 概率密度估计 Probability density estimation 使用深度神经网络来估计一个随机变量或一组随机变量的概率密度函数，其输出是数据的概率分布 常用评价指标衡量分布差异 对数似然损失 KL 散度 应用 数据生成 样本采样","link":"/2024/06/27/knowledge/deep-learning/"},{"title":"hpc","text":"GPU 编程与性能优化 CUDA 与 cuDNN GPU 编程基本原理 特别是如何使用 CUDA 进行并行计算，以及 cuDNN 库在加速深度学习中的应用 内存管理 核函数设计 性能监控与调优 性能优化 掌握一些基本的性能分析工具和方法，比如使用 nvprof 或 TensorFlow Profiler 分析模型运行瓶颈，并实施相应的优化措施。","link":"/2024/06/26/knowledge/hpc/"},{"title":"generative-model","text":"VAE AE VAE VAE GAN Diffusion GAN DDPM 生成过程被设计成一系列的马尔可夫步骤，其中每一个步骤只依赖于前一步的状态 DDIM 采样 DDIM 在每一步都会预测并尝试直接达到最终的清晰状态，而不是仅仅依赖于当前的模糊状态 DDPM 通过马尔科夫链推导下一状态，DDIM 通过减少推理步数加速推理 Stable Diffusion ControlNet RAG DIT 指标 MSE 计算公式 PSNR 单位：信噪比 计算公式 SSIM 通过均值和协方差衡量两张图像的 Luminance（亮度）、Contrast（对比度）和 Structure（结构） 计算公式 取值范围：[-1, 1]","link":"/2024/07/01/knowledge/generative-model/"},{"title":"图像处理","text":"1.对深度学习相关神经网络理解深入，如 DNN、CNN、RNN、GAN 等； 2.有深厚的理论研究背景和数据基础，熟悉 EM、MCMC、LR、LDA、PCA、时间序列等数学方法； 4.熟悉一种以上的深度学习的开源框架，如 Caffe、TensorFlow、ARMAILibrary、SNPE、OpenGLES 等。","link":"/2024/09/20/knowledge/image-process/"},{"title":"数学基础","text":"微积分 求导 梯度 偏微分 积分 统计学，概率论 推断统计学和概率论的基本概念 高斯分布 贝叶斯公式 最小二乘法 线性回归 逻辑回归 线性代数 向量 向量空间 向量投影 矩阵运算 特征值分解 SVD分解 其他 取石子问题 参考资料 一堆石子有10个，两个人A、B轮流从中取石子，规定每次至少取1个，最多取3个。取走最后石子的人获胜，在A先手的情况下请问必胜的是？(填A或者B) 解题思路：，所以A只要先取走2个石子，接下来每一轮，都能保证只剩下4的倍数颗石子给B，所以A必胜","link":"/2024/07/08/knowledge/math/"},{"title":"leetcode-host","text":"编程题： 逆时针打印数组 （剑指 offer 和 leetcode54 都有的常见题，常为顺时针打印数组） 给先序遍历重构二叉树 （例如输入为 124XXX3XX，X 表示空，无叶子节点） 有随机数 0-2 0-3 0-4 构建 100 的随机数 （使用 0-3 和 0-4 构建 20 与 0-4 构建的 5 形成 100 的随机数） 智力题： 49 个人中至少几个人生日是同一月 两个人只握一次手，一共握了 45 次，问一共几个人（10 人） 编程题： 数组合并（leetcode88）【简单】 区间合并，也叫线段合并（leetcode56）【中等】 以上内容+能否完全覆盖，题目为： 单个线段[2,6]可称为完全覆盖[4,6]，现有两组线段 AB，每组中有一定数目的线段，判断 A 组能否完全覆盖 B 组 例如： [[1, 3], [2, 6]] [[1, 4], [4, 5]] True [[1, 2], [4, 7]] [[2, 5], [6, 7]] False 非递归中序遍历 重建二叉树 根据前序和中序遍历，返回后序遍历 用两个队列实现一个栈 解法：一个队列放入，一个队列输出。因为栈是后入先出，所以把 q1 的元素依次删除并插入 q2，再删除最后一个元素。然后 q1 赋值为 q2，q2 初始化为空，这样才能不断删除。 问题 1：交叉熵公式 解答：交叉熵公式如下： 这里公式定义，x、y 都是表示概率分布。其中 x 是正确的概率分布，而 y 是我们预测出来的概率分布，这个公式算出来的结果，表示 y 与正确答案 x 之间的错误程度（即：y 错得有多离谱），结果值越小，表示 y 越准确，与 x 越接近。 问题 3：对后验概率估计的思考 解答：对于很多条件概率问题，可以等价于求后验概率问题。 问题 4：针对归一化问题的数据线性排序思考 解答：基数排序是一种针对该问题很好的解决方式，往往因为其平均复杂度为被忽略其线性。 问题 5：带有容错的最长公共子串如何实现（动态规划问题） 解答： 暂时还没想到。 问题 6：剑指 Offer 原题，螺旋遍历 解答：主要找规律找出循环条件：并且。 https://github.com/DWCTOD/interview/blob/master/detail/%E4%BD%9C%E4%B8%9A%E5%B8%AE%20%E8%A7%86%E8%A7%89%E7%AE%97%E6%B3%95%E5%B7%A5%E7%A8%8B%E5%B8%88%20%E9%9D%A2%E7%BB%8F%EF%BC%882020%E5%B1%8A%EF%BC%89.md 第一题 leetcode29 题 第二题： 给定 n，用 1 到 n 作为二叉搜索树的节点值，返回 n 个点所能组成的二叉搜索树的个数 如 n=3 大数乘法 https://www.nowcoder.com/discuss/353156289771544576","link":"/2024/06/25/knowledge/leetcode-host/"},{"title":"opencv","text":"https://segmentfault.com/a/1190000044071469 cv2.imread(path, flag) flag： 12[return] type: numpy.ndarray size: (H, W, C), C -&gt; (BGR) ImageToTensor 12345image = cv2.imread(path)image_rgb = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)image = torch.from_numpy(image_rgb).transpose(1, 2).transpose(0, 1)image = image.float().div(255) cv2.imwrite(path, image) TensorToImage 123image = image.mul(255).byte()image = image.transpose(0, 1).transpose(1, 2)image = image.cpu().numpy() 通道处理 12b, g, r = cv2.split(image)image = cv2.merge((b, g, r)) 图像处理与计算机视觉基础 二值化 cv2.threshold() 边缘检测 cv2.Canny() 图像滤波 cv2.filter2D() cv2.blur() cv2.GaussianBlur() 图像形态学操作 cv2.erode() 腐蚀 cv2.dilate() 膨胀 cv2.morphologyEx() 开闭运算、形态学梯度、顶帽、黑帽等","link":"/2024/06/28/knowledge/opencv/"},{"title":"Prompt工程","text":"Prompt Engineering Guide","link":"/2024/07/22/knowledge/prompt/"},{"title":"python","text":"Python修饰器（语法糖） 详解Python修饰器（语法糖） 123@修饰函数def 待修饰函数(): pass 在调用待修饰函数时，直接调用修饰函数 在实现修饰函数时，需要注意待修饰函数的参数和返回值","link":"/2024/07/01/knowledge/python/"},{"title":"RAG技术","text":"深度解析RAG技术在大模型时代的原理与实践 概念 Alt text 分类 Alt text 朴素 RAG 步骤 索引: 直接将输入数据向量化 检索: 对向量数据库进行匹配 生成: 最后将输入数据与向量数据库匹配结果共同放入大模型中生成最终结果 高级 RAG 步骤 增加了对数据的预筛选，围绕预检索和后检索提出了多种优化策略 模块化 RAG 步骤 引入多个特定的功能模块和替换现有模块上。 整个过程不仅限于顺序检索和生成，还包括迭代和自适应检索等方法 RAG for Code Code Generation REDCODER 摘要和代码双向生成 APICoder COCOGEN","link":"/2024/07/09/knowledge/rag/"},{"title":"semantic-segmentation","text":"语义分割模型：从 FCN 到 DeepLab V3+的全面解析 2015 FCN（全卷积网络） 2015 年提出，首次将卷积神经网络应用于像素级分类，开创了端到端语义分割的先河。FCN 将最后几层全连接层替换为卷积层，允许任意大小的输入图像，并通过上采样（反卷积）恢复分割图的分辨率。 U-Net 特别适用于医学图像分割，其特征是编码器-解码器结构，解码器层与编码器层之间有跳跃连接，用以恢复细节信息。 2016 SegNet 基于 VGG 网络，改进了 FCN 的上采样部分，使用了编码器-解码器结构，其中解码器的上采样层使用了编码器的池化索引来恢复细节。 DeepLab V1 DeepLab 系列模型是语义分割领域的另一个重要里程碑。DeepLab V1 首次引入了 Atrous Convolution 模块，弥补了删除池化模块后感受野大小的影响。 2017 PSPNet（金字塔场景解析网络） 引入了金字塔池化模块（Pyramid Pooling Module），捕获不同尺度的信息，增强了模型对不同大小物体的分割能力。 DeepLab V2 DeepLab V2 在 ResNet 的基础上引入了 ASPP（Atrous Spatial Pyramid Pooling）模块，进一步扩大了感受野并提高了分割精度。 MobileNet V1 深度可分离卷积的应用 2018 DeepLab V3 而 DeepLab V3 通过增加 ASPP 模块的宽度，进一步提升了模型的性能。DeepLab V3+是 DeepLab 系列的最新版本，它增加了网络深度，将 Xception 网络作为主干，使用深度可分离卷积 Depthwise Separable Convolution， 结合了解码器模块，以恢复细节信息，进一步提升了多尺度处理能力。 ENet 专为实时应用设计，使用了高效的编码-解码结构和跳过连接。 MobileNet V2 Inverted Residuals 和 Linear Bottleneck ICNet 多尺度级联输入，深度监督 BiSeNet 结合了两个分支，Spatial 分支和 Context 分支。 2019 HRNet 维持高分辨率流以捕获更多细节，同时进行多尺度融合。 DANet Spatial Attention 和 Channel Attention Fast-SCNN：Fast Semantic Segmentation Network 共享下采样权重双分支网络 OCRNet（Object Contextual Representations） 引入了对象上下文表示，使用注意力机制来建模像素间的关系，增强对局部和全局上下文的理解。 CCNet 提出了交叉注意机制（Criss-Cross Attention），允许模型以较低的计算成本建模长距离依赖 MobileNet V3 SE 模块和 Swish 2021 SegFormer SETR 图像块编码，输入纯 Transformer 网络提取特征，reshape 特征后卷积上采样 MaskFormer 将分割统一为 Mask 分类任务 2023 Segment Anything Model (SAM) 一种通用的分割模型，能够处理多种类型的分割任务，包括语义分割、实例分割和全景分割。","link":"/2024/07/01/knowledge/semantic-segmentation/"},{"title":"Pytorch Source Code","text":"init 处理逻辑 判断当前运行环境，加载必须库文件 Define basFic utilities 定义基本工具 typename; is_tensor; ... Define numeric constants 定义数值常量 e; inf; nan; pi Define Storage and Tensor classes 定义 Storage 和 Tensor 类 ctypes 库 一个可以在 python 中调用由 C、C++编写并导出的 dll 动态链接库的包 ctypes.CDLL('vcruntime140.dll') 加载使用 C、C++编写的vcruntime140.dll文件 .pyi 文件 python 中的类型提示文件，也被叫做存根文件 stub file 用于提供代码的静态类型信息，也可以用来表示公共的接口 .pyi 文件给出变量或函数的静态类型，实现了 python 和 C、C++的绑定 参考 [1] Pytorch 底层源码解读（一）概览","link":"/2024/06/20/knowledge/torch/"},{"title":"ANCL","text":"Auxiliary Network Continual Learning (ANCL) Paper: Achieving a Better Stability-Plasticity Trade-off via Auxiliary Networks in Continual Learning Authors: Sanghwan Kim ; Lorenzo Noci ; Antonio Orvieto ; Thomas Hofmann Code: https://github.com/kim-sanghwan/ANCL Framework: Continual Learning (CL) 持续学习 符号定义 PT：Previous Task CT：Current Task 含义 保留 PT 信息的同时，继续在 CT 中进行学习 难点：Catastrophic Forgetting 灾难性的遗忘 对于梯度更新学习的模型，在学习 CT 的过程中更倾向于覆盖 PT 学习的梯度 换而言之，Stability-Plasticity Dilemma Martial Mermillod, Aur ́ elia Bugaiska, and Patrick Bonin. The stability-plasticity dilemma: Investigating the continuum from catastrophic forgetting to age-limited learning effects, 2013. 1 Stability: 在 PT 具有较好的泛化能力 Plasticity: 在 CT 学习新概念 所以，如何平衡 Stability 和 Plasticity是研究的重点 任务分类 类别增量学习(Class-Incremental Learning)的前世今生、开源工具包 Task Incremental Learning (TIL)：训练和测试阶段均为模型提供当前任务标识 Domain Incremental Learning (DIL)：测试阶段不提供当前任务标识 Class Incremental Learning (CIL)：测试阶段自动识别当前任务标识和分类 学习难度逐渐增加，ANCL 在 TIL 和 CIL 设置中进行了评估 相关工作 增加 Auxiliary Network 或 Extra Module Active Forgetting with synaptic Expansion-Convergence (AFEC) 超参控制新旧参数的融合 当前工作 框架化使用 Auxiliary Network 的 CL，使得 Auxiliary Network 插件化 通过和调整正则化项 局限 不同方法依赖于不同的超参 参考 [1] 类别增量学习(Class-Incremental Learning)的前世今生、开源工具包","link":"/2024/06/20/paper/ANCL/"},{"title":"Bi-VAEGAN","text":"Paper: Bi-directional Distribution Alignment for Transductive Zero-Shot Learning Authors: Zhicai Wang, Yanbin Hao, Tingting Mu, Ouxiang Li, Shuo Wang, Xiangnan He Code: GitHub Framework: Zero-shot Learning (ZSL) Hugo Larochelle, Dumitru Erhan, and Yoshua Bengio. Zerodata learning of new tasks. In AAAI, volume 1, page 3, 2008. 1 目标 解决训练时缺少例子或标签的问题 Conventional ZSL / Inductive ZSL 核心挑战 在存在Class Relevance的条件下，使得分类器能从 Seen Classes 提取信息迁移到 Unseen Classes 当中 Class Relevance 通常作为 Auxiliary Data 提供 Auxiliary Data 可以为人工标注、文字描述、知识图谱或 Formal Description of Knowledge（如嵌入向量） Domain Shift Problem 仅从 Auxiliary Data 学习容易导致 Unseen Classes 的真实分布与其建模分布之间存在差异 Proposed: Transductive ZSL (TZSL) 允许在训练中额外加入为目标类别收集的无标签示例 Generative Models 作用 Synthesize Examples 合成样本 Learn the Unseen Data Distribution 学习 unseen 数据分布 分类 Unconditional Generation Conditional Generation Auxiliary 信息是信息量更丰富的类标签，通过 Auxiliary 信息作为 Condition，可以学习到 Data-Auxiliary 联合分布，这连接了 Visual 空间和 Auxiliary 空间，使得生成器具有信息迁移的能力 难点 将 seen classes 所学迁移到 unseen classes f-VAEGAN 提出方法 Transductive Regressor Normalization Class Prior Estimation (CPE) 架构 VAE 编码器，得到维隐藏表征向量 条件生成器，以类别属性为条件，从正态分布采样维向量用于视觉特征生成 Wasserstein GAN（WGAN）的判别器，用于 seen classes WGAN 的判别器，用于 unseen classes 映射视觉空间到特征空间的 Regressor WGAN 的判别器，用于特征判别 Workflow Alt text Level-1 和对抗性训练 Level-2 和、对抗性训练","link":"/2024/06/21/paper/Bi-VAEGAN/"},{"title":"DEADiff","text":"Paper: DEADiff: An Efficient Stylization Diffusion Model with Disentangled Representations Authors: Tianhao Qi, Shancheng Fang, Yanze Wu, Hongtao Xie, Jiawei Liu, Lang Chen, Qian He, Yongdong Zhang Code &amp; Dataset: GitHub 研究背景 基于扩散模型的文本-图像生成模型（T2I）的发展，一些工作尝试引入参考图像作为生成模型的状态，风格图像就是其中一种 利用T2I的已有工作 基于本文转换的方法，将风格图像编码为文本嵌入空间的编码，这种图像到文本的模态转换容易导致信息的丢失 针对风格微调参数的方法容易导致过拟合，且在现实生产中不具有实用性 通过图像编码器提取风格图像特征 T2IAdapter-Style 和 IP-Adapter 使用 Transformer 作为图像编码器，以 CLIP 图像嵌入作为输入，并通过 U-Net 交叉注意层利用提取的图像特征 BLIP-Diffusion 通过 Q-Former 将图像嵌入转化为文本嵌入空间，作为扩散模型文本编码器的输入 研究方法 参考资料 Querying Transformer（Q-Former） 由 Image Transformer 和 Text Transformer 组成，共享 Self-Attention 层参数 Image Transformer 提取与本文内容最相近的视觉特征 输入：图像特征和可学习 Queries 由于共享 Self-Attention 层参数，Queries 可同时与图像特征和文本特征进行交互 Text Transformer 作为输入文本的编码器和解码器 提取风格特征和内容特征 解耦风格特征与内容特征提取 DEADiff STRE（Style Representation Extraction） 使用风格相同的图像作为扩散模型的风格图像和输出目标 CLIP 提取的风格图像特征作为Q-Former输入的图像特征，文本“Style”提取特征作为Q-Former的文本特征，内部做交叉注意力，输出与文本相关的图像特征作为风格特征 SERE（Content Representation Extraction） 使用主体相同但风格不同的图像作为扩散模型的风格图像和输出目标 CLIP 提取的风格图像特征作为Q-Former输入的图像特征，文本“Content”提取特征作为Q-Former的文本特征，内部做交叉注意力，输出与文本相关的图像特征作为内容特征 Disentangled Conditioning Mechanism（DCM）分离条件机制 在使用Diffusion模型去噪的过程中，提取的风格特征和语义特征将作为交叉注意力层的状态输入，从而引导模型更有效地分离风格特征和语义特征 模型使用Stable Diffusion v1.5作为文本-图像生成模型，将16个交叉注意力层编号为0-15，其中，4-8层为Coarse层，其余为Fine层 Disentangled Conditioning Mechanism 输入 风格信息将作为高分辨率Fine层的状态输入，使得提取的风格特征更注重笔画、纹理和颜色等细节信息 语义信息将作为低分辨率Coarse层的状态输入 网络结构 Text-image Crossattention Layer 1）计算图像特征的Key和Value 2）固定参数计算文本特征的Key和Value 3）计算Query 4）分别拼接图像和文本的Key以及图像和文本的Value 5）计算交叉注意力 构建成对数据 准备主体词列表和风格词列表，组合得到相同主体或相同风格的提示词对，利用Text-to-images模型生成图像 构建文本提示词 1）主体词：人物、动物、物体和场景四种类别，12000 2）风格词：艺术风格、艺术家风格、笔触等，650 3）1个主体词对应约14个风格词构成提示词组合，160000 Midjourney生成图像 1个提示词生成4张分辨率为的图像，上采样到后，构建文本-图像对，1060000 成对图像选择 1）风格特征学习：随机选择相同提示词生成的图像构成图像对 2）内容特征学习：随机选择主体词相同但风格不同的提示词对应的图像对","link":"/2024/08/30/paper/DEADiff/"},{"title":"Code of Pixel-to-Prototype Constrast","text":"Generate CAMs Feature map Class feature map Score of class CAMs Pixel-to-Prototype Contrast Pseudo mask Pixel-wise projected feature Pixel-to-prototype contrast Prototype set Temperature Contrast 像素特征与原型的相似度 Prototype Estimation in Batch Top K pixels of class c CAM as confidences Estimate prototypes from pixel-wise feature embeddings that are with the top K confidences Prototype Loss Cross Prototype Contrast Cross CAM Contrast Intra-view Contrast Strategy to slove the matter of in accurate pseudo label [50] Semi-hard prototype mining Hard pixel sampling Code 归一化 归一化 作用 保证所有元素之和为 1 将向量转换为概率分布 归一化 1234# 按通道执行L2归一化v = v / (torch.norm(v, dim=1, keepdim=True) + 1e-5)# orv = torch.nn.functional.normalize(v, dim=1) 作用 方向不变性：向量的方向不变，长度变为 1，使得向量表示不再依赖于其大小 数值稳定性：将向量的大小规范在一个相对较小的区间 减小特征尺度的差异 便于执行相似性度量 Max 归一化 归一化后向量的最大值为 1 Max-Min 归一化 归一化后向量值范围为[0, 1] Forward cam 1234# fea是最后一层输出的特征图self.fc8 = nn.Conv2d(4096, 21, 1, bias=False)cam = self.fc8(fea)cam = torch.nn.functional.interpolate(cam, (H, W), mode='bilinear', align_corners=True) cam_rv_down 清洗 CAM 12345678910with torch.no_grad(): cam_d = torch.nn.functional.relu(cam.detach()) # max norm cam_d_max = torch.max(cam_d.view(n, c, -1), dim=-1)[0].view(n, c, 1, 1)+1e-5 cam_d_norm = torch.nn.functional.relu(cam_d - 1e-5) / cam_d_max # 计算保留概率值最大分类，反相为背景概率，其余分类置0 cam_d_norm[:, 0, :, :] = 1 - torch.max(cam_d_norm[:, 1:, :, :], dim=1)[0] cam_max = torch.max(cam_d_norm[:,1:,:,:], dim=1, keepdim=True)[0] cam_d_norm[:,1:,:,:][cam_d_norm[:,1:,:,:] &lt; cam_max] = 0 增强 CAM 1234567891011121314151617181920# 根据像素相似度调整CAMcam_rv_down = self.PCM(cam_d_norm, f)# PCMdef PCM(self, cam, f): n,c,h,w = f.size() cam = torch.nn.functional.interpolate(cam, (h,w), mode='bilinear', align_corners=True).view(n,-1,h*w) # 多尺度特征融合 f = self.f9(f) f = f.view(n, -1, h*w) # 特征按通道L2归一化 f = f / (torch.norm(f, dim=1, keepdim=True) + 1e-5) # 计算像素相似度矩阵 aff = torch.nn.functional.relu(torch.matmul(f.transpose(1, 2), f), inplace=True) # 相似度矩阵L1归一化 aff = aff/(torch.sum(aff, dim=1, keepdim=True) + 1e-5) # CAM加权 cam_rv = torch.matmul(cam, aff).view(n, -1, h, w) return cam_rv cam_rv 1cam_rv = torch.nn.functional.interpolate(cam_rv_down, (H,W), mode='bilinear', align_corners=True) f_proj 12self.fc_proj = torch.nn.Conv2d(4096, 128, 1, bias=False)f_proj = torch.nn.functional.relu(self.fc_proj(fea), inplace=True) prototype 12345678910111213141516171819202122232425262728293031323334353637f_proj1 = torch.nn.functional.interpolate(f_proj1, size=(128 // 8, 128 // 8), mode='bilinear', align_corners=True)cam_rv1_down = torch.nn.functional.interpolate(cam_rv1_down, size=(128 // 8, 128 // 8), mode='bilinear', align_corners=True)cam_rv2_down = cam_rv2_downwith torch.no_grad(): fea1 = f_proj1.detach() c_fea1 = fea1.shape[1] cam_rv1_down = torch.nn.functional.relu(cam_rv1_down.detach()) # CAM Max-min归一化 n1, c1, h1, w1 = cam_rv1_down.shape max1 = torch.max(cam_rv1_down.view(n1, c1, -1), dim=-1)[0].view(n1, c1, 1, 1) min1 = torch.min(cam_rv1_down.view(n1, c1, -1), dim=-1)[0].view(n1, c1, 1, 1) cam_rv1_down[cam_rv1_down &lt; min1 + 1e-5] = 0. norm_cam1 = (cam_rv1_down - min1 - 1e-5) / (max1 - min1 + 1e-5) cam_rv1_down = norm_cam1 # 设置背景阈值 cam_rv1_down[:, 0, :, :] = args.bg_threshold # 根据图像级标签保留相应的类别 scores1 = torch.nn.functional.softmax(cam_rv1_down * label, dim=1) # 计算伪标签 pseudo_label1 = scores1.argmax(dim=1, keepdim=True) n_sc1, c_sc1, h_sc1, w_sc1 = scores1.shape scores1 = scores1.transpose(0, 1) fea1 = fea1.permute(0, 2, 3, 1).reshape(-1, c_fea1) # 获取各个分类CAM值最高的值与索引 top_values, top_indices = torch.topk(cam_rv1_down.transpose(0, 1).reshape(c_sc1, -1), k=h_sc1 * w_sc1 // 8, dim=-1) prototypes1 = torch.zeros(c_sc1, c_fea1).cuda() # [21, 128] # 遍历各个分类 for i in range(c_sc1): # 获取k个像素对应的特征 top_fea = fea1[top_indices[i]] # CAM值加权平均k个特征得到分类原型 prototypes1[i] = torch.sum(top_values[i].unsqueeze(-1) * top_fea, dim=0) / torch.sum(top_values[i]) # 各个原型L2归一化 prototypes1 = torch.nn.functional.normalize(prototypes1, dim=-1) prototype similarity 12345678910111213141516171819202122232425n_f, c_f, h_f, w_f = f_proj1.shape# [N, H, W, C] -&gt; [N x H x W, C]f_proj1 = f_proj1.permute(0, 2, 3, 1).reshape(n_f * h_f * w_f, c_f)# 特征L2归一化f_proj1 = torch.nn.functional.normalize(f_proj1, dim=-1)pseudo_label1 = pseudo_label1.reshape(-1)positives1 = prototypes2[pseudo_label1]negitives1 = prototypes2# for targetn_f, c_f, h_f, w_f = f_proj2.shapef_proj2 = f_proj2.permute(0, 2, 3, 1).reshape(n_f * h_f * w_f, c_f)f_proj2 = torch.nn.functional.normalize(f_proj2, dim=-1)pseudo_label2 = pseudo_label2.reshape(-1)positives2 = prototypes1[pseudo_label2]negitives2 = prototypes1A1 = torch.exp(torch.sum(f_proj1 * positives1, dim=-1) / 0.1)A2 = torch.sum(torch.exp(torch.matmul(f_proj1, negitives1.transpose(0, 1)) / 0.1), dim=-1)loss_nce1 = torch.mean(-1 * torch.log(A1 / A2))A3 = torch.exp(torch.sum(f_proj2 * positives2, dim=-1) / 0.1)A4 = torch.sum(torch.exp(torch.matmul(f_proj2, negitives2.transpose(0, 1)) / 0.1), dim=-1)loss_nce2 = torch.mean(-1 * torch.log(A3 / A4))loss_cross_nce = 0.1 * (loss_nce1 + loss_nce2) / 2","link":"/2023/11/14/paper/Code-of-Pixel-to-Prototype-Constrast/"},{"title":"BiFormer","text":"Paper: BiFormer: Vision Transformer with Bi-Level Routing Attention Authors: Lei Zhu, Xinjiang Wang, Zhanghan Ke, Wayne Zhang, Rynson Lau Code: GitHub Framework: Transformer 优势 long-range dependency inductive-bias-free high parallelism 劣势 计算量大 内存占用大 现有方案：引入稀疏性 局部窗口 轴向注意力 空洞注意力 存在问题 筛选 key/value 时没有区分 query Bi-level Routing Attention (BRA) Sparsity 利用稀疏性来节省计算量和内存，同时只包含 GPU 友好的稠密矩阵乘法 Query-aware 为各个 Query 筛选语义最相关的 Key-Value 对 伪代码 1234567891011121314151617181920212223242526272829# input: features (H, W, C). Assume H==W.# output: features (H, W, C).# S: square root of number of regions.# k: number of regions to attend.# patchify input (H, W, C) -&gt; (Sˆ2, HW/Sˆ2, C)x = patchify(input, patch_size=H//S)# linear projection of query, key, valuequery, key, value = linear_qkv(x).chunk(3, dim=-1)# regional query and key (Sˆ2, C)query_r, key_r = query.mean(dim=1), key.mean(dim=1)# adjacency matrix for regional graph (Sˆ2, Sˆ2)A_r = mm(query_r, key_r.transpose(-1, -2))# compute index matrix of routed regions (Sˆ2, K)I_r = topk(A_r, k).index# gather key-value pairskey_g = gather(key, I_r)# (Sˆ2, kHW/Sˆ2, C)value_g = gather(value, I_r)# (Sˆ2, kHW/Sˆ2, C)# token-to-token attentionA = bmm(query, key_g.transpose(-2, -1))A = softmax(A, dim=-1)output = bmm(A, value_g) + dwconv(value)# recover to (H, W, C) shapeoutput = unpatchify(output, patch_size=H//S)","link":"/2024/06/26/paper/biformer/"},{"title":"Hunyuan-DiT","text":"Paper: Hunyuan-DiT : A Powerful Multi-Resolution Diffusion Transformer with Fine-Grained Chinese Understanding Authors: Zhimin Li, Jianwei Zhang, Qin Lin, Jiangfeng Xiong, Yanxin Long, Xinchi Deng, Yingfang Zhang, Xingchao Liu, Minbin Huang, Zedong Xiao, Dayou Chen, Jiajun He, Jiahao Li, Wenyue Li, Chen Zhang, Rongwei Quan, Jianxiang Lu, Jiabin Huang, Xiaoyan Yuan, Xiaoxiao Zheng, Yixuan Li, Jihong Zhang, Chao Zhang, Meng Chen, Jie Liu, Zheng Fang, Weiyan Wang, Jinbao Xue, Yangyu Tao, Jianchen Zhu, Kai Liu, Sihuan Lin, Yifu Sun, Yun Li, Dongdong Wang, Mingtao Chen, Zhichao Hu, Xiao Xiao, Yan Chen, Yuhong Liu, Wei Liu, Di Wang, Yong Yang, Jie Jiang, Qinglin Lu Code &amp; Pretrained Model: GitHub 背景 现有一些基于 Diffusion 的文生图模型，如 DALL-E、SD 和 Pixart 缺乏对中文提示词的理解，而 AltDiffusion、PAI-Diffusion 和 Taiyi 这一类具有中文理解能力的模型则仍有进步空间 基于 DiT 的模块改进 Hunyuan-DiT 图像编码器 使用预训练 VAE 提取图像编码用于学习数据分布，SDXL 中的 VAE 相比于 SD1.5 中的 VAE 有较大的提升 文本编码器 使用预训练中英双语 CLIP 模型以及多语种 T5 模型提取文本编码 混元 DiT 按照的大小分块 为了提升模型在细粒度文本条件表现，在特征提取模块使用交叉注意力层融合文本特征 Transformer 块包含编码块和解码块，块中包含了自注意力-交叉注意力-FFN 在解码块增加了与编码块之间的跳层链接 训练时使用 v-prediction 的方式具有更好的表现 v-predition 相关资料 在使用 v-prediction 方法时，模型不直接预测噪声 ε，而是预测了一个加权后的量 v，这个量结合了噪声 ε 和原始数据 x 的信息，能在采样步骤较少的情况下提供有效的信号来指导采样过程 位置编码和多分辨率图像生成 两种位置编码可视化 使用二维 RoPE 对绝对位置和相对位置进行编码 为了实现多分辨率图像生成，尝试了两种类型的编码 Extended Positional Encoding，随着宽高的不同，编码结果也会有巨大的差异 Centralized Interpolative Positional Encoding，定义边界为编码结果变化的范围，且以图像中心为编码 0 点 提升训练稳定性 使用 QK-Norm，在计算 Q、K 和 V 之前增加归一化层 在跳层模块后增加归一化层，从而避免梯度爆炸 使用 FP32 进行训练避免溢出 数据流 数据收集与筛选 图像重标注 多轮对话增强提示","link":"/2024/09/03/paper/hunyuan-dit/"},{"title":"Img-Diff","text":"Paper: Img-Diff: Contrastive Data Synthesis for Multimodal Large Language Models Authors: Qirui Jiao, Daoyuan Chen, Yilun Huang, Yaliang Li, Ying Shen Code &amp; Dataset: GitHub Application: 提出的组件和端到端构建工作流程在 Data-Juicer 中作为数据处理算子和可配置文件来实现 专业名词 Multimodal Large Language Models (MLLMs) Visual Question Answering (VQA) Image Difference Captioning (IDC) 专注于图像之间的细微差别而不仅仅是物体描述 背景 提升多模态大语言模型的表现通常有两种路径，一种是提升模型架构，另一种则是提升数据质量 多数多模态大语言模型进行两阶段训练，第一阶段训练实现图像-文字数据对的模态对齐，第二阶段则注重通过 Instruction Tuning 数据集微调提升模型的问答能力 通过提升图像-文字数据对的数量可以提升模型的语义对齐能力，但有可能影响模型的问答能力，因此更多工作集中于研究 Instruction Tuning 数据集的增强 提出方法与 InstructPix2Pix 类似，采用 Prompt-toPrompt 技术以及生成模型 Stable-Diffusion-XL 来生成相似图像对，在生成阶段结合了多个筛选阶段来确保数据质量，强调模型关注特定区域而不是整个图像的差异 数据合成方法 利用对比学习的原理来生成 MLLM 图像-文本数据。该方法侧重于替换图像对中的对象，引导 MLLM 识别特定区域的相似性和差异 Step1. 创建相似图像并形成图像对，这些图像对之间的唯一区别是图中的对象 Step2. 提出差异区域生成器（Difference Area Generator），提取包含图像对之间的对象差异的边界框 Step3. 提出差异描述生成器，通过 MLLM 为具有对象差异的区域生成描述性文本，并创建问答对，例如“该区域中哪些对象发生了变化？” 过程中应用的大模型 Vicuna-1.5-13B 魔塔 由 LMSYS 研发的，基于 Llama 2 微调的 Transformer 架构自回归语言模型 Stable-Diffusion-XL Hugging Face 由文字生成图像的潜在空间扩散模型：1）UNet 的大小是原来的 3 倍，引入文本编码器编码器（OpenCLIP ViT bigG/14）与原始文本编码器相结合；2）引入大小和裁剪条件，以防止训练数据被丢弃，并更好地控制生成的图像应如何裁剪；3）引入两阶段模型过程，基础模型（也可以作为独立模型运行）生成图像作为优化模型的输入，该模型添加了额外的高质量细节 CLIP Github CLIP (Contrastive Language-Image Pre-Training) ，一个在图像-文字数据对上预训练的模型，可以通过给定图片判断最相关的文字片段 FastSAM Github SAM 作者提出的具有更快推理速度的分割模型 BLIP BLIP（Bootstrapping Language-Image Pre-training），一个统一视觉语言理解与生成的预训练模型，可以通过图像生成图像标注 LLaVA-NEXT Github 多模态大模型 全流程梳理 流程图 模块一：创建相似数据对 Image Pairs Generation 从 MS COCO 获取 118K 图像描述 使用 Vicuna-1.5-13B 替换图像描述中的对象 输入 Prompt：“这里有一个句子：‘INPUT’。请仅将这句话中的一个宾语替换为另一个宾语。” INPUT 为原始描述，LLM 生成替换后的描 使用生成的描述文本对，利用图像生成模型 Stable-Diffusion-XL 和图像编辑技术 Prompt-to-Prompt 生成仅替换少量对象的图像对 模块二：差异区域生成器（Difference Area Generator） 该模型用于识别图像对之间对象差异的位置 由于预训练的目标检测模型有检测类别的限制，所以生成器基于分割方法和图像相似度比较识别差异位置，如下图所示 Difference Area Generator 通过 Image Similarity Filter 获得相似度高但不完全相同的图像对 使用 FastSAM 分割每张图像 根据分割获得的边界框信息裁剪图像，并使用 Image-text Matching Filter 判断裁剪后的子图像是否存在有效对象 使用 Difference Detector 来确定图像对的边界框区域之间是否确实存在差异，并进行 IoU 过滤以去除重叠的边界框，最终获得有效的边界框信息 Filter1：Image Similarity Filter 根据图像相似度过滤图像对 计算流程 该模块首先使用 CLIP 提取图像特征，然后计算特征间的余弦相似度 若余弦相似度在预设阈值内，则图像对将被视为有效 应用阶段 在使用 FastSAM 进行分割之前，使用该模块来确保图像对高度相似但不完全相同 在 Difference Detector 阶段，根据边界框信息裁剪子图像后，使用该模块过滤子图像对并仅保留不同的子图像对 Filter2：Image-text Matching Filter 确定图像是否包含有效对象（即已替换或正在替换的对象） 计算流程 该模块首先使用 BLIP 提取图像特征，然后将其与对象名称的文本特征进行比较 当图文匹配分数落在预设阈值内时，则认为图像包含有效对象 应用阶段 根据分割信息进行子图像裁剪后，使用该模块来确定子图像是否包含有效对象并获得相应的有效边界框 Filter3：Difference Detector 确定图像对的边界框区域之间是否存在差异 计算流程 根据边界框从图像 A 和 B 中裁剪两个子图像 通过 Image Similarity Filter 过滤子图像对，仅当差异足够显著时才认为边界框有效 处理完所有边界框后，使用 IoU 方法过滤重叠的边界框，仅保留差异程度较高的边界框，最终输出所有有效边界框 模块三：差异描述生成器（Difference Captions Generator） 获得有效的边界框区域后，使用 Difference Captions Generator 生成有关这些区域内容的差异描述 图像对可能包含多个差异，而单个描述无法完全捕获所有差异，因此每个描述仅关注一个图像对中的一个边界框 该模块分为两个阶段，如下图所示 Difference Captions Generator 第一阶段，模型为边界框区域生成内容描述，然后使用 Image-text Matching Filter 和 Captions Similarity Filter 筛选带内容描述的边界框 第二阶段，模型使用内容描述和用红框标注的图像生成差异描述 阶段一：对象标注和筛选 对于每个图像对，首先选择图像之间相似度最低的 N 个边界框区域（本项目中 N 设置为 5）作为候选区域 对于每个边界框，我们使用 MLLM LLaVA-NEXT 来描述其相应的区域 第一个筛选过程，通过 Image-text Matching Filter 检查区域与描述是否对应 第二个筛选过程，通过 Captions Similarity Filter 评估描述之间是否存在差异，使用 CLIP 来获取文本特征并计算它们之间的余弦相似度，当分数足够低时，可认为两个标题不同 筛选完成后，获得的有效的边界框和描述将用于后续的差异描述生成 阶段二：差异描述生成 对于每个图像对的各个有效边界框生成差异描述 根据边界框信息在图像中绘制两个红色框，突出显示差异以便于定位 为 MLLM LLaVA-NEXT 提供边界框区域的描述，并引导模型根据内容描述和红色框生成差异描述","link":"/2024/08/15/paper/img-diff/"},{"title":"AMD GPU MI210 深度学习疑难杂症","text":"ROCm 官方文档 ROCm Pytorch PyTorch for ROCm 官方文档 方法一：使用 docker 镜像 rocm/pytorch 方法二：Pytorch 官方轮子 APEX Pytorch 扩展，用于混合精度与分布式训练的工具 Github ModuleNotFoundError: fused_layer_norm_cuda 通过源码安装 APEX 解决，注意 Pytorch 版本对应的源码分支 GitHub 拉取相应分支源码 cd apex 输入命令安装 apex，耗费时间可能较长 12345# if pip &gt;= 23.1 (ref: https://pip.pypa.io/en/stable/news/#v23-1) which supports multiple `--config-settings` with the same key...pip install -v --no-build-isolation --config-settings \"--build-option=--cpp_ext\" --config-settings \"--build-option=--cuda_ext\" ./# otherwisepython setup.py install --cpp_ext --cuda_ext","link":"/2024/09/11/project/rocm/"},{"title":"machine-learning","text":"K-Nearest Neighbors（KNN） KNN 算法是一种基本的监督学习方法，用于分类和回归问题。在分类问题中，KNN 算法通过测量不同特征值之间的距离，找出输入实例与训练数据集中最接近的 k 个样本，然后根据这 k 个邻居的类别来预测新实例的类别。对于回归问题，KNN 则是预测新实例的连续值输出。 基本步骤 选择 K 值 K 值是算法中的一个超参数，表示考虑最近的邻居数量。选择 K 值时，较小的 K 值会使决策边界更加复杂，可能过拟合；较大的 K 值会简化决策边界，可能欠拟合。 计算距离 对于每一个训练样本，计算其与待分类样本之间的距离。常见的距离度量有欧氏距离、曼哈顿距离等。 找到 K 个最近邻 从计算出的距离中选出最小的 k 个，这 k 个样本即为待分类样本的最近邻。 预测类别或值 对于分类任务，对这 k 个样本的类别进行投票，票数最多的类别作为预测结果。 对于回归任务，通常采用这 k 个样本的目标值的平均值或加权平均值作为预测结果。 优点 算法简单直观，易于理解和实现。 不需要假设数据分布，适用于非线性分类问题。 可以处理多类问题。 缺点 计算成本高，特别是在大数据集上，因为需要计算测试样本与所有训练样本之间的距离。 需要选择合适的距离度量和 K 值。 对于不平衡的数据集，少数类别的预测可能会受到影响。 支持向量机 Support Vector Machine（SVM） 是一种非常强大的监督学习模型，主要用于分类和回归分析。在分类问题中，SVM 寻找一个最优的超平面（在高维空间中可能是一条线、一个平面或一个超平面），该超平面能够最大化地将不同类别的数据分开。这个超平面被称为最大间隔超平面，而那些离超平面最近的训练样本点则被称为支持向量。 SVM 的核心思想在于找到一个决策边界，使得正负两类样本被尽可能远地分开，同时确保边界两侧的样本被正确分类。在非线性情况下，SVM 通过使用核函数（Kernel Function）将低维空间中的数据映射到高维空间，从而在高维空间中找到一个线性的决策边界。 多类分类问题解决策略（如 K 类） 一对一（One-vs-One, OvO） 在这种策略中，每一对类别都会进行一次二分类，这意味着总共需要训练 C(K, 2) = K(K-1)/2 个 SVM 模型。最终分类时，每个样本都会被 K(K-1)/2 个模型分类，然后根据多数投票原则决定最终类别。 一对多（One-vs-All, OvA） 也称为 One-vs-Rest。对于 K 类问题，需要训练 K 个 SVM 模型，每个模型区分一类样本和其他所有类别的样本。当分类新的样本时，每个模型都会给出一个决策，样本会被分配给具有最高置信度分值的那类。 多分类 SVM 这是一种直接将多类分类问题建模的方法，它试图一次性找到一个超平面矩阵来区分所有的 K 类。 逐步二分类 DAGSVM Directed Acyclic Graph SVM 策略，这是一个专门设计用于多类分类的 SVM 变体，可以避免传统的投票机制。在逐步二分类（DAGSVM）策略中，构造了一个有向无环图（DAG），其中每个节点代表一个二分类器。DAG 的构建方式是从所有类别中选择两个类别进行比较，胜者（即被分类器认为更有可能的类别）继续与其他类别进行比较，直到确定最终的类别。这种方式确保了每个测试样本只需要通过一系列的二分类器，而不必经历所有可能的二分类器组合，从而避免了简单的多数投票机制。 svm 为什么要转成对偶问题进行求解，为什么对偶问题的解是原问题的解? svm 如何进行多分类，多分类 hinge loss 什么形式? 朴素贝叶斯分类器 朴素贝叶斯分类器是一种基于概率论的分类方法，尤其在文本分类、情感分析、垃圾邮件过滤等领域有着广泛的应用。它基于贝叶斯定理以及特征条件独立的假设，尽管这个假设在现实世界的数据中往往不成立，但朴素贝叶斯分类器在许多情况下仍然表现出良好的性能，尤其是在数据量较大时。 贝叶斯定理 朴素贝叶斯分类器的基础是贝叶斯定理，它描述了已知某些条件下，事件 A 发生的概率 P(A|B)，可以用以下公式表示： 先验概率 (Prior Probability) 先验概率是指在观察到任何数据或证据之前，基于以往知识或经验对某事件发生概率的估计。它是我们的“先验”信念，反映了我们对某个假设在没有任何新信息情况下的初始信心程度。例如，在投掷一枚硬币的情况下，如果我们没有任何关于这枚硬币是否公平的信息，我们可能会设定正反两面出现的概率都是 50%，这就是一种先验概率。 后验概率 (Posterior Probability) 后验概率是指在观察到某些数据或证据之后，对事件发生概率的更新估计。它是根据先验概率以及新的观测数据，通过应用贝叶斯定理计算得出的。后验概率反映了我们对某个假设的新信念，这种信念是在考虑到所有可用信息后形成的。继续用硬币的例子，如果我们连续投掷硬币十次，其中有九次正面朝上，我们可能会调整我们对硬币公平性的信念，认为硬币可能是偏向正面的，此时我们对硬币偏向正面的概率的估计就是后验概率。 特征条件独立假设 朴素贝叶斯分类器假设所有特征在给定类别的情况下是相互独立的。这意味着，对于分类问题中的每个特征，其取值不会受到其它特征取值的影响，即使在实际中这种独立性可能并不存在。这一假设简化了计算，使得分类器的训练变得相对简单，同时也使得朴素贝叶斯分类器能够处理高维数据。 分类过程 朴素贝叶斯分类器的工作流程通常包括以下步骤： 训练阶段：计算每个类别的先验概率以及每个特征在各类别下的条件概率。 预测阶段：对于一个新的输入样本，计算其属于每个类别的后验概率，然后选择具有最大后验概率的类别作为预测结果。 优点 算法简单，易于实现。 训练速度快，所需的计算资源较少。 在特征条件独立假设下，可以处理高维数据。 缺点 条件独立假设在实际应用中往往是不成立的，这可能会影响分类器的准确性。 对于从未在训练数据中出现过的特征值，朴素贝叶斯分类器会给出 0 的概率，这可能导致分类失败。为了避免这种情况，通常会采用平滑技术，如拉普拉斯平滑。 独立成分分析（ICA） 独立成分分析是一种统计和计算技术，用于估计一组随机变量的潜在因素或成分，这些成分被认为是相互独立的。ICA 的目标是找到一组变换，将混合的观测信号分解成独立的源信号。与主成分分析不同，ICA 关注的是信号的统计独立性，而非方差最大化。ICA 在信号分离、盲源分离等领域有广泛应用。 数据降维 奇异值分解（SVD） 异值分解是一种矩阵分解技术，广泛应用于信号处理、数据压缩、模式识别等多个领域。对于任意一个矩阵 A（m×n），SVD 将其分解为三个矩阵的乘积：UΣV^T，其中 U 是 m×m 的正交矩阵，Σ 是 m×n 的对角矩阵，其对角线上的元素是 A 的奇异值（通常是降序排列的非负实数），V 是 n×n 的正交矩阵。SVD 在数据分析中常用于提取矩阵的主要成分，如降维、数据压缩、特征提取等。 主成分分析（PCA） 主成分分析是一种常用的统计方法，用于识别数据中的模式和结构。PCA 通过正交变换将可能相关的变量转换为一组线性不相关的变量，称为主成分。主成分是原变量的线性组合，第一个主成分具有数据的最大方差，第二个主成分具有次大方差，同时与第一个主成分正交，以此类推。PCA 常用于降维、数据可视化和噪声消除。 线性判别分析（Linear Discriminant Analysis, LDA） 是一种常用的数据分析和分类技术，最初由 R.A. Fisher 提出，因此有时也被称作 Fisher 判别分析。LDA 主要用于寻找最佳的低维空间投影，使得不同类别的样本在该空间中的分离度最大化，同时保持同类样本之间的紧凑性。LDA 经常被用于降维和分类任务，特别是在监督学习的场景下。 LDA 的主要目标 LDA 的主要目标是找到一个投影，使得投影后的数据在不同类别间具有最大的可区分性，同时在同一类别内具有最小的差异性。具体而言，LDA 试图最大化类间散度和最小化类内散度，以此来增强分类效果。 LDA 的步骤 计算类内散度矩阵 Sw 和类间散度矩阵 Sb： 类内散度矩阵 Sw 反映了同一类别内部样本之间的变异程度。 类间散度矩阵 Sb 则反映了不同类别之间中心点的变异程度。 求解特征向量和特征值： 通过求解 Sw 和 Sb 的广义特征值问题，找到能最大化类间差异的投影方向。 选择投影方向： 选择前 k 个最大的特征值对应的特征向量作为投影方向，其中 k 是希望得到的降维后的特征数量。 投影数据： 将原始数据投影到找到的投影方向上，得到降维后的数据。 分类： 降维后的数据可以使用各种分类器进行分类，如线性分类器。 LDA 的应用 LDA 在多个领域都有广泛应用，包括但不限于： 人脸识别：LDA 可以用于提取人脸图像的重要特征，从而在低维空间中进行人脸识别。 生物信息学：在基因表达数据的分析中，LDA 可以帮助识别不同疾病状态的基因表达模式。 市场细分：在市场营销中，LDA 可以用来识别消费者群体的不同特征，帮助进行市场细分。 医学诊断：LDA 可用于诊断疾病，通过分析患者数据来区分健康个体和患病个体。 LDA 与主成分分析（PCA） 方法类似，但 PCA 是一种无监督的降维方法，而 LDA 是有监督的，它利用了类标签信息，这使得 LDA 在分类任务中通常能提供更好的性能。然而，LDA 也有其局限性，比如它假设数据服从高斯分布，且不同类别的协方差矩阵相等，这在某些情况下可能不成立。 拉普拉斯特征映射（Laplacian Eigenmaps） 是一种非线性降维技术，由 Belkin 和 Niyogi 在 2003 年提出，主要用于在保持数据局部结构的同时，将高维数据嵌入到低维空间中。Laplacian Eigenmaps 的核心思想是利用数据点之间的相似性来构建一个图，然后在图上找到最优的低维表示，这种表示能够尽可能地保持原数据点间的局部距离不变。 拉普拉斯特征映射的基本步骤： 构建邻接矩阵：首先，为数据集中的每个点 i 找到其最近的 k 个邻居，然后构建邻接矩阵 W，其中 W(i,j) &gt; 0 如果点 j 是点 i 的邻居，否则 W(i,j) = 0。W(i,j)的具体值可以是点 i 和点 j 之间的相似性度量，比如高斯核函数。 构建度矩阵：度矩阵 D 是一个对角矩阵，其中 D(i,i)等于第 i 个点的度，即邻接矩阵 W 中第 i 行元素之和。 构建拉普拉斯矩阵：拉普拉斯矩阵 L 定义为 L = D - W。这是图拉普拉斯算子的一种形式，它捕获了图的连通性和拓扑性质。 求解特征值问题：寻找拉普拉斯矩阵 L 的特征向量和特征值。最小的几个非零特征值对应的特征向量提供了数据点在低维空间中的坐标。 数据点的低维表示：将数据点映射到低维空间中，使用最小的几个非零特征值对应的特征向量作为坐标轴。这些特征向量捕捉了数据集的内在几何结构，使得数据点在低维空间中的相对位置保持了它们在高维空间中的局部关系。 应用场景： 数据可视化：Laplacian Eigenmaps 可以用于将高维数据可视化，通过降维将数据投影到二维或三维空间，以便于人类观察和理解。 聚类分析：通过在低维嵌入空间中应用聚类算法，可以发现数据的潜在结构和簇。 图像处理：在图像分析和识别中，可以利用 Laplacian Eigenmaps 来提取图像的关键特征。 与其它降维技术的对比： 与 PCA（主成分分析）相比，Laplacian Eigenmaps 更关注数据的局部结构，而 PCA 则关注全局的方差最大化。PCA 是一种线性降维技术，而 Laplacian Eigenmaps 能够处理非线性数据集，这对于复杂数据结构特别有用。 总的来说，Laplacian Eigenmaps 是一种强有力的非线性降维工具，特别适用于保持数据的局部连通性和拓扑结构，使其成为分析高维数据集的有效手段。 LLE KPCA ISOMAP 决策树（Decision Trees） 决策树是一种用于分类和回归的机器学习算法。它通过构建一棵树形结构，其中内部节点表示特征上的测试，分支表示测试结果，而叶子节点表示类别或数值。决策树通过递归地分割数据集，每次分割都选择最优的特征和分割点，以最大程度地减少不纯度（如基尼不纯度或熵）。决策树易于理解和解释，可以处理数值和分类数据，但容易过拟合，通常需要剪枝技术来控制复杂度。 决策树对连续值和离散值特征是否会重复利用作为分割特征? ID3 算法的局限性 只处理离散属性：ID3 算法只能处理离散属性，无法直接处理连续属性。 信息增益偏好：ID3 使用信息增益作为分裂标准，这导致它偏好具有更多值的属性，即使这些属性可能并不提供更多的分类信息。 缺失值处理：ID3 算法没有提供处理缺失值的有效机制。 过拟合问题：ID3 算法容易过拟合，因为它总是尽可能深地生长树，直到所有叶子节点都是纯的。 C4.5 的改进 处理连续属性：C4.5 可以处理连续属性，通过寻找最佳的切分点将连续属性转化为二元离散属性。 使用增益率：C4.5 使用增益率（Gain Ratio）代替信息增益作为分裂属性的选择标准。增益率考虑了信息增益与属性的熵的比率，从而避免了信息增益偏好的问题。 处理缺失值：C4.5 提供了一种处理缺失值的方法，允许在不知道完整信息的情况下仍能进行分类。 剪枝策略：C4.5 引入了剪枝技术，以防止过拟合。它使用两种剪枝方法：悲观剪枝（使用一个用户定义的阈值来决定是否剪枝）和错误估计剪枝（基于树的错误率来决定是否剪枝）。 生成规则集：C4.5 不仅生成决策树，还可以从决策树中导出一套规则，这使得模型更容易被理解和解释。 处理多类分类：C4.5 可以很好地处理多类分类问题，而不仅仅是二分类。 CART 算法的特点： 二叉树结构：CART 生成的决策树是二叉树，每个非叶节点都有两个子节点，这使得树的结构更加简单，易于理解和实现。 变量选择：对于分类树，CART 使用基尼不纯度（Gini Impurity）作为特征选择的标准；对于回归树，则使用平方误差（Mean Squared Error）来评估分裂的质量。 可处理连续和离散变量：CART 能够自动处理连续和离散变量，对于连续变量，它会找到最佳的分割点来创建二元分裂。 缺失值处理：CART 提供了处理缺失值的策略，例如使用替代值或者通过预测缺失值的可能性来分配样本到不同的分支。 剪枝策略：CART 采用成本复杂性剪枝（Cost Complexity Pruning）来防止过拟合，这是一种后剪枝方法，通过在树的复杂性和拟合数据的误差之间寻找平衡点来简化决策树。 BP 神经网络（Backpropagation Neural Networks） BP 神经网络是人工神经网络的一种，通过前向传播计算输出，再通过反向传播调整权重，以最小化网络输出与期望输出之间的误差。BP 算法使用梯度下降法更新权重，使网络能够学习复杂的非线性映射。神经网络可以处理大量输入和输出，具有很强的学习能力，但训练时间可能较长，且可能遇到局部最优解的问题。 随机森林（Random Forests） 随机森林是一种集成学习方法，通过构建多个决策树并将它们的预测结果进行投票来改进预测性能。随机森林在构建每棵树时，不仅随机选择样本，还随机选择特征，这样可以减少过拟合风险并提高模型的泛化能力。随机森林能够处理高维数据，对于异常值和缺失值具有较好的鲁棒性。 随机森林与 GBDT，解释、比较异同、哪种方法单棵决策树的深度更大? 判别式模型 vs 生成式模型 判别式模型（Discriminative Models） 直接学习输入特征和输出标签之间的映射关系，目标是最小化预测错误。例如，BP 神经网络、支持向量机都属于判别式模型的范畴，它们直接学习如何将输入映射到输出。 生成式模型（Generative Models） 试图学习数据和标签的联合分布，通常用于估计类条件概率。例如，朴素贝叶斯分类器就是一种生成式模型，它估计了给定类别的条件下特征的概率分布。 NLP TF-IDF（Term Frequency-Inverse Document Frequency） 是一种用于信息检索和文本挖掘的统计方法，它评估一个词对一个文档集合或语料库中的某篇文档的重要程度。TF-IDF 由两部分组成：词频（TF）和逆文档频率（IDF）。词频是指一个词在文档中出现的频率，而逆文档频率则反映了一个词在整个文档集合中的罕见程度。 文本分类中，从含有兼类可分为多标签分类和单标签分类 距离算法 1. 欧几里得距离（Euclidean Distance） 欧几里得距离是最直观的距离度量方式，它计算的是两个点在多维空间中的直线距离。在二维空间中，如果点 A 的坐标是((x_1, y_1))，点 B 的坐标是((x_2, y_2))，那么它们之间的欧几里得距离为： [ d_{AB} = ] 推广到 n 维空间，如果有两个 n 维向量( = (a_1, a_2, ..., a_n))和( = (b_1, b_2, ..., b_n))，它们之间的欧几里得距离为： [ d*{AB} = ] 2. 曼哈顿距离（Manhattan Distance） 曼哈顿距离，也称为城市街区距离或 L1 距离，它衡量的是两点在坐标轴上的绝对距离之和。在 n 维空间中，两个向量()和()之间的曼哈顿距离为： [ d*{AB} = ^{n}|a_i - b_i| ] 曼哈顿距离在网格布局或出租车行驶路径计算中很有用，因为它忽略了两点之间的直线距离，只考虑沿坐标轴方向的移动。 3. Canberra 距离 Canberra 距离是一种用于非负向量的度量，它在计算距离时考虑了分母，使得距离受向量元素值的影响更大。两个非负向量()和()之间的 Canberra 距离定义为： [ d*{AB} = ^{n} ] Canberra 距离在处理稀疏数据时特别有用，因为它可以更好地处理零值。 4. 切比雪夫距离（Chebyshev Distance） 切比雪夫距离，也称为棋盘距离或 L∞ 距离，它衡量的是两个点在任一坐标轴上的最大绝对差值。在 n 维空间中，两个向量()和()之间的切比雪夫距离为： [ d*{AB} = (|a_i - b_i|) ] 切比雪夫距离在某些应用场景中特别有用，比如在国际象棋中计算棋子移动的最短步数。 每种距离度量都有其特点和适用场景，选择合适的距离算法对于数据的分析和建模至关重要。例如，欧几里得距离适用于测量连续空间中的相似性，而曼哈顿距离和切比雪夫距离可能在某些特殊的空间布局或数据结构中更有优势。 梯度算子 梯度算子是数字图像处理和计算机视觉领域中用于边缘检测和特征提取的重要工具。梯度算子能够突出图像中的边缘和纹理，通过计算图像像素强度的局部变化来检测边界。以下是几种常用的梯度算子模板： 1. Sobel 算子 Sobel 算子是使用最广泛的边缘检测算子之一，它通过计算图像在水平和垂直方向上的梯度来检测边缘。Sobel 算子由两个 3x3 的卷积核构成： 水平方向梯度算子： 垂直方向梯度算子： Sobel 算子对图像进行卷积操作后，可以得到水平和垂直方向上的梯度分量，最后通过合并这两个分量来获得边缘强度。 2. Prewitt 算子 Prewitt 算子与 Sobel 算子类似，也使用两个 3x3 的卷积核来检测水平和垂直方向上的梯度，但 Prewitt 算子的权重分布较为均匀： 水平方向梯度算子： 垂直方向梯度算子： 3. Roberts 交叉算子 Roberts 交叉算子使用 2x2 的小型卷积核，可以快速计算图像的梯度： 水平方向梯度算子： 垂直方向梯度算子： Roberts 算子对噪声敏感，但在计算速度上有优势。 4. Laplacian 算子 Laplacian 算子不是直接检测梯度，而是检测二阶导数，用于检测边缘的拐点。一个常用的 Laplacian 算子模板是： 5. LoG 算子 全称为 Laplacian of Gaussian 算子，是一种在图像处理和计算机视觉中用于边缘检测和特征提取的算法。LoG 算子结合了高斯平滑滤波器（Gaussian filter）和拉普拉斯算子（Laplacian operator），其主要目的是在抑制噪声的同时检测图像中的边缘和其他显著特征。 工作原理： 高斯平滑： 首先，LoG 算子使用高斯滤波器对图像进行平滑处理。高斯滤波器通过应用一个高斯分布的权重矩阵（核）来对每个像素及其邻域进行加权平均，从而减少图像中的高频噪声。 拉普拉斯算子： 接着，经过高斯平滑后的图像会被进一步应用拉普拉斯算子。拉普拉斯算子是一个二阶微分算子，它对图像的亮度变化进行二次导数计算，能够突出图像中的突变区域，如边缘。 LoG 滤波器： 将上述两个步骤合在一起，LoG 算子实际上是计算高斯平滑后的图像的拉普拉斯变换。数学上，LoG 算子定义为高斯函数的拉普拉斯变换，表达式为： [ ^2 g(x,y,) = g(x,y,) + g(x,y,) ] 其中 (g(x,y,)) 是二维高斯函数，() 是标准差，决定了高斯核的宽度。 应用： LoG 算子常被用于边缘检测和图像特征点检测，特别是在尺度空间理论中，它可以用来检测图像中的关键点，这些点可能对应于角点、边缘交叉点或其他重要的图像特征。 由于 LoG 算子的输出可能包含大量的零过零点（zero-crossings），这些点被认为是潜在的边缘或特征位置。因此，在实际应用中，通常会对 LoG 响应进行阈值处理，只保留那些超过一定阈值的零过零点作为有效的特征点。 实现： 在实践中，LoG 算子可以通过卷积操作实现，即将图像与预定义的 LoG 核进行卷积。LoG 核是高斯核和拉普拉斯算子的离散近似，其形状取决于选择的()值。 为了简化计算，有时会先应用高斯核，再单独应用拉普拉斯算子，尽管这在数学上不完全等同于 LoG 算子，但可以达到类似的效果，这种方法被称为 DoG（Difference of Gaussians）算子，是 LoG 的一个近似版本，常用于尺度不变特征变换（SIFT）等算法中。 LoG 算子因其在抑制噪声和定位边缘及特征点方面的有效性，在图像处理领域有着广泛的应用。 色彩模型 是用来描述颜色的一种数学表示方法，它们按照不同的用途和目的对颜色进行分类和编码。你提到的色彩模型可以大致分为两大类：面向硬件设备的色彩模型和面向视觉感知的色彩模型。 面向硬件设备的彩色模型 这类模型主要用于设备的颜色表示，它们的设计是为了与特定类型的硬件或色彩再现过程兼容。 RGB 模型（Red, Green, Blue）： RGB 模型是基于加色法原理的色彩模型，通常用于显示设备如电视、电脑显示器和手机屏幕。在 RGB 模型中，红、绿、蓝三种颜色的光以不同的强度混合，可以产生广泛的色彩范围。 CMYK 模型（Cyan, Magenta, Yellow, Key/Black）： CMYK 模型是基于减色法原理的色彩模型，主要用于印刷行业。在印刷过程中，青色、洋红色、黄色和黑色油墨以不同比例混合，可以复制出各种色彩。 YCrCb 模型（Luminance/Chrominance）： YCrCb 模型是一种用于视频系统的色彩模型，其中 Y 代表亮度（Luminance），Cr 和 Cb 分别代表红色和蓝色的色差（Chrominance）。这种模型被用于数字电视和图像压缩中，如 JPEG 2000 和 H.26x 系列视频编码标准。 面向视觉感知的彩色模型 这类模型的设计更贴近人类视觉系统对颜色的感知方式，通常用于色彩管理和色彩选择界面。 HSI 模型（Hue, Saturation, Intensity）： HSI 模型将颜色分为色调（Hue）、饱和度（Saturation）和强度（Intensity）三个组成部分。色调描述了颜色的基本类型，饱和度表示颜色的纯度，强度则是颜色的亮度。 HSV 模型（Hue, Saturation, Value）： HSV 模型与 HSI 模型类似，但将强度（Intensity）替换为价值（Value）。价值表示颜色的明暗程度，而色调和饱和度的定义与 HSI 模型相同。 HSB 模型（Hue, Saturation, Brightness）： HSB 模型实际上与 HSV 模型是相同的，只是将“Value”称为“Brightness”。在某些软件中，如 Adobe Photoshop，使用 HSB 来表示 HSV 模型。","link":"/2024/06/26/knowledge/machine-learning/"},{"title":"面试经验","text":"语义分割模型研究 1. 介绍一下经典的语义分割模型 FCN (全卷积网络) FCN 是最早的端到端语义分割模型之一，它通过去除全连接层并替换为卷积层来实现任意输入尺寸的图像处理。FCN 利用跳跃连接来结合不同层次的特征图以恢复细节信息。 SegNet SegNet 基于 VGG16 架构，其主要特点是使用了编码器-解码器结构，并且在解码阶段采用了在编码阶段存储的索引来上采样，从而减少了参数量。 UNet UNet 在医学图像分割中特别受欢迎，因为它能够有效地处理小数据集。它包含一个收缩路径（下采样）和一个对称的扩展路径（上采样），并在每个解码步骤中从编码器获取相应的特征图。 DeepLabV3+ DeepLab 系列是谷歌提出的一组模型，它们使用了空洞卷积（又称扩张卷积）来捕捉多尺度信息，并引入了 ASPP 模块来进一步增强模型的多尺度感受野。DeepLabV3+增加了解码模块来恢复分割细节。 HRNet HRNet 专注于在整个网络中保持高分辨率表示，通过并行的流来处理不同分辨率的特征图，并通过交换模块来融合这些特征。这有助于保留更多的细节信息，防止信息损失。 通道、空间注意力机制 注意力机制（如 SENet 中的通道注意力或 CBAM 中的空间注意力）允许模型专注于重要的特征图区域，同时抑制不相关的背景噪声，提高模型的准确性。 ViT (Vision Transformer) ViT 是一个完全基于 Transformer 架构的视觉模型，它将图像切分为固定大小的补丁，并将这些补丁作为序列传递给标准的 Transformer 编码器。这种设计使得 ViT 可以捕捉全局上下文信息。 SwinFormer SwinFormer 是专门为视觉任务设计的 Transformer 变体，它提出了窗口注意力机制来实现局部和非局部的特征交互，同时保持计算效率。 SegFormer SegFormer 是一个轻量级的分割模型，它结合了 Transformer 编码器（如 MiT）和简单的解码头来实现高效的多尺度特征提取和分割。 SAM (Segment Anything Model) SAM 是一种通用的分割模型，它可以用于任何对象的分割任务而无需特定类别的训练数据。它结合了强大的预训练视觉模型和灵活的提示机制来适应不同的分割需求。 Q: FCN 如何解决不同大小输入的问题？ A: FCN 通过移除最后的全连接层，代之以卷积层，允许模型接受任意大小的输入图像，并产生相同大小的分割图。 Q: UNet 如何应对数据不足的情况？ A: UNet 的设计包括了一个可以学习到更抽象特征的收缩路径，以及一个可以恢复位置信息的扩展路径，这使得它能够在较少的数据上训练而不会过拟合。 Q: DeepLabV3+中的 ASPP 模块是什么？ A: ASPP（Atrous Spatial Pyramid Pooling）模块通过不同 rate 的空洞卷积来捕捉不同尺度的信息，从而增强模型对物体不同大小的适应能力。 Q: ViT 是如何处理图像输入的？ A: ViT 首先将输入图像划分为固定的补丁，然后将这些补丁展平成一系列向量，并添加位置嵌入，之后这些向量将被送入 Transformer 编码器进行处理。 2. 隧道病害检测领域常用模型 在隧道病害检测领域，尤其是针对细小裂缝的检测，需要模型具备良好的细节捕捉能力和多尺度感知能力。以下是一些常用的模型及其特点： UNet 特点: UNet 因其在医学图像分割中的成功应用而知名，同样适用于隧道病害检测。通过跳层连接（skip connections），UNet 能够结合高层语义信息与底层细节信息，这对于细小裂缝的检测非常重要。 应用: 在隧道检测中，UNet 可以帮助识别裂缝等病害，特别是在数据尺度较小的情况下表现良好。 DeepLabV3+ 特点: 使用空洞卷积（Atrous Convolution）来扩大感受野而不增加参数数量，并引入 ASPP 模块来捕获多尺度信息。DeepLabV3+还增加了额外的解码模块来恢复分割细节。 应用: 对于隧道病害检测而言，DeepLabV3+的多尺度感知能力非常适合识别不同宽度和长度的裂缝。 SENet (Squeeze-and-Excitation Networks) 特点: 通过引入通道注意力机制来动态地调整通道权重，从而让模型更加关注于那些对于分类任务更重要的特征。 应用: 在隧道病害检测中，SENet 可以通过加强裂缝区域的特征表达，来提高裂缝识别的连续性。 HRNet (High-Resolution Net) 特点: HRNet 在整个网络中都保持高分辨率的特征图，通过并行的多分支结构来融合不同尺度的信息，避免了在恢复分辨率过程中细节信息的丢失。 应用: 这种特性对于隧道病害检测尤其有用，因为裂缝往往非常细小，需要保持尽可能高的分辨率来确保准确识别。 SegFormer 特点: SegFormer 结合了 Transformer 编码器（如 MiT）和简单的解码头来实现高效的多尺度特征提取。相比于传统的 CNN，Transformer 能够更好地捕捉全局上下文信息。 应用: 在隧道病害检测中，SegFormer 可以通过其强大的全局感知能力来识别那些跨越较大区域的裂缝或其他病害。 Q: 在隧道病害检测中，为什么 UNet 的跳层连接很重要？ A: 跳层连接允许 UNet 在解码过程中重新引入编码阶段丢失的细节信息。这对于细小裂缝的检测至关重要，因为裂缝通常很窄，容易在下采样的过程中丢失。 Q: DeepLabV3+的 ASPP 模块是如何工作的？ A: ASPP 模块使用不同膨胀率的空洞卷积来捕捉不同尺度的信息，这样即使在不改变输出尺寸的情况下，也可以有效地扩大模型的感受野。 Q: SENet 的通道注意力机制如何帮助改进病害检测？ A: 通道注意力机制允许 SENet 根据每个通道的重要性动态地调整权重，这意味着模型可以更加聚焦于那些最能代表病害特征的通道，从而提高检测精度。 Q: 在隧道病害检测中，HRNet 如何保证高分辨率特征图？ A: HRNet 通过保持多个并行的高分辨率流，并在每一层之间交换信息，从而在整个网络中维持高分辨率特征图。这种方法有助于保留更多细节信息，这对于检测细小裂缝非常重要。 Q: SegFormer 如何在病害检测中发挥作用？ A: SegFormer 利用 Transformer 的强大能力来捕捉全局依赖关系，并通过多尺度特征提取来增强对不同大小病害的检测性能。这对于识别隧道中各种类型的病害非常有效。 3. 如何处理小目标和大目标同时存在于一张图片中的情况？ 在语义分割任务中，经常会遇到图像中小目标和大目标共存的情况。为了同时有效地检测和分割这些不同尺度的目标，可以采用以下几种策略和技术： 多尺度注意力金字塔 (Multi-Scale Attention Pyramid) 解释: 多尺度注意力金字塔通过在不同尺度上应用注意力机制来捕获不同大小的目标。这通常涉及到构建一个多尺度特征金字塔，每个尺度上的特征图都会经过注意力机制处理，以突出不同尺度的目标。 例子: 比如 DeepLab 系列中的 ASPP（Atrous Spatial Pyramid Pooling）模块就是一种典型的多尺度处理方式，它通过不同膨胀率的卷积来捕捉多尺度信息。 混合 CNN 和 Transformer (Hybrid CNN and Transformer Models) 解释: 混合 CNN 和 Transformer 模型结合了传统卷积神经网络在局部特征提取方面的优势和 Transformer 在捕获长距离依赖关系方面的强大能力。这种组合可以在不同尺度上同时处理局部和全局信息。 例子: SegFormer 就是一个实例，它使用了 Transformer 编码器（如 MiT）来提取多尺度特征，并结合了简单的解码头来进行最终的分割预测。 Q: 为什么多尺度注意力金字塔对于处理小目标和大目标是有效的？ A: 多尺度注意力金字塔通过在多个尺度上进行特征提取，使得模型可以在不同级别的细节上关注目标，从而更好地捕捉到小目标的细节和大目标的整体形状。 Q: SegFormer 如何结合 CNN 和 Transformer 的优点？ A: SegFormer 使用了基于 Transformer 的编码器来捕捉全局上下文信息，并通过多尺度特征提取来增强对不同大小目标的理解。同时，它的解码头设计简单高效，可以很好地将多尺度特征融合在一起，以实现高质量的分割结果。 Q: 在实际应用中，如何选择合适的多尺度处理方法？ A: 选择多尺度处理方法时，应考虑应用场景的具体需求。如果场景中存在大量的小目标，则应选择那些能够较好地保留细节信息的方法；而对于包含大范围目标的场景，则可能需要更强的全局感知能力。 Q: 除了多尺度处理，还有哪些技术可以用来改善小目标和大目标的分割效果？ A: 另外一些技术还包括使用金字塔结构（如 FPN）、增强数据（如数据扩增）、改进损失函数（如 Focal Loss）等方法，这些都可以辅助提升模型在不同尺度目标上的表现。 4. 你在实际项目中使用过哪些语义分割模型？遇到了什么挑战？怎么解决？ 在实际项目中，我使用过多种语义分割模型来处理隧道病害检测的任务，包括但不限于 UNet、DeepLabV3+、HRNet 和 SegFormer。在具体的应用过程中，确实遇到了一些挑战，并采取了相应的解决方案。 裂缝和瓷砖剥落病害尺度不平衡 挑战: 不同尺度的病害（如细小裂缝和大面积的瓷砖剥落）在图像中占比不同，这可能导致模型对某一类病害的检测能力较弱。 解决方案: 使用加权交叉熵损失函数，或 Focal Loss，来平衡不同类别之间的损失贡献，从而提高小目标的检测能力。 结合 Transformer 和 CNN 的优势，利用 Transformer 捕捉全局信息，同时使用 CNN 来提取局部特征，以增强模型对不同尺度病害的感知能力。 病害图像背景复杂 挑战: 背景复杂度高会干扰模型对病害关键特征的提取，导致误检或漏检。 解决方案: 使用原型特征提取方法，该方法能够学习到特定类别的原型特征，有助于在复杂背景下提取出病害的关键特征。 利用 VQGAN 的预训练 CodeBook 来提取更为鲁棒的特征表示，增强模型的抗干扰能力。 标注不准确 挑战: 数据标注过程中可能会出现误差，导致模型训练偏差或泛化能力下降。 解决方案: 采用软标签的学习方式，即在训练过程中使用带有不确定性的标签，而非硬性分配标签，这样可以减弱损失函数的强约束，使模型更具弹性。 使用边缘优化方法，如 PointRend，来改进分割边界，或者采用边缘再分割技术，通过形态学操作（如膨胀、腐蚀）来选取合适的像素点，提高分割的准确性。 Q: Focal Loss 是如何解决类别不平衡问题的？ A: Focal Loss 通过降低易分类样本的权重，从而更加关注难以分类的样本。这样可以减轻类别不平衡带来的影响，尤其是在小目标检测中表现得尤为明显。 Q: Transformer 和 CNN 的结合如何提高模型性能？ A: Transformer 擅长捕捉全局依赖关系，而 CNN 则在局部特征提取方面表现出色。将两者结合起来可以互补各自的优势，使得模型既能捕捉到局部细节又能理解整体上下文。 Q: 原型特征提取方法是如何工作的？ A: ProtoPNet 通过学习一组原型特征，并在测试时计算输入图像与这些原型特征之间的相似度，从而决定分类结果。原型特征为一类像素特征的均值，且随着训练不断变化。这种方法可以增强模型对特定类别的理解，并提高其在复杂背景下的鲁棒性。 Q: 使用 VQGAN 的预训练 CodeBook 有什么好处？ A: 使用 VQGAN 的预训练 CodeBook 可以帮助模型学习到更加紧凑和有意义的离散特征表示，这对于处理复杂背景下的病害检测任务非常有帮助，可以减少背景噪声的影响。 Q: PointRend 是如何优化分割边界的？ A: PointRend 通过在分割边界处选择不确定性较高的像素点重预测，来细化分割结果。这种方法可以在保持较高效率的同时，显著提高分割边界的准确性。 Q: 6. 除了 Self-Attention，还有哪些机制可以增强模型的表达能力？ A: （层归一化（Layer Normalization）可以帮助模型更好地学习长距离依赖；残差连接（Residual Connections）有助于梯度流动；使用 Transformer 中的多头注意力机制） 损失函数 1. 介绍一下常用的损失函数 交叉熵损失（Cross Entropy Loss） 解释: 交叉熵损失是最常见的分类损失函数之一，它衡量的是模型预测的概率分布与真实标签的概率分布之间的差异。 代码示例: 1234567891011121314151617181920212223import torchimport torch.nn as nnimport torch.nn.functional as Fclass CrossEntropyLoss(nn.Module): def __init__(self, weight=None, reduction='mean'): super(CrossEntropyLoss, self).__init__() self.weight = weight self.reduction = reduction def forward(self, pred, label): # pred (B, C, H, W) # label (B, H, W) if len(label.shape) &lt; len(pred.shape): # 如果标签不是one-hot形式 label = F.one_hot(label, num_classes=pred.shape[1]) pred = F.softmax(pred, dim=1) loss = -label * torch.log(pred) if self.weight is not None: loss = loss * self.weight.view(1, - 1, 1, 1) if self.reduction == 'mean': return loss.mean() elif self.reduction == 'sum': return loss.sum() Focal Loss 解释: Focal Loss 旨在解决类别不平衡的问题，通过在交叉熵的基础上增加一个调节因子来降低容易分类样本的权重，增加难分类样本的权重。 代码示例: 123456789101112131415161718192021class FocalLoss(nn.Module): def __init__(self, weight=None, reduction='mean', gamma=2): super(FocalLoss, self).__init__() self.weight = weight self.reduction = reduction self.gamma = gamma def forward(self, pred, label): # pred (B, C, H, W) # label (B, H, W) if len(label.shape) &lt; len(pred.shape): label = F.one_hot(label, num_classes=pred.shape[1]) pred = F.softmax(pred, dim=1) pt = torch.where(label &gt; 0, pred, 1 - pred) loss = -torch.pow(1 - pt, self.gamma) * label * torch.log(pt) if self.weight is not None: loss = loss * self.weight.view(1, -1, 1, 1) if self.reduction == 'mean': return loss.mean() elif self.reduction == 'sum': return loss.sum() Dice 损失 解释: Dice 损失通常用于分割任务中，特别是当需要关注分割边界时。它定义为两个集合交集的两倍除以它们的并集。 公式: ( DICE = 1 - )，其中( X )是预测结果，( Y )是真实标签。 代码示例: 123456789101112class DiceLoss(nn.Module): def __init__(self): super(DiceLoss, self).__init__() def forward(self, pred, label): smooth = 1. pred = F.softmax(pred, dim=1) if len(label.shape) &lt; len(pred.shape): label = F.one_hot(label, num_classes=pred.shape[1]) intersection = (pred * label).sum(dim=(2, 3)) dice_score = (2. * intersection + smooth) / (pred.sum(dim=(2, 3)) + label.sum(dim=(2, 3)) + smooth) return 1 - dice_score.mean() Q: 为什么交叉熵函数在不平衡数据表现不佳？ A: 交叉熵函数在不平衡数据集中表现不佳的原因在于，它会过度强调多数类别的样本，而忽略少数类别的样本。在不平衡数据集中，少数类别的样本往往包含更有价值的信息，但交叉熵损失函数并没有专门对待这些样本，导致模型倾向于偏向多数类别。 Q: Focal Loss 是如何解决类别不平衡的问题的？ A: Focal Loss 通过引入一个调节因子( (1 - p_t)^{} )，其中( p_t )是模型对正确类别的预测概率，( )是调节因子的指数。这个调节因子降低了容易分类样本的贡献，同时增加了难分类样本的贡献，从而使得模型更加关注那些难以分类的样本。 Q: Class Weight 是如何帮助平衡数据的？ A: Class Weight 是通过为不同类别赋予不同的权重来实现的，这样可以调整损失函数中各类别对总损失的贡献比例。在不平衡数据集中，可以为少数类别赋予更高的权重，从而增加它们在训练过程中的重要性。 Q: 如何平衡正负样本的比例来改善模型性能？ A: 平衡正负样本比例可以通过多种方法实现，例如过采样少数类别、欠采样多数类别、使用合成样本（如 SMOTE）、调整 Class Weight 等。这些方法可以单独使用或组合使用，以达到更好的模型训练效果。此外，还可以在训练过程中动态调整样本权重，以进一步优化模型性能。 数据集构建 1. 数据处理的过程 数据收集: 收集来自不同来源的原始数据。 预处理: 包括数据格式转换、缩放、裁剪等操作。 清洗: 移除或修正错误、重复或无关的数据。 标注: 对数据进行标记，以便机器学习模型可以学习到正确的模式。 划分: 将数据分成训练集、验证集和测试集，用于模型训练、调参和评估。 2. 数据收集 采集车采集隧道衬砌图像: 使用专门的采集设备在隧道内拍摄衬砌图像，确保图像质量和涵盖不同类型的衬砌材料。 3. 数据标注 LabelMe: 使用 LabelMe 这样的工具进行手动标注，确保标注的准确性和一致性。 4. 数据预处理 局部匀光算法: 对图像进行光照均匀化处理，消除光照不均带来的影响。 数据分块: 将大图像分割成若干个小块，便于处理和标注。 5. 数据清洗 去除重复数据: 通过哈希或特征匹配等方式识别并删除重复项。 修复损坏文件: 检查并修复损坏的图像文件。 清理噪声: 去除不需要的对象或背景干扰。 6. 数据划分 随机划分: 使用随机种子将数据集分成训练、验证和测试集。 按类别划分: 确保每个子集中都有各个类别的代表性样本。 7. 如何处理数据集中存在的标注错误？ 复查标注: 定期复查已有的标注，确保标注的一致性和准确性。 使用一致性检查: 应用一致性检查工具来发现和纠正标注错误。 引入专家审查: 让领域专家复审标注结果，尤其是对于复杂的案例。 8. 在数据集构建过程中，如何保证数据的多样性和代表性？ 涵盖不同场景: 确保数据覆盖隧道的不同部分，如墙壁、天花板、地面等。 不同条件: 包括白天、夜晚、晴天、雨天等多种天气和光照条件。 不同材质: 包括瓷砖、混凝土、金属等多种衬砌材料。 不同病害类型: 包括裂缝、剥落、渗水等多种病害形式。 9. 有哪些工具或技术可以用来自动化数据标注过程？ Paddle: 使用 Paddle 框架提供的工具和服务进行自动化的数据标注。 LabelStudio: LabelStudio 是一个开源的标注工具，支持多种数据类型和标注任务，可以用于简化数据标注流程。 Q: 局部匀光算法是如何工作的？ A: 局部匀光算法通常通过调整图像的局部对比度或亮度来均匀化光照条件。计算局部块的平均灰度，上采样和高斯模糊得到亮度图，计算原图与亮度图的差异还原亮度分布。还可以使用直方图均衡化、Retinex 算法或自适应直方图均衡化（CLAHE）等方法来改善光照不均的问题。 Q: 数据分块的好处是什么？ A: 数据分块可以使处理和标注工作更加高效。通过将图像分割成小块，可以更容易地进行并行处理，同时也有助于标注人员专注于图像的特定区域。 Q: 数据清洗中的“清理噪声”具体指的是什么？ A: Q: 如何确保数据集划分的合理性？ A: 确保数据集划分合理的方法之一是使用分层抽样，这样可以确保每个子集中都有各个类别的代表性样本。此外，可以使用交叉验证来评估模型在不同子集上的表现。 Q: 自动化数据标注技术的局限性是什么？ A: 自动化数据标注虽然提高了效率，但也可能存在误标的情况，尤其是在数据复杂或变化多端的情况下。因此，通常还需要人工复查和修正标注结果。 训练方法研究 1. MMCV 解释: MMCV 是一个基于 PyTorch 的开放源码计算机视觉工具箱，它提供了丰富的模型实现、数据处理和训练框架。 用途: MMCV 可以帮助快速搭建和训练模型，同时也提供了很多实用功能，如数据增强、模型融合等。 2. 训练框架 Config: 配置文件用于定义整个训练过程的参数，包括数据路径、模型结构、损失函数、优化器等。 Data: 数据加载器负责读取、预处理和批处理数据。 Model: 模型定义，包括网络结构和前向传播逻辑。 Loss: 损失函数用于量化模型预测与真实标签之间的差距。 Trainer: 训练循环逻辑，包括前向传播、反向传播、优化更新等。 3. PytorchLightning 解释: PytorchLightning 是一个用于简化 PyTorch 模型开发的库，提供了一套简洁的 API 来管理训练流程。 优点: 减少了样板代码，方便配置和调试，支持多种训练策略如 GPU 分布式训练、混合精度训练等。 4. 学习率的选择 解释: 学习率决定了模型参数更新的速度。过高会导致训练不稳定甚至发散，过低则会使训练速度过慢。 策略: 可以通过学习率查找（Learning Rate Finder）来找到合适的初始学习率。 5. 优化器的选择 解释: 优化器决定了梯度下降的方式，常用的有 SGD、Adam、RMSprop 等。 选择: 根据任务特性和模型结构选择合适的优化器，例如 Adam 适合稀疏数据，SGD 适合大规模数据集。 6. 显存问题 解释: 训练深度学习模型时，显存不足是一个常见问题。 解决: 可以通过梯度累积、模型剪枝、分批加载数据等方式缓解显存压力。 7. 如何有效地使用迁移学习来加速模型训练？ 使用预训练模型作为初始权重: 从预训练模型加载权重，可以加速收敛并提高性能。 冻结基础层，仅训练顶层: 在训练初期可以冻结预训练层，只训练新添加的层。 适应性调整预训练模型的输出层以匹配新的任务: 修改输出层以适应新的任务需求。 8. 如何设置学习率衰减策略来避免过拟合？ 使用学习率调度器如 StepLR、Cosine Annealing 等: 通过周期性地降低学习率来促进收敛。 观察验证集上的性能变化来动态调整学习率: 当验证集性能不再提升时，可以适当降低学习率。 使用早停法（Early Stopping）: 在验证集性能不再改善时提前终止训练。 9. 在训练过程中，如何监控和诊断模型的训练状态？ 使用 TensorBoard 或其他可视化工具来跟踪损失和指标: 监控训练过程中的损失曲线和指标变化。 定期保存检查点并进行评估: 保存中间训练结果，便于后续评估或恢复训练。 记录训练过程中的关键参数和配置: 保持训练配置的一致性和可重复性。 10. DDP (Distributed Data Parallel) 解释: DDP 是 PyTorch 中的一个分布式训练工具，允许多个 GPU 并行训练同一个模型。 使用: 可以通过设置 torch.nn.parallel.DistributedDataParallel 来启用 DDP 训练。 11. 混合精度 解释: 混合精度训练是指在模型训练过程中同时使用单精度（32 位）和半精度（16 位）浮点运算。 优势: 可以减少内存占用，加快训练速度，同时保持较高的精度。 Q: MMDetection 有哪些主要组件？ A: MMDetection 主要包括数据读取、模型定义、训练流程管理、评估工具等多个组件，支持多种模型和任务的快速开发。 Q: 如何选择合适的优化器？ A: 选择优化器时需要考虑任务的特性和数据集的特点。例如，对于稀疏数据，Adam 通常表现较好；而对于大规模数据集，SGD 由于其更快的收敛速度可能是更好的选择。 Q: 如何解决显存不足的问题？ A: 可以通过使用梯度累积、模型剪枝、数据分批加载等方法来缓解显存不足的问题。此外，还可以考虑使用更大的显卡或者分布式训练方案。 Q: 学习率查找是如何工作的？ A: 学习率查找通过逐渐增加学习率并观察损失的变化趋势来确定一个好的初始学习率。通常会在训练初期执行一次，以找到损失开始显著下降的学习率值。 Q: 如何使用早停法来避免过拟合？ A: 在训练过程中，当验证集的性能不再提高时，可以停止训练。这通常通过监测验证集上的损失或准确率来实现，当这些指标在一个预定的周期内没有改善时，就触发早停。 低光增强及图像复原模型研究 1. RefinxNet 解释: RefinxNet 基于 Refinx 原理分离亮度和细节。 特点: RefinxNet 通过多级特征融合，增强了对细节的捕捉能力，并且使用残差学习来减少训练难度。 2. Zero-DCE 解释: Zero-DCE 是一种无参考的低光图像增强方法，它不需要任何参考图像即可完成增强任务。 特点: Zero-DCE 通过学习图像的亮度和色彩分布来调整图像，使得在没有任何参考图像的情况下也能获得较好的增强效果。 3. DDPM 解释: DDPM（Diffusion Denoising Probabilistic Models）是一种基于扩散过程的图像生成模型，也可以应用于图像复原任务。 特点: DDPM 通过逐步去除噪声来恢复图像，适用于多种图像复原任务。 4. ControlNet 解释: ControlNet 是一种可以控制生成过程的模型，可以在图像复原任务中引入额外的控制信号。 特点: ControlNet 可以根据额外的输入（如边缘图、分割图等）来引导图像复原过程，提高复原的准确性和可控性。 5. LocalLightEnhance 解释: LocalLightEnhance 是一种局部光照增强方法，通过局部调整图像的光照来提高图像质量。 特点: LocalLightEnhance 更加注重局部细节的恢复，适用于需要精细调整光照的场景。 6. Position Embedding 解释: 位置嵌入（Position Embedding）是在 Transformer 模型中引入的一种机制，用于捕捉序列中元素的位置信息。 公式: ( pos{(2i)} = (x / i{}) ), ( pos{(2i+1)} = (x / i{}) ) 类型: 包括相对位置编码和空间位置编码，前者用于捕捉序列元素之间的相对位置，后者用于标记序列中元素的空间位置。 7. HunyuanDiT 解释: HunyuanDiT 是一种用于图像复原的扩散模型，结合了多尺度特征提取和注意力机制。 特点: HunyuanDiT 通过多尺度特征提取来捕捉不同层次的细节，并通过注意力机制来加强重要特征的表达。 8. 如何评估低光图像增强算法的效果？ 定量指标: 使用 LOE（Low Light Enhancement）、PSNR（峰值信噪比）、SSIM（结构相似性指数）等指标来评估图像质量。 主观反馈: 通过用户研究或专家评审来获取关于图像视觉效果的主观反馈。 视觉效果: 比较增强前后图像的视觉效果，评估算法在实际应用中的表现。 8. 对于极端低光条件下的图像，有哪些有效的增强方法？ 增强曝光度: 通过增加曝光度来提高图像的整体亮度。 局部调整: 使用局部光照增强方法来恢复图像的细节。 多帧融合: 通过融合多张连续拍摄的低光图像来减少噪声并提高图像质量。 预处理: 在增强前进行预处理，如去除噪声或增强对比度。 9. 如何利用先验知识来改进图像复原模型？ 物理模型: 引入物理模型来模拟成像过程，帮助模型更好地理解图像退化的原因。 场景特定知识: 根据特定场景的特点，如光照方向、纹理特征等，对模型进行定制化调整。 上下文信息: 利用上下文信息（如图像中的语义信息）来指导图像复原过程。 多模态信息: 结合其他模态的数据（如深度信息、语义分割图等）来增强图像复原的效果。 Q: RefinxNet 中的多尺度特征融合是如何实现的？ A: Q: Zero-DCE 的优势是什么？ A: Zero-DCE 的最大优势在于它不需要参考图像即可完成增强任务，这使得它在实际应用中更加便捷。 Q: DDPM 在图像复原中的作用是什么？ A: DDPM 通过逐步去除图像中的噪声来恢复图像的原始信息，适用于多种复原任务。 Q: ControlNet 在图像复原中的应用有哪些？ A: ControlNet 可以通过引入额外的控制信号来引导图像复原过程，从而实现更精确的复原效果。 Q: 如何选择合适的定量指标来评估图像增强的效果？ A: 选择指标时需要考虑具体的评估目标，如 LOE 侧重于低光环境下的增强效果，而 PSNR 和 SSIM 则更多关注图像的整体质量。 Q: 在极端低光条件下，多帧融合为什么有效？ A: 多帧融合可以利用多张图像中的信息来补偿单张图像中的信息不足，从而提高图像质量。 Q: 如何利用场景特定知识来改进图像复原模型？ A: 可以根据场景的特点，如光照条件、纹理特征等，对模型进行定制化调整，使其更适合特定场景的应用。 图像处理算法 低通滤波 解释: 低通滤波器用于平滑图像，去除高频噪声，保留低频成分。 应用: 低通滤波常用于图像去噪、模糊处理等场景。 例子: 常见的低通滤波器有均值滤波器、高斯滤波器等。 边缘提取算法 解释: 边缘提取算法用于检测图像中的边缘，即像素强度的急剧变化。 应用: 边缘检测广泛应用于图像分割、特征识别等领域。 例子: 常见的边缘提取算子有 Sobel 算子、Canny 边缘检测、Prewitt 算子等。 在不同的应用场景中，如何选择适合的滤波器？ 根据需求选择滤波器类型: 例如，去噪可以选择高斯滤波，边缘检测可以选择 Sobel 算子。 考虑滤波器的频率响应特性: 不同滤波器对不同频率成分的处理效果不同，需根据实际需求选择合适的滤波器。 如何结合多种边缘提取算法来获得更好的效果？ 组合使用: 可以将多种边缘提取算法的结果进行组合，例如先用 Canny 边缘检测提取粗略边缘，然后再用 Sobel 算子细化边缘。 多尺度处理: 在不同尺度上应用不同的边缘检测算法，然后综合结果，以提高边缘检测的鲁棒性。 融合算法: 利用融合算法（如投票机制）来综合多种边缘检测算法的结果，以获得更准确的边缘。 除了传统的边缘检测算法，还有哪些现代方法？ 深度学习方法: 使用卷积神经网络（CNN）进行边缘检测，例如基于 U-Net、SegNet 等网络架构的边缘检测模型。 超像素分割: 利用超像素分割技术，将图像分割成多个超像素区域，然后在超像素级别进行边缘检测。 图割方法: 利用图论中的图割方法来检测图像中的边缘。 主动轮廓模型（Snake 模型）: 通过能量最小化的方法来寻找图像中的边缘。 Q: 高斯滤波器和平均滤波器有什么区别？ A: 高斯滤波器通过高斯函数加权平均邻域内的像素值，可以有效地保留图像的主要特征，同时去除噪声。而平均滤波器则是简单地取邻域内像素的平均值，可能会导致图像模糊。 Q: Sobel 算子的工作原理是什么？ A: Sobel 算子通过两个 3x3 的卷积核分别对图像进行水平和垂直方向的卷积，从而计算图像在两个方向上的梯度，进而检测出边缘。 Q: 在图像处理中，如何选择合适的滤波器大小？ A: 滤波器的大小取决于需要处理的特征的尺度。较大的滤波器可以处理更大范围的特征，但可能会丢失细节；较小的滤波器则更适合处理局部特征。 Q: 如何评估边缘检测算法的效果？ A: 可以通过定量指标（如边缘检测的准确率、召回率等）和定性评价（如视觉效果）来评估边缘检测算法的效果。此外，还可以使用人工标注的边缘作为基准来比较算法的性能。 Q: 深度学习在边缘检测中的优势是什么？ A: 深度学习方法可以自动学习特征表示，具有很强的表达能力和适应性，可以处理复杂的边缘检测任务，并且在大数据集上有很好的表现。 分割边缘优化 PointRend Multimul DataSyn SegRefiner 如何评价分割边缘的质量？ 在分割任务中，如何平衡速度和精度？ 有没有尝试过将其他领域的技术应用于分割边缘优化？ Transformer 模型 Transformer 模型是如何处理长依赖关系的？ 如何将 Transformer 应用于非自然语言处理的任务？ Transformer 模型的局限性是什么？ （计算成本高，尤其是在长序列上；训练数据需求量大；难以捕捉局部特征） Diffusion 模型 1. Diffusion 模型的训练过程 正向扩散过程: 在这一过程中，原始数据被逐渐添加高斯噪声，直到变成纯噪声为止。这一过程可以看作是一个数据退化的过程，目的是构造一个从清晰数据到噪声的连续分布演变。 逆向扩散过程: 该过程尝试从噪声中逐步去除噪声，逐步重建原始数据。此过程涉及训练一个模型，使其能够预测在某一步骤中应该去除多少噪声。 2. Diffusion 模型与 GAN 模型的区别 显式地学习噪声分布: Diffusion 模型通过显式地学习噪声分布来生成数据，而 GAN 则是通过生成器和判别器之间的对抗训练来间接学习数据分布。 3. 如何在 Diffusion 模型中加入条件信息？ 使用条件向量作为输入的一部分: 条件信息可以直接作为模型输入的一部分，帮助指导生成过程。 在生成过程中加入外部信息: 例如，在文本到图像生成中，可以将文本描述嵌入到模型中，如 StableDiffusion 通过交叉注意力机制融合特征。 ControlNet: 在某些情况下，可以通过增加一个可训练的分支并将条件信息通过跳层连接引入，类似于 ControlNet 的做法。 4. 在图像生成任务中，Diffusion 模型相比传统方法有什么优势？ 学习分布时任务目标更简单: Diffusion 模型通过逐步添加和去除噪声来学习数据分布，这比直接学习复杂的高维数据分布要简单得多。 速度较慢: 尽管任务目标简单，但由于需要多次迭代去除噪声，因此生成过程相对缓慢。 5. 如何评估 Diffusion 模型的生成质量？ 结果容易模糊: Diffusion 模型由于其生成过程的本质，可能会导致生成的图像模糊不清。评估时可以使用诸如 Fréchet Inception Distance (FID) 或 Inception Score (IS) 等指标来量化生成图像的质量。 定性评估: 除了量化指标外，还可以通过人工视觉评估来检查生成图像的真实感和多样性。 6. Diffusion 推理加速 并行化: 利用 GPU 的并行计算能力来加速每一步的去噪过程。 采样步数减少: DDIM 通过减少去噪步骤的数量来加速推理，但这可能会影响生成图像的质量。 模型剪枝: 通过对模型进行剪枝，减少不必要的计算资源消耗。 量化: 通过量化技术减少模型大小和计算复杂度，从而加快推理速度。 Q: Diffusion 模型中的正向扩散过程如何实现？ A: 正向扩散过程通常通过一系列高斯噪声的添加来实现，每次添加一定强度的噪声，直到原始数据完全被噪声掩盖。这一过程可以视为数据分布的退化。 Q: Diffusion 模型中的逆向扩散过程是如何去噪的？ A: 逆向扩散过程通过训练一个去噪模型来逐步去除噪声，每次迭代都试图预测并移除一定量的噪声，直至恢复出原始数据。 Q: 控制 Diffusion 模型生成结果的方法有哪些？ A: 可以通过条件输入（如类别标签、文本描述等）来控制生成结果，或者通过调整模型参数（如去噪步数、噪声强度等）来微调生成效果。 Q: Diffusion 模型在生成图像时为什么会模糊？ A: 由于 Diffusion 模型需要多次迭代去除噪声，这个过程中累积的小误差可能会导致生成的图像变得模糊不清。 Q: 如何解决 Diffusion 模型生成结果模糊的问题？ A: 可以尝试改进去噪算法，比如引入注意力机制来增强模型的局部感知能力，或者采用后处理技术（如锐化滤波）来增强生成图像的清晰度。 多模态模型 SAM CLIP BLIP HunyuanDiT 大语言模型 RAG 工程 Prompt 优化","link":"/2024/09/26/interview/experience/"}],"tags":[{"name":"leetcode","slug":"leetcode","link":"/tags/leetcode/"},{"name":"二分法","slug":"二分法","link":"/tags/%E4%BA%8C%E5%88%86%E6%B3%95/"},{"name":"排序辅助","slug":"排序辅助","link":"/tags/%E6%8E%92%E5%BA%8F%E8%BE%85%E5%8A%A9/"},{"name":"algorithm","slug":"algorithm","link":"/tags/algorithm/"},{"name":"tree","slug":"tree","link":"/tags/tree/"},{"name":"企业","slug":"企业","link":"/tags/%E4%BC%81%E4%B8%9A/"},{"name":"动态规划","slug":"动态规划","link":"/tags/%E5%8A%A8%E6%80%81%E8%A7%84%E5%88%92/"},{"name":"双指针","slug":"双指针","link":"/tags/%E5%8F%8C%E6%8C%87%E9%92%88/"},{"name":"区间合并","slug":"区间合并","link":"/tags/%E5%8C%BA%E9%97%B4%E5%90%88%E5%B9%B6/"},{"name":"堆","slug":"堆","link":"/tags/%E5%A0%86/"},{"name":"哈希表","slug":"哈希表","link":"/tags/%E5%93%88%E5%B8%8C%E8%A1%A8/"},{"name":"区间","slug":"区间","link":"/tags/%E5%8C%BA%E9%97%B4/"},{"name":"数学推理","slug":"数学推理","link":"/tags/%E6%95%B0%E5%AD%A6%E6%8E%A8%E7%90%86/"},{"name":"链表","slug":"链表","link":"/tags/%E9%93%BE%E8%A1%A8/"},{"name":"矩阵处理","slug":"矩阵处理","link":"/tags/%E7%9F%A9%E9%98%B5%E5%A4%84%E7%90%86/"},{"name":"滑动窗口","slug":"滑动窗口","link":"/tags/%E6%BB%91%E5%8A%A8%E7%AA%97%E5%8F%A3/"},{"name":"栈","slug":"栈","link":"/tags/%E6%A0%88/"},{"name":"字符串","slug":"字符串","link":"/tags/%E5%AD%97%E7%AC%A6%E4%B8%B2/"},{"name":"树","slug":"树","link":"/tags/%E6%A0%91/"},{"name":"written test","slug":"written-test","link":"/tags/written-test/"},{"name":"interview","slug":"interview","link":"/tags/interview/"},{"name":"计算机基础","slug":"计算机基础","link":"/tags/%E8%AE%A1%E7%AE%97%E6%9C%BA%E5%9F%BA%E7%A1%80/"},{"name":"语言","slug":"语言","link":"/tags/%E8%AF%AD%E8%A8%80/"},{"name":"操作系统","slug":"操作系统","link":"/tags/%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F/"},{"name":"正则匹配","slug":"正则匹配","link":"/tags/%E6%AD%A3%E5%88%99%E5%8C%B9%E9%85%8D/"},{"name":"prompt","slug":"prompt","link":"/tags/prompt/"},{"name":"NLP","slug":"NLP","link":"/tags/NLP/"},{"name":"code","slug":"code","link":"/tags/code/"},{"name":"pytorch","slug":"pytorch","link":"/tags/pytorch/"},{"name":"CVPR2023","slug":"CVPR2023","link":"/tags/CVPR2023/"},{"name":"CL","slug":"CL","link":"/tags/CL/"},{"name":"ZSL","slug":"ZSL","link":"/tags/ZSL/"},{"name":"GAN","slug":"GAN","link":"/tags/GAN/"},{"name":"多模态","slug":"多模态","link":"/tags/%E5%A4%9A%E6%A8%A1%E6%80%81/"},{"name":"扩散模型","slug":"扩散模型","link":"/tags/%E6%89%A9%E6%95%A3%E6%A8%A1%E5%9E%8B/"},{"name":"风格迁移","slug":"风格迁移","link":"/tags/%E9%A3%8E%E6%A0%BC%E8%BF%81%E7%A7%BB/"},{"name":"CVPR2022","slug":"CVPR2022","link":"/tags/CVPR2022/"},{"name":"Prototype","slug":"Prototype","link":"/tags/Prototype/"},{"name":"Transformer","slug":"Transformer","link":"/tags/Transformer/"},{"name":"文生图","slug":"文生图","link":"/tags/%E6%96%87%E7%94%9F%E5%9B%BE/"},{"name":"Multimodal","slug":"Multimodal","link":"/tags/Multimodal/"},{"name":"Data Synthesis","slug":"Data-Synthesis","link":"/tags/Data-Synthesis/"},{"name":"Data enhancement","slug":"Data-enhancement","link":"/tags/Data-enhancement/"}],"categories":[{"name":"algorithm","slug":"algorithm","link":"/categories/algorithm/"},{"name":"written test","slug":"written-test","link":"/categories/written-test/"},{"name":"interview","slug":"interview","link":"/categories/interview/"},{"name":"计算机基础","slug":"计算机基础","link":"/categories/%E8%AE%A1%E7%AE%97%E6%9C%BA%E5%9F%BA%E7%A1%80/"},{"name":"source code","slug":"source-code","link":"/categories/source-code/"},{"name":"paper","slug":"paper","link":"/categories/paper/"}],"pages":[]}