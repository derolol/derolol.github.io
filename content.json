{"posts":[{"title":"排序辅助算法题目集合","text":"leetcode274 H 指数 给你一个整数数组 citations ，其中 citations[i] 表示研究者的第 i 篇论文被引用的次数。计算并返回该研究者的 h 指数。 根据维基百科上 h 指数的定义：h 代表“高引用次数” ，一名科研人员的 h 指数 是指他（她）至少发表了 h 篇论文，并且 至少 有 h 篇论文被引用次数大于等于 h 。如果 h 有多种可能的值，h 指数 是其中最大的那个。 题解 12345678910int hIndex(vector&lt;int&gt;&amp; citations) { sort(citations.begin(), citations.end()); int h = 0, i = citations.size() - 1; while (i &gt;= 0 &amp;&amp; citations[i] &gt; h) { h++; i--; } return h; }","link":"/2024/07/17/algorithm/after-sort/"},{"title":"平衡二叉搜索树（BST）","text":"相关题目 leetcode108：将有序数组转换为二叉搜索树","link":"/2024/06/24/algorithm/binary-sort-tree/"},{"title":"双指针算法题目集合","text":"思路 指针从两端收缩 遍历解空间为上三角，参考解析 leetcode11 盛最多水的容器 给定一个长度为 n 的整数数组 height 。有 n 条垂线，第 i 条线的两个端点是 (i, 0) 和 (i, height[i]) 。 找出其中的两条线，使得它们与 x 轴共同构成的容器可以容纳最多的水。 返回容器可以储存的最大水量。 说明：你不能倾斜容器 题解 两端逼近，优先短边收缩 leetcode167. 两数之和 II - 输入有序数组 给你一个下标从 1 开始的整数数组 numbers ，该数组已按 非递减顺序排列 ，请你从数组中找出满足相加之和等于目标数 target 的两个数。如果设这两个数分别是 numbers[] 和 numbers[] ，则 1 &lt;= index1 &lt; index2 &lt;= numbers.length 。 以长度为 2 的整数数组 [, ] 的形式返回这两个整数的下标 和 。 你可以假设每个输入只对应唯一的答案 ，而且你不可以重复使用相同的元素。 你所设计的解决方案必须只使用常量级的额外空间。 题解 两端逼近，&lt; target 收缩左值，&gt; target 收缩右值","link":"/2024/07/11/algorithm/double-pointer/"},{"title":"动态规划算法题目集合","text":"牛客 2021360 春招编程题(第一批) 火星人的宝藏 X 星人发现了一个藏宝图，在藏宝图中标注了 N 个宝库的位置。这 N 个宝库连成了一条直线，每个宝库都有若干枚金币。 X 星人决定乘坐热气球去收集金币，热气球每次最多只能飞行 M 千米（假设热气球在飞行过程中并不会发生故障）此外，由于设计上的缺陷，热气球最多只能启动 K 次。 X 星人带着热气球来到了第 1 个宝库（达到第 1 个宝库时热气球尚未启动），收集完第 1 个宝库的金币后将启动热气球前往下一个宝库。如果他决定收集某一个宝库的金币，必须停下热气球，收集完之后再重新启动热气球。当然，X 星人每到一个宝库是一定会拿走这个宝库所有金币的。 已知每一个宝库距离第 1 个宝库的距离（单位：千米）和宝库的金币数量。 请问 X 星人最多可以收集到多少枚金币？ 输入描述： 单组输入。 第 1 行输入三个正整数 N、M 和 K，分别表示宝库的数量、热气球每次最多能够飞行的距离（千米）和热气球最多可以启动的次数，三个正整数均不超过 100，相邻两个正整数之间用空格隔开。 接下来 N 行每行包含两个整数，分别表示第 1 个宝库到某一个宝库的距离（千米）和这个宝库的金币枚数。（因为初始位置为第 1 个宝库，因此第 1 个宝库所对应行的第 1 个值为 0。） 输入保证所有的宝库按照到第 1 个宝库的距离从近到远排列，初始位置为第 1 个宝库。 输出描述： 输出一个正整数，表示最多可以收集的金币数。 示例 1 输入例子： 5 10 2 0 5 8 6 10 8 18 12 22 15 输出例子： 25 例子说明： X 星人启动热气球两次，分别收集第 1 个、第 3 个和第 4 个宝库的金币，一共可以得到的金币总数为 5+8+12=25 枚。 题解 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354#include &lt;iostream&gt;#include &lt;vector&gt;using namespace std;int main() { int N, M, K; while (cin &gt;&gt; N &gt;&gt; M &gt;&gt; K) { // 注意 while 处理多个 case vector&lt;vector&lt;int&gt;&gt; ship(N, vector&lt;int&gt;(2, 0)); vector&lt;vector&lt;int&gt;&gt; dp(N, vector&lt;int&gt;(K + 1, 0)); int k, v; for (int i = 0; i &lt; N; i ++) { cin &gt;&gt; k &gt;&gt; v; ship[i][0] = k; ship[i][1] = v; if (i &gt; 0 &amp;&amp; k &lt;= M) { dp[i][1] = max(dp[i - 1][1], v); } } if (N - 1 &lt;= K) { long long ret = 0; for (int i = 1; i &lt; N; i ++) { if (ship[i][0] - ship[i - 1][0] &gt; M) break; ret += ship[i][1]; // cout &lt;&lt; ret &lt;&lt; endl; } cout &lt;&lt; ret + ship[0][1]; return 0; } for (int k = 2; k &lt;= K; k ++) { for (int n = k; n &lt; N; n ++) { int nn = n - 1; int maxV = 0; while (nn &gt;= k - 1) { if (dp[nn][k - 1] == 0) { nn --; continue; } if (ship[n][0] - ship[nn][0] &gt; M) break; maxV = max(maxV, dp[nn][k - 1] + ship[n][1]); nn --; } dp[n][k] = maxV; // if (k == K) cout &lt;&lt; \"n: \" &lt;&lt; n &lt;&lt; \" k: \" &lt;&lt; k &lt;&lt; \" dp:\" &lt;&lt; dp[n][k] &lt;&lt; endl; } } int ret = 0; for (int i = K; i &lt; N; i ++) { ret = max(ret, dp[i][K]); } cout &lt;&lt; ret + ship[0][1]; } return 0;}// 64 位输出请用 printf(\"%lld\") leetcode221 最大正方形 在一个由 '0' 和 '1' 组成的二维矩阵内，找到只包含 '1' 的最大正方形，并返回其面积。 1dp[i][j] = min(min(dp[i - 1][j - 1], dp[i - 1][j]), dp[i][j - 1]) leetcode873 最长的斐波那契子序列的长度 思路 状态定义 其中表示以位置 i 和 j 结尾的子序列 12345678910111213141516171819202122232425class Solution {public: int lenLongestFibSubseq(vector&lt;int&gt;&amp; arr) { int n = arr.size(); if (n &lt; 3) return 0; if (n == 3) { if (arr[0] + arr[1] == arr[2]) return 3; return 0; } int maxLen = 0; vector&lt;vector&lt;int&gt;&gt; m(n, vector&lt;int&gt;(n, 0)); unordered_map&lt;int, int&gt; diff; for (int i = 1; i &lt; n - 1; i ++) { diff[arr[i - 1]] = i - 1; for (int j = i + 1; j &lt; n; j ++) { int interval = arr[j] - arr[i]; if (diff.count(interval)) { m[i][j] = m[diff[interval]][i] + 1; maxLen = max(maxLen, m[i][j]); } } } return maxLen &gt; 0 ? maxLen + 2 : 0; }};","link":"/2024/07/20/algorithm/dynamic-programming/"},{"title":"贪心算法题目集合","text":"leetcode452 用最少数量的箭引爆气球 有一些球形气球贴在一堵用 XY 平面表示的墙面上。墙面上的气球记录在整数数组 points ，其中 points[i] = [, ] 表示水平直径在 和 之间的气球。你不知道气球的确切 y 坐标。 一支弓箭可以沿着 x 轴从不同点 完全垂直 地射出。在坐标 x 处射出一支箭，若有一个气球的直径的开始和结束坐标为 , 且满足 ≤ x ≤ ，则该气球会被 引爆 。可以射出的弓箭的数量 没有限制 。 弓箭一旦被射出之后，可以无限地前进。 给你一个数组 points ，返回引爆所有气球所必须射出的 最小弓箭数 。 题解 排序，记录最小右边界（箭所能移动的最远方向），若不在射程范围内，记录新的右边界，否则更新最小右边界","link":"/2024/07/17/algorithm/greedy/"},{"title":"哈希表算法题目集合","text":"leetcode128 最长连续序列 给定一个未排序的整数数组 nums ，找出数字连续的最长序列（不要求序列元素在原数组中连续）的长度。 请你设计并实现时间复杂度为 O(n) 的算法解决此问题。 题解 unordered_set 去重 遍历找到各个连续子序列的头 123456789101112131415161718class Solution {public: int longestConsecutive(vector&lt;int&gt;&amp; nums) { unordered_set&lt;int&gt; num_set; for (auto n : nums) num_set.insert(n); int max_len = 0; for (auto n : num_set) { if (!num_set.count(n - 1)) { int cur_num = n; while (num_set.count(cur_num + 1)) { cur_num ++; } max_len = max(cur_num - n + 1, max_len); } } return max_len; }};","link":"/2024/07/12/algorithm/hash-table/"},{"title":"堆题目集合","text":"最小堆 排列组合 TopK 问题 优先队列 priority_queue 123456789101112131415161718192021222324252627282930313233343536#include &lt;iostream&gt;#include &lt;queue&gt;using namespace std;int main(){ priority_queue&lt;int,vector&lt;int&gt;,less&lt;int&gt;&gt; p1; //默认大根堆 p1.push(2); p1.push(1); p1.push(3); //输出 3 2 1 while(p1.empty()== false){ cout&lt;&lt;p1.top()&lt;&lt;\" \"; p1.pop(); } priority_queue&lt;int,vector&lt;int&gt;,greater&lt;int&gt;&gt; p2; //小根堆 p2.push(2); p2.push(1); p2.push(3); //输出 1 2 3 while(p2.empty()== false){ cout&lt;&lt;p2.top()&lt;&lt;\" \"; p2.pop(); } // 基本数据类型-pair-pair类型默认先比较第一个元素，第一个相等比较第二个 priority_queue&lt;pair&lt;int, int&gt; &gt; a; a.push(pair&lt;int,int&gt;{1,2}); a.push(pair&lt;int,int&gt;{1,3}); a.push(pair&lt;int,int&gt;{2,5}); while (a.empty()== false){ cout&lt;&lt;a.top().first&lt;&lt;\" \"&lt;&lt; a.top().second&lt;&lt;endl; a.pop(); } //输出: //2 5 //1 3 //1 2 return 0;} 多路归并 每次迭代有 N 个选择，将 N 个选择放入堆得到排序后的值 题型 一维 根据题意队列初始化 1 个值 二维 根据题意队列按序初始化 1 个列表 初始化左上角值，set 判断是否重复插入 注意去重的问题 leetcode264 丑数 II（一维） 给你一个整数 n ，请你找出并返回第 n 个 丑数 。 丑数 就是质因子只包含 2、3 和 5 的正整数。 1234567891011121314151617181920int nthUglyNumber(int n) { long times_num[] = {2, 3, 5}; set&lt;long&gt; record; priority_queue&lt;long, vector&lt;long&gt;, greater&lt;long&gt;&gt; q; q.push(1); int index = 1; while (index &lt;= n) { int cur_num = q.top(); if (index == n) return cur_num; q.pop(); for (long t : times_num) { long new_num = cur_num * t; if (record.contains(new_num)) continue; q.push(new_num); record.insert(new_num); } index ++; } return 0;} leetcode373 查找和最小的 K 对数字（二维） 给定两个以 非递减顺序排列 的整数数组 和 , 以及一个整数 。 定义一对值 ，其中第一个元素来自 ，第二个元素来自 。 请找到和最小的 k 个数对 。 12345678910111213141516171819202122vector&lt;vector&lt;int&gt;&gt; kSmallestPairs(vector&lt;int&gt;&amp; nums1, vector&lt;int&gt;&amp; nums2, int k) { vector&lt;vector&lt;int&gt;&gt; result; int l1 = nums1.size(), l2 = nums2.size(); auto cmp = [&amp;](const auto&amp; a, const auto&amp; b) { return nums1[a.first] + nums2[a.second] &gt; nums1[b.first] + nums2[b.second]; }; priority_queue&lt;pair&lt;int, int&gt;, vector&lt;pair&lt;int, int&gt;&gt;, decltype(cmp)&gt; q(cmp); for (int i = 0; i &lt; l1; i ++) { q.push({i, 0}); } while (result.size() &lt; k) { auto [ia, ib] = q.top(); result.push_back({nums1[ia], nums2[ib]}); q.pop(); if (ib + 1 &lt; l2) q.push(pair&lt;int, int&gt;{ia, ib + 1}); } return result;}","link":"/2024/08/01/algorithm/heap/"},{"title":"链表题目集合","text":"leetcode86 分隔链表 思路 遍历链表，分两个链表存储","link":"/2024/08/01/algorithm/linked-list/"},{"title":"数学推理算法题目集合","text":"2024 360 秋招提前批 最大曼哈顿距离 给定一个包含若干个点的集合，求其中任意两点之间的最大/最小曼哈顿距离。 题解 由上述式子可得： 整理合并可得： 所以，问题转换为比较点和点的横纵坐标和与差。 在遍历各个点的时候分别计算横纵坐标的和与差，且分别排序。 将最大值减去最小值得到两种情况下的最大曼哈顿距离，然后再从中选择最大的曼哈顿距离作为最终答案。 123456789vector&lt;int&gt; d1;vector&lt;int&gt; d2;for (int i = 0; i &lt; points.size(); i ++) d1.push_back(points[i][0] - points[i][1]) d2.push_back(points[i][0] + points[i][1])sort(d1.begin(), d1.end())sort(d2.begin(), d2.end())cout &lt;&lt; max(d1[d1.size() - 1] - d1[0], d2[d2.size() - 1] - d2[0])) leetcode172 阶乘后的零 给定一个整数 n ，返回 n! 结果中尾随零的数量。 提示 1234567891011int trailingZeroes(int n) { int times = 0; for (int i = 5; i &lt;= n; i += 5) { int count = i; while (count &amp;&amp; count % 5 == 0) { times ++; count /= 5; } } return times;} leetcode201 数字范围按位与 给你两个整数 left 和 right ，表示区间 [left, right] ，返回此区间内所有数字 按位与 的结果（包含 left 、right 端点）。 题解 相邻两个数相与，末尾的 0 将会被消除，通过移位找公共前缀得到区间相与结果 12345678910int rangeBitwiseAnd(int left, int right) { int shift = 0; // 找到公共前缀 while (left &lt; right) { left &gt;&gt;= 1; right &gt;&gt;= 1; ++shift; } return left &lt;&lt; shift;} leetcode1823 找出游戏的获胜者 思路 单链表模拟 取余推测 leetcode 题解 leetcode29 两数相除 思路 特殊情况处理 除数左移位，与被除数比较 leetcode238 除自身以外数组的乘积 题目 给你一个整数数组 nums，返回 数组 answer ，其中 answer[i] 等于 nums 中除 nums[i] 之外其余各元素的乘积 。 题目数据 保证 数组 nums 之中任意元素的全部前缀元素和后缀的乘积都在 32 位 整数范围内。 请不要使用除法，且在 O(n) 时间复杂度内完成此题。 题解 正反向前缀积，使用中间变量记录累积 123456789101112131415161718class Solution {public: vector&lt;int&gt; productExceptSelf(vector&lt;int&gt;&amp; nums) { vector&lt;int&gt; ret(nums.size()); int mul = nums[0]; ret[0] = 1; for (int i = 1; i &lt; nums.size(); i ++) { ret[i] = mul; mul *= nums[i]; } mul = nums[nums.size() - 1]; for (int i = nums.size() - 2; i &gt;= 0; i --) { ret[i] *= mul; mul *= nums[i]; } return ret; }};","link":"/2024/07/21/algorithm/math/"},{"title":"reorganize-string","text":"重构字符串 中等 相关标签 相关企业 提示 给定一个字符串 s ，检查是否能重新排布其中的字母，使得两相邻的字符不同。 返回 s 的任意可能的重新排列。若不可行，返回空字符串 \"\" 。 https://leetcode.cn/problems/reorganize-string/solutions/","link":"/2024/06/28/algorithm/reorganize-string/"},{"title":"矩阵处理算法题目集合","text":"leetcode48 旋转图像 思路 1/4 矩阵旋转 leetcode289 生命游戏 根据 百度百科 ， 生命游戏 ，简称为 生命 ，是英国数学家约翰·何顿·康威在 1970 年发明的细胞自动机。 给定一个包含 m × n 个格子的面板，每一个格子都可以看成是一个细胞。每个细胞都具有一个初始状态： 1 即为 活细胞 （live），或 0 即为 死细胞 （dead）。每个细胞与其八个相邻位置（水平，垂直，对角线）的细胞都遵循以下四条生存定律： 如果活细胞周围八个位置的活细胞数少于两个，则该位置活细胞死亡； 如果活细胞周围八个位置有两个或三个活细胞，则该位置活细胞仍然存活； 如果活细胞周围八个位置有超过三个活细胞，则该位置活细胞死亡； 如果死细胞周围正好有三个活细胞，则该位置死细胞复活； 下一个状态是通过将上述规则同时应用于当前状态下的每个细胞所形成的，其中细胞的出生和死亡是同时发生的。给你 m x n 网格面板 board 的当前状态，返回下一个状态。 题解 leecode题解 使用二进制的第二位存储修改后的状态。 12345678910111213141516171819202122232425262728293031323334class Solution {public: void gameOfLife(vector&lt;vector&lt;int&gt;&gt;&amp; board) { int dx[] = {-1, 0, 1, -1, 1, -1, 0, 1}; int dy[] = {-1, -1, -1, 0, 0, 1, 1, 1}; for(int i = 0; i &lt; board.size(); i++) { for(int j = 0 ; j &lt; board[0].size(); j++) { int sum = 0; for(int k = 0; k &lt; 8; k++) { int nx = i + dx[k]; int ny = j + dy[k]; if(nx &gt;= 0 &amp;&amp; nx &lt; board.size() &amp;&amp; ny &gt;= 0 &amp;&amp; ny &lt; board[0].size()) { sum += (board[nx][ny]&amp;1); // 只累加最低位 } } if(board[i][j] == 1) { if(sum == 2 || sum == 3) { board[i][j] |= 2; // 使用第二个bit标记是否存活 } } else { if(sum == 3) { board[i][j] |= 2; // 使用第二个bit标记是否存活 } } } } for(int i = 0; i &lt; board.size(); i++) { for(int j = 0; j &lt; board[i].size(); j++) { board[i][j] &gt;&gt;= 1; //右移一位，用第二bit覆盖第一个bit。 } } }};","link":"/2024/07/22/algorithm/matrix/"},{"title":"滑动窗口题目集合","text":"leetcode209 长度最小的子数组 题目 给定一个含有 n 个正整数的数组和一个正整数 target 。 找出该数组中满足其总和大于等于 target 的长度最小的子数组[, , ..., , ] ，并返回其长度。如果不存在符合条件的子数组，返回 0 。 题解 确定左值，向右扩展直到满足 target，记录当前窗口大小，移动左值，重复上述动作，直到右值不能再拓展 123456789101112131415161718192021222324class Solution {public: int minSubArrayLen(int target, vector&lt;int&gt;&amp; nums) { int size = nums.size(); int left = 0; int right = 0; int sum = nums[0]; int minLen = size + 1; while (left &lt; size &amp;&amp; right &lt; size) { if (sum &gt;= target) { minLen = min(minLen, right - left + 1); sum -= nums[left]; left ++; continue; } if (right == size - 1) break; right ++; sum += nums[right]; } if (minLen == size + 1) return 0; return minLen; }};","link":"/2024/08/01/algorithm/sliding-window/"},{"title":"栈算法题目集合","text":"leetcode71 简化路径 给你一个字符串 path ，表示指向某一文件或目录的 Unix 风格 绝对路径 （以 '/' 开头），请你将其转化为更加简洁的规范路径。 在 Unix 风格的文件系统中，一个点（.）表示当前目录本身；此外，两个点 （..） 表示将目录切换到上一级（指向父目录）；两者都可以是复杂相对路径的组成部分。任意多个连续的斜杠（即，'//'）都被视为单个斜杠 '/' 。 对于此问题，任何其他格式的点（例如，'...'）均被视为文件/目录名称。 请注意，返回的 规范路径 必须遵循下述格式： 始终以斜杠 '/' 开头。 两个目录名之间必须只有一个斜杠 '/' 。 最后一个目录名（如果存在）不能 以 '/' 结尾。 此外，路径仅包含从根目录到目标文件或目录的路径上的目录（即，不含 '.' 或 '..'）。 返回简化后得到的规范路径。 题解 参考题解 leetcode150 逆波兰表达式求值 给你一个字符串数组 tokens ，表示一个根据 逆波兰表示法 表示的算术表达式。 请你计算该表达式。返回一个表示表达式值的整数。 注意： 有效的算符为 '+'、'-'、'*' 和 '/' 。 每个操作数（运算对象）都可以是一个整数或者另一个表达式。 两个整数之间的除法总是 向零截断 。 表达式中不含除零运算。 输入是一个根据逆波兰表示法表示的算术表达式。 答案及所有中间计算结果可以用 32 位 整数表示。 题解 从左向右遍历，用栈存储数字，遇到符号出栈两个数字 12345678910111213141516171819202122232425262728class Solution {public: int evalRPN(vector&lt;string&gt;&amp; tokens) { long result = 0; stack&lt;int&gt; nums; for (string token : tokens) { if (token != \"+\" &amp;&amp; token != \"-\" &amp;&amp; token != \"*\" &amp;&amp; token != \"/\") { nums.push(stoi(token)); continue; } int n2 = nums.top();nums.pop(); int n1 = nums.top();nums.pop(); if (token == \"+\") { nums.push(n1 + n2); } else if (token == \"-\") { nums.push(n1 - n2); } else if (token == \"*\") { nums.push(n1 * n2); } else if (token == \"/\") { nums.push(n1 / n2); } } return nums.top(); }};","link":"/2024/07/12/algorithm/stack/"},{"title":"字符串题目集合","text":"leetcode767 重构字符串 返回 s 的任意可能的重新排列。若不可行，返回空字符串 \"\" 。 题解 [参考题解]h(ttps://leetcode.cn/problems/reorganize-string/solutions/)","link":"/2024/08/01/algorithm/string/"},{"title":"树算法题目集合","text":"leetcode199 二叉树的右视图 给定一个二叉树的根节点 root，想象自己站在它的右侧，按照从顶部到底部的顺序，返回从右侧所能看到的节点值。 leetcode230 二叉搜索树中第 K 小的元素 给定一个二叉搜索树的根节点 root ，和一个整数 k ，请你设计一个算法查找其中第 k 小的元素（从 1 开始计数）。 题解 root-&gt;left-&gt;val &lt; root-&gt;val &lt; root-&gt;right-&gt;val 12345678910111213141516171819202122232425int kthSmallest(TreeNode* root, int k) { int count = 0; stack&lt;TreeNode*&gt; treeStack; treeStack.push(root); while (treeStack.size() &gt; 0) { TreeNode* base = treeStack.top(); if (base-&gt;left == nullptr) { count ++; treeStack.pop(); if (count == k) { return base-&gt;val; } if (base-&gt;right != nullptr) { treeStack.push(base-&gt;right); continue; } } else { treeStack.push(base-&gt;left); base-&gt;left = nullptr; continue; } } return 0;} leetcode148 排序链表 给你链表的头结点 head ，请将其按升序排列并返回排序后的链表 题解 归并排序链表，递归分治，子链表按序合并 树和算式 Leetcode 题解 各种表达式没有本质区别，他们其实是同一个语法树，只是遍历方式不同而得到的不同式子；是一个事物的一体多面，只不过是从不同角度观察罢了。 中缀表达式是其对应的语法树的中序遍历； 后缀表达式是其对应的语法树的后序遍历； 前缀表达式是其对应的语法树的前序遍历； leetcode94 树的中序遍历 中序遍历 使用栈数据结构控制遍历节点顺序，类比递归调用的栈 12345678910111213141516171819202122232425262728293031323334/** * Definition for a binary tree node. * struct TreeNode { * int val; * TreeNode *left; * TreeNode *right; * TreeNode() : val(0), left(nullptr), right(nullptr) {} * TreeNode(int x) : val(x), left(nullptr), right(nullptr) {} * TreeNode(int x, TreeNode *left, TreeNode *right) : val(x), left(left), right(right) {} * }; */class Solution {public: vector&lt;int&gt; inorderTraversal(TreeNode* root) { vector&lt;int&gt; ret; if (root == nullptr) return ret; stack&lt;TreeNode*&gt; tree; tree.push(root); while (! tree.empty()) { TreeNode* top = tree.top(); if (top-&gt;left != nullptr) { tree.push(top-&gt;left); top-&gt;left = nullptr; continue; } ret.push_back(top-&gt;val); tree.pop(); if (top-&gt;right != nullptr) { tree.push(top-&gt;right); } } return ret; }};","link":"/2024/07/16/algorithm/tree/"},{"title":"Hexo操作","text":"Hexo 在子目录中新建文章 1hexo new [layout] -p path/&lt;title&gt;","link":"/2024/07/08/hexo/basic/"},{"title":"人工智能代理（Artificial Intelligence Agent）","text":"AI Agent深度解析：潜力与挑战并存的智能新世界 定义 感知环境、进行决策和执行动作的智能实体 应用 ABI/GBI生成式BI或是数据分析 Code Agent代码助手 基于RAG技术的知识问答 Coding Agent aiXcoder 代码大模型在企业的应用实践 PPT 语言模型发展历程 深度神经网络 预训练模型 大语言模型 论文 aiXcoder构建 训练数据收集与筛选 Github数据爬取与过滤 删除低质量项目 相似算法去重 去除敏感数据 去除注释数据 删除语法错误代码 删除静态分析缺陷代码 代码文件顺序重排 训练 任务 随机Mask抽象语法树节点 存在问题 项目级代码生成下的长序列依赖 测试 更符合真实场景的测评集 基于CodeFuse的下一代研发探索 PPT Github 数据来源 Github 预训练&amp;微调 推理加速&amp;部署 发展 仓库级代码 大模型落地到代码助手场景的探索实践 PPT 百度大模型驱动下的智能代码助手提效实践 https://qcon.infoq.cn/2023/shanghai/presentation/5679","link":"/2024/07/09/knowledge/ai-agent/"},{"title":"计算机基础 computer-fundamental","text":"数据库 left join 和 right join、union 是怎么操作的","link":"/2024/06/27/knowledge/computer-fundamental/"},{"title":"computer-network","text":"http 中的 get、post 区别，是怎么做的","link":"/2024/06/26/knowledge/computer-network/"},{"title":"计算机视觉 Computer Vision","text":"【三年面试五年模拟】算法工程师的求职面试秘籍 &gt; 从 ReLU 到 GELU，一文概览神经网络的激活函数 https://github.com/DWCTOD/interview/blob/master/detail/%E4%BD%9C%E4%B8%9A%E5%B8%AE%20%E8%A7%86%E8%A7%89%E7%AE%97%E6%B3%95%E5%B7%A5%E7%A8%8B%E5%B8%88%20%E9%9D%A2%E7%BB%8F%EF%BC%882020%E5%B1%8A%EF%BC%89.md https://github.com/GYee/CV_interviews_Q-A 图像处理与计算机视觉基础 基本概念 原理 图像预处理 特征提取 对象检测 图像分类 图像分割 OpenCV 库 读写图像 图像滤波 几何变换 特征检测与描述 深度学习基础 梯度下降 滑动平均 模型微调（Fine-tuning） 基础模块 池化层 Pooling Layer 归一化层 Normalization Layer BN，Batch Normalization IN，Instance Normalization LN，Layer Normalization GN，Group Normalization BN 是怎么做，作用是什么 激活层 常见激活函数 Sigmoid Tanh ReLU LeakyReLU SoftPlus ELU SELU，自归一化 Swish，类 Sigmoid 作为开关 GELU GLU 特性 梯度消失 存在偏导过小 梯度爆炸 偏导累乘过大 梯度裁剪 输出均值为 0 能避免每次权重只能往单一反向变化 ReLU 计算复杂度低 ReLU 的负半轴为输出值增加稀疏性，减少计算量，但同时会让一些神经元不能更新 SoftPlus，ReLU 的平滑 全连接层 Linear 嵌入层 Embedding 卷积层 Convolution 特征 局部感知、权值共享、平移不变、多核 1×1 卷积 特征增强 特征融合 改变通道数 分类 空洞卷积 分组卷积 转置卷积层 Transpose Convolution 优化模块 残差结构 Residual Connection 将输入与层输出相加 优势 缓解梯度消失，增加网络深度 保留信息，特征重用 空间金字塔池化（Spatial Pyramid Pooling，SPP） 空洞空间金字塔池化（Atrous Spatial Pyramid Pooling，ASPP） HDC 可变形卷积 Deformable Convolution 可分离卷积 Separable Convolution Transformer 模块结构 多头自注意力机制（Multi-Head Self-Attention Mechanism） 前馈神经网络（Feed-Forward Neural Network） 层归一化（Layer Normalization） 残差连接（Residual Connections） Self-Attention 除以的原因 矩阵计算导致的元素值整体偏大，从而引发梯度消失 计算后的数据平方差为，除以，将分布的方差纠正回接近 1 并行化的体现 序列计算多头注意力 影响计算量的因素 序列长度：点积、矩阵乘 头数量 优势 并行处理整个序列 长距离依赖 缺点 计算量大 超参调优 超长序列处理能力 Cross-Attention Convolutional Attention SENet -CBAM 基础模型 前馈神经网络 卷积神经网络（CNN） 循环神经网络（RNN） 长短时记忆网络（LSTM） Inception 模型调优 模型优化 正则化 L1 正则化 L2 正则化 损失函数 已知 softmax 输出概率序列与实际分布概率序列，计算两者交叉熵 超参数调整 在深度学习中，超参数（Hyperparameters）是指在训练开始前设置的模型参数，不是通过训练学习得到的。超参数的选择对模型性能有很大的影响，不同的超参数设置可能导致显著不同的训练结果。 优化器选择 SGD AdaGrad RMSProp Adam 学习率衰减 LR 中的连续值特征是如何处理的 为什么 LR 要先对数据进行归一化处理 LR 用了 sigmoid 函数，那么 LR 是线性模型还是非线性模型，为什么 线性 分段 余弦 WarmUp 周期性 缩放法则 Scaling-Law 在 AI 领域中，描述模型性能如何随着模型规模（如参数数量、训练数据量、计算资源等）变化而变化的一组经验法则 应用 设计更大规模的模型 指导研究人员如何设计和训练更大规模的模型，以实现更高的性能 优化资源分配 如确定是否应增加模型参数数量、增加训练数据量，还是增加计算资源，以实现最优的性能提升 预测性能 根据现有模型的性能和缩放法则，可以预测更大规模模型的性能 常见模型评估指标 准确率 召回率 F1 分数 浮点数运算次数 FLOPs 帧每秒 FPS 深度学习框架 训练范式 123456789101112131415161718192021222324252627# 加载数据dataloader = DataLoader()# 加载模型model = MyModel()# 损失函数criterion = nn.CrossEntropyLoss()# 优化器optimizer = optim.Adam(model.parameters(), lr=0.001)# 学习率衰减策略# 训练num_epochsfor epoch in range(num_epochs): # 遍历完整数据集 for inputs, labels in dataloader: # 梯度置零 optimizer.zero_grad() # 模型推理 outputs = model(inputs) # 计算损失 loss = criterion(outputs, labels) # 累加梯度 loss.backward() # 梯度更新 optimizer.step() 评估范式 1234567# 设置模型评估模式model.eval()# 取消梯度更新with torch.no_grad(): for inputs, labels in test_dataloader: outputs = model(inputs) # 计算准确率或其他指标 PyTorch Tensor 存储 头信息区（Tensor）：tensor 的形状（size）、步长（stride）、数据类型（type）等 存储区（Storage）：数据 stride 属性 指定维度中一个元素到下一个元素的步长 维度变换 类型 方法 描述 维度顺序 permute 指定维度重排，返回共享存储区的 tensor transpose 交换维度，返回共享存储区的 tensor 形状变换 view 返回共享存储区 tensor，要求存储连续，否则调用 contiguous contiguous 开辟新的存储区构建连续 tensor reshape 若连续则返回原 tensor，否则创建新 tensor 广播 broadcast_to 冗余维度 squeeze 压缩维度 unsqueeze 展开维度 扩展维度 expand 扩展大小为 1 的维度 repeat 按照指定维度重复 tensor 展平维度 flatten ravel 维度剪裁 narrow 维度展开 unfold 张量乘法 方法 应用 torch.matmul() 多维矩阵相乘 torch.mm() 2 维矩阵相乘 torch.bmm() 批矩阵相乘 torch.dot() 点积 torch.mv() 矩阵向量相乘 torch.einsum() 复杂张量运算 torch.einsum('ij,jk-&gt;ik', a, b) 张量合并与拆分 stack 扩展维度拼接 cat 根据维度拼接 split 按大小分 chunk 按块分 nn.Module 模块基类 nn.Sequential 线性模块容器 计算图 环境搭建 数据加载 模型定义 训练 验证 保存 加载模型 PytorchLightning 逻辑思维与项目经验 逻辑思维 准备通过解决实际问题来展示你的逻辑思维能力和数据分析洞察力，可以是以往项目中的案例分析。 团队合作与挑战接受度 思考并准备实例说明你如何在团队中有效沟通、协作解决问题，以及面对技术挑战时的态度和解决策略。","link":"/2024/06/26/knowledge/computer-vision/"},{"title":"计算机视觉 Computer Vision","text":"【三年面试五年模拟】算法工程师的求职面试秘籍 &gt; 从 ReLU 到 GELU，一文概览神经网络的激活函数 https://github.com/DWCTOD/interview/blob/master/detail/%E4%BD%9C%E4%B8%9A%E5%B8%AE%20%E8%A7%86%E8%A7%89%E7%AE%97%E6%B3%95%E5%B7%A5%E7%A8%8B%E5%B8%88%20%E9%9D%A2%E7%BB%8F%EF%BC%882020%E5%B1%8A%EF%BC%89.md https://github.com/GYee/CV_interviews_Q-A 图像处理与计算机视觉基础 基本概念 原理 图像预处理 特征提取 对象检测 图像分类 图像分割 OpenCV 库 读写图像 图像滤波 几何变换 特征检测与描述 深度学习基础 梯度下降 滑动平均 模型微调（Fine-tuning） 基础模块 池化层 Pooling Layer 归一化层 Normalization Layer BN，Batch Normalization IN，Instance Normalization LN，Layer Normalization GN，Group Normalization BN 是怎么做，作用是什么 激活层 常见激活函数 Sigmoid Tanh ReLU LeakyReLU SoftPlus ELU SELU，自归一化 Swish，类 Sigmoid 作为开关 GELU GLU 特性 梯度消失 存在偏导过小 梯度爆炸 偏导累乘过大 梯度裁剪 输出均值为 0 能避免每次权重只能往单一反向变化 ReLU 计算复杂度低 ReLU 的负半轴为输出值增加稀疏性，减少计算量，但同时会让一些神经元不能更新 SoftPlus，ReLU 的平滑 全连接层 Linear 嵌入层 Embedding 卷积层 Convolution 特征 局部感知、权值共享、平移不变、多核 1×1 卷积 特征增强 特征融合 改变通道数 分类 空洞卷积 分组卷积 转置卷积层 Transpose Convolution 上采样 插值 反卷积 PixelShuffle 优化模块 残差结构 Residual Connection 将输入与层输出相加 优势 缓解梯度消失，增加网络深度 保留信息，特征重用 空间金字塔池化（Spatial Pyramid Pooling，SPP） 空洞空间金字塔池化（Atrous Spatial Pyramid Pooling，ASPP） HDC 可变形卷积 Deformable Convolution 可分离卷积 Separable Convolution Transformer 模块结构 多头自注意力机制（Multi-Head Self-Attention Mechanism） 前馈神经网络（Feed-Forward Neural Network） 层归一化（Layer Normalization） 残差连接（Residual Connections） Self-Attention 除以的原因 矩阵计算导致的元素值整体偏大，从而引发梯度消失 计算后的数据平方差为，除以，将分布的方差纠正回接近 1 并行化的体现 序列计算多头注意力 影响计算量的因素 序列长度：点积、矩阵乘 头数量 优势 并行处理整个序列 长距离依赖 缺点 计算量大 超参调优 超长序列处理能力 Cross-Attention Convolutional Attention SENet -CBAM 基础模型 前馈神经网络 卷积神经网络（CNN） 循环神经网络（RNN） 长短时记忆网络（LSTM） 模型调优 模型优化 正则化 L1 正则化 L2 正则化 损失函数 已知 softmax 输出概率序列与实际分布概率序列，计算两者交叉熵 超参数调整 在深度学习中，超参数（Hyperparameters）是指在训练开始前设置的模型参数，不是通过训练学习得到的。超参数的选择对模型性能有很大的影响，不同的超参数设置可能导致显著不同的训练结果。 优化器选择 SGD AdaGrad RMSProp Adam 学习率衰减 LR 中的连续值特征是如何处理的 为什么 LR 要先对数据进行归一化处理 LR 用了 sigmoid 函数，那么 LR 是线性模型还是非线性模型，为什么 线性 分段 余弦 WarmUp 周期性 缩放法则 Scaling-Law 在 AI 领域中，描述模型性能如何随着模型规模（如参数数量、训练数据量、计算资源等）变化而变化的一组经验法则 应用 设计更大规模的模型 指导研究人员如何设计和训练更大规模的模型，以实现更高的性能 优化资源分配 如确定是否应增加模型参数数量、增加训练数据量，还是增加计算资源，以实现最优的性能提升 预测性能 根据现有模型的性能和缩放法则，可以预测更大规模模型的性能 常见模型评估指标 准确率 召回率 F1 分数 浮点数运算次数 FLOPs 帧每秒 FPS 深度学习框架 训练范式 PyTorch Tensor 存储 头信息区（Tensor）：tensor 的形状（size）、步长（stride）、数据类型（type）等 存储区（Storage）：数据 stride 属性 指定维度中一个元素到下一个元素的步长 view 方法 返回共享存储区的 tensor 计算图 环境搭建 数据加载 模型定义 训练 验证 保存 加载模型 PytorchLightning 逻辑思维与项目经验 逻辑思维 准备通过解决实际问题来展示你的逻辑思维能力和数据分析洞察力，可以是以往项目中的案例分析。 团队合作与挑战接受度 思考并准备实例说明你如何在团队中有效沟通、协作解决问题，以及面对技术挑战时的态度和解决策略。","link":"/2024/06/26/knowledge/cv/"},{"title":"数据结构 Data Structure","text":"数组 用连续内存存储数据 读写操作复杂度 O(1) 字符串 用连续内存存储字符 链表 由指针把若干个节点连接成链状结构 树 节点之间用指针链接 除根节点之外每个节点只有一个父节点，根节点没有父节点 叶子节点没有子节点 二叉树 每个节点最多只能有两个子节点 二叉搜索树 若其左子树不为NULL，则左子树上所有节点的值都＜根节点的值 若其右子树不为NULL，则右子树上所有节点的值都＞根节点的值 其左右子树也分别是二叉搜索树 查询复杂度 堆 最大堆：根节点的值最大 最小堆：根节点的值最小 栈 先进后出 队列 先进先出 图 多线程 异常处理 算法 排序 算法 时间复杂度 稳定性 冒泡排序 √ 选择排序 × 归并排序 √ 快速排序 × 堆排序 × 查找 二分查找 动态规划","link":"/2024/06/26/knowledge/data-structure/"},{"title":"图像数据集 Image Dataset","text":"数据集建立 数据增强 AutoAugment：搜索最优图像处理操作组合 RandAugment：已有图像处理集合，确定操作次数 N 和操作幅度 M 分类任务 Mixup：按比例混合图像和标签 Cutout：图像掩码 CutMix：图像掩码混合 目标检测任务 Mosaic：图像合成 语义分割任务 Copy-Paste：分割结果粘贴到另一张图 数据预处理 数据增强 旋转、平移、缩放、翻转、随机色调(H)、饱和度(S)、明度(V)调整、等 数据归一化 Normalization Min-Max 归一化 Z-Score 归一化 RobustScaler 归一化 One-Hot 编码 数据类别不平衡问题 采样比例 数据生成","link":"/2024/06/26/knowledge/dataset/"},{"title":"deep-learning","text":"【三年面试五年模拟】算法工程师的求职面试秘籍 分类 Classification 将输入数据划分到预定义的有限标签中，输出为预测的类别标签 常用评价指标 准确率 精确率 召回率 F1 分数 应用 花卉图像分类 垃圾邮件拦截 回归 Regression 建立数值型随机自变量的模型并进行连续的因变量预测，输出为数值 常用评价指标 均方误差 R2 分数 应用 股票价格预测 房价预测 聚类 Clustering 将无标签的数据分成多个类（簇），确保类内样本相似，类间样本相异，其输出是聚类结果（簇划分，簇标签，簇中心等） 常用评价指标 样本紧密度 样本分隔度 应用 用户分群 异常检测 决策 Decision making 通过神经网络理解给定目标，约束条件和可用信息，预测出最佳或满意的动作决策，其输出是一连串的动作 常用评价指标 最终回报 平均奖励 应用 游戏 AI 自动驾驶 概率密度估计 Probability density estimation 使用深度神经网络来估计一个随机变量或一组随机变量的概率密度函数，其输出是数据的概率分布 常用评价指标衡量分布差异 对数似然损失 KL 散度 应用 数据生成 样本采样","link":"/2024/06/27/knowledge/deep-learning/"},{"title":"generative-model","text":"VAE AE VAE VAE GAN Diffusion GAN DDPM 生成过程被设计成一系列的马尔可夫步骤，其中每一个步骤只依赖于前一步的状态 DDIM 采样 DDIM 在每一步都会预测并尝试直接达到最终的清晰状态，而不是仅仅依赖于当前的模糊状态 DDPM 通过马尔科夫链推导下一状态，DDIM 通过减少推理步数加速推理 Stable Diffusion ControlNet RAG DIT 指标 MSE 计算公式 PSNR 单位：信噪比 计算公式 SSIM 通过均值和协方差衡量两张图像的 Luminance（亮度）、Contrast（对比度）和 Structure（结构） 计算公式 取值范围：[-1, 1]","link":"/2024/07/01/knowledge/generative-model/"},{"title":"hpc","text":"GPU 编程与性能优化 CUDA 与 cuDNN GPU 编程基本原理 特别是如何使用 CUDA 进行并行计算，以及 cuDNN 库在加速深度学习中的应用 内存管理 核函数设计 性能监控与调优 性能优化 掌握一些基本的性能分析工具和方法，比如使用 nvprof 或 TensorFlow Profiler 分析模型运行瓶颈，并实施相应的优化措施。","link":"/2024/06/26/knowledge/hpc/"},{"title":"leetcode-host","text":"编程题： 逆时针打印数组 （剑指 offer 和 leetcode54 都有的常见题，常为顺时针打印数组） 给先序遍历重构二叉树 （例如输入为 124XXX3XX，X 表示空，无叶子节点） 有随机数 0-2 0-3 0-4 构建 100 的随机数 （使用 0-3 和 0-4 构建 20 与 0-4 构建的 5 形成 100 的随机数） 智力题： 49 个人中至少几个人生日是同一月 两个人只握一次手，一共握了 45 次，问一共几个人（10 人） 编程题： 数组合并（leetcode88）【简单】 区间合并，也叫线段合并（leetcode56）【中等】 以上内容+能否完全覆盖，题目为： 单个线段[2,6]可称为完全覆盖[4,6]，现有两组线段 AB，每组中有一定数目的线段，判断 A 组能否完全覆盖 B 组 例如： [[1, 3], [2, 6]] [[1, 4], [4, 5]] True [[1, 2], [4, 7]] [[2, 5], [6, 7]] False 非递归中序遍历 重建二叉树 根据前序和中序遍历，返回后序遍历 用两个队列实现一个栈 解法：一个队列放入，一个队列输出。因为栈是后入先出，所以把 q1 的元素依次删除并插入 q2，再删除最后一个元素。然后 q1 赋值为 q2，q2 初始化为空，这样才能不断删除。 问题 1：交叉熵公式 解答：交叉熵公式如下： 这里公式定义，x、y 都是表示概率分布。其中 x 是正确的概率分布，而 y 是我们预测出来的概率分布，这个公式算出来的结果，表示 y 与正确答案 x 之间的错误程度（即：y 错得有多离谱），结果值越小，表示 y 越准确，与 x 越接近。 问题 3：对后验概率估计的思考 解答：对于很多条件概率问题，可以等价于求后验概率问题。 问题 4：针对归一化问题的数据线性排序思考 解答：基数排序是一种针对该问题很好的解决方式，往往因为其平均复杂度为被忽略其线性。 问题 5：带有容错的最长公共子串如何实现（动态规划问题） 解答： 暂时还没想到。 问题 6：剑指 Offer 原题，螺旋遍历 解答：主要找规律找出循环条件：并且。 https://github.com/DWCTOD/interview/blob/master/detail/%E4%BD%9C%E4%B8%9A%E5%B8%AE%20%E8%A7%86%E8%A7%89%E7%AE%97%E6%B3%95%E5%B7%A5%E7%A8%8B%E5%B8%88%20%E9%9D%A2%E7%BB%8F%EF%BC%882020%E5%B1%8A%EF%BC%89.md 第一题 leetcode29 题 第二题： 给定 n，用 1 到 n 作为二叉搜索树的节点值，返回 n 个点所能组成的二叉搜索树的个数 如 n=3 大数乘法 https://www.nowcoder.com/discuss/353156289771544576","link":"/2024/06/25/knowledge/leetcode-host/"},{"title":"数学基础","text":"微积分 求导 梯度 偏微分 积分 统计学，概率论 推断统计学和概率论的基本概念 高斯分布 贝叶斯公式 最小二乘法 线性回归 逻辑回归 线性代数 向量 向量空间 向量投影 矩阵运算 特征值分解 SVD分解","link":"/2024/07/08/knowledge/math/"},{"title":"opencv","text":"https://segmentfault.com/a/1190000044071469 cv2.imread(path, flag) flag： 12[return] type: numpy.ndarray size: (H, W, C), C -&gt; (BGR) ImageToTensor 12345image = cv2.imread(path)image_rgb = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)image = torch.from_numpy(image_rgb).transpose(1, 2).transpose(0, 1)image = image.float().div(255) cv2.imwrite(path, image) TensorToImage 123image = image.mul(255).byte()image = image.transpose(0, 1).transpose(1, 2)image = image.cpu().numpy() 通道处理 12b, g, r = cv2.split(image)image = cv2.merge((b, g, r)) 图像处理与计算机视觉基础 二值化 cv2.threshold() 边缘检测 cv2.Canny() 图像滤波 cv2.filter2D() cv2.blur() cv2.GaussianBlur() 图像形态学操作 cv2.erode() 腐蚀 cv2.dilate() 膨胀 cv2.morphologyEx() 开闭运算、形态学梯度、顶帽、黑帽等","link":"/2024/06/28/knowledge/opencv/"},{"title":"python","text":"Python修饰器（语法糖） 详解Python修饰器（语法糖） 123@修饰函数def 待修饰函数(): pass 在调用待修饰函数时，直接调用修饰函数 在实现修饰函数时，需要注意待修饰函数的参数和返回值","link":"/2024/07/01/knowledge/python/"},{"title":"RAG技术","text":"深度解析RAG技术在大模型时代的原理与实践 概念","link":"/2024/07/09/knowledge/rag/"},{"title":"semantic-segmentation","text":"语义分割模型：从 FCN 到 DeepLab V3+的全面解析 2015 FCN（全卷积网络） 2015 年提出，首次将卷积神经网络应用于像素级分类，开创了端到端语义分割的先河。FCN 将最后几层全连接层替换为卷积层，允许任意大小的输入图像，并通过上采样（反卷积）恢复分割图的分辨率。 U-Net 特别适用于医学图像分割，其特征是编码器-解码器结构，解码器层与编码器层之间有跳跃连接，用以恢复细节信息。 2016 SegNet 基于 VGG 网络，改进了 FCN 的上采样部分，使用了编码器-解码器结构，其中解码器的上采样层使用了编码器的池化索引来恢复细节。 DeepLab V1 DeepLab 系列模型是语义分割领域的另一个重要里程碑。DeepLab V1 首次引入了 Atrous Convolution 模块，弥补了删除池化模块后感受野大小的影响。 2017 PSPNet（金字塔场景解析网络） 引入了金字塔池化模块（Pyramid Pooling Module），捕获不同尺度的信息，增强了模型对不同大小物体的分割能力。 DeepLab V2 DeepLab V2 在 ResNet 的基础上引入了 ASPP（Atrous Spatial Pyramid Pooling）模块，进一步扩大了感受野并提高了分割精度。 MobileNet V1 深度可分离卷积的应用 2018 DeepLab V3 而 DeepLab V3 通过增加 ASPP 模块的宽度，进一步提升了模型的性能。DeepLab V3+是 DeepLab 系列的最新版本，它增加了网络深度，将 Xception 网络作为主干，使用深度可分离卷积 Depthwise Separable Convolution， 结合了解码器模块，以恢复细节信息，进一步提升了多尺度处理能力。 ENet 专为实时应用设计，使用了高效的编码-解码结构和跳过连接。 MobileNet V2 Inverted Residuals 和 Linear Bottleneck ICNet 多尺度级联输入，深度监督 BiSeNet 结合了两个分支，Spatial 分支和 Context 分支。 2019 HRNet 维持高分辨率流以捕获更多细节，同时进行多尺度融合。 DANet Spatial Attention 和 Channel Attention Fast-SCNN：Fast Semantic Segmentation Network 共享下采样权重双分支网络 OCRNet（Object Contextual Representations） 引入了对象上下文表示，使用注意力机制来建模像素间的关系，增强对局部和全局上下文的理解。 CCNet 提出了交叉注意机制（Criss-Cross Attention），允许模型以较低的计算成本建模长距离依赖 MobileNet V3 SE 模块和 Swish 2021 SegFormer SETR 图像块编码，输入纯 Transformer 网络提取特征，reshape 特征后卷积上采样 MaskFormer 将分割统一为 Mask 分类任务 2023 Segment Anything Model (SAM) 一种通用的分割模型，能够处理多种类型的分割任务，包括语义分割、实例分割和全景分割。","link":"/2024/07/01/knowledge/semantic-segmentation/"},{"title":"Pytorch Source Code","text":"init 处理逻辑 判断当前运行环境，加载必须库文件 Define basFic utilities 定义基本工具 typename; is_tensor; ... Define numeric constants 定义数值常量 e; inf; nan; pi Define Storage and Tensor classes 定义 Storage 和 Tensor 类 ctypes 库 一个可以在 python 中调用由 C、C++编写并导出的 dll 动态链接库的包 ctypes.CDLL('vcruntime140.dll') 加载使用 C、C++编写的vcruntime140.dll文件 .pyi 文件 python 中的类型提示文件，也被叫做存根文件 stub file 用于提供代码的静态类型信息，也可以用来表示公共的接口 .pyi 文件给出变量或函数的静态类型，实现了 python 和 C、C++的绑定 参考 [1] Pytorch 底层源码解读（一）概览","link":"/2024/06/20/knowledge/torch/"},{"title":"ANCL","text":"Auxiliary Network Continual Learning (ANCL) Paper: Achieving a Better Stability-Plasticity Trade-off via Auxiliary Networks in Continual Learning Authors: Sanghwan Kim ; Lorenzo Noci ; Antonio Orvieto ; Thomas Hofmann Code: https://github.com/kim-sanghwan/ANCL Framework: Continual Learning (CL) 持续学习 符号定义 PT：Previous Task CT：Current Task 含义 保留 PT 信息的同时，继续在 CT 中进行学习 难点：Catastrophic Forgetting 灾难性的遗忘 对于梯度更新学习的模型，在学习 CT 的过程中更倾向于覆盖 PT 学习的梯度 换而言之，Stability-Plasticity Dilemma Martial Mermillod, Aur ́ elia Bugaiska, and Patrick Bonin. The stability-plasticity dilemma: Investigating the continuum from catastrophic forgetting to age-limited learning effects, 2013. 1 Stability: 在 PT 具有较好的泛化能力 Plasticity: 在 CT 学习新概念 所以，如何平衡 Stability 和 Plasticity是研究的重点 任务分类 类别增量学习(Class-Incremental Learning)的前世今生、开源工具包 Task Incremental Learning (TIL)：训练和测试阶段均为模型提供当前任务标识 Domain Incremental Learning (DIL)：测试阶段不提供当前任务标识 Class Incremental Learning (CIL)：测试阶段自动识别当前任务标识和分类 学习难度逐渐增加，ANCL 在 TIL 和 CIL 设置中进行了评估 相关工作 增加 Auxiliary Network 或 Extra Module Active Forgetting with synaptic Expansion-Convergence (AFEC) 超参控制新旧参数的融合 当前工作 框架化使用 Auxiliary Network 的 CL，使得 Auxiliary Network 插件化 通过和调整正则化项 局限 不同方法依赖于不同的超参 参考 [1] 类别增量学习(Class-Incremental Learning)的前世今生、开源工具包","link":"/2024/06/20/paper/ANCL/"},{"title":"Bi-VAEGAN","text":"Paper: Bi-directional Distribution Alignment for Transductive Zero-Shot Learning Authors: Zhicai Wang, Yanbin Hao, Tingting Mu, Ouxiang Li, Shuo Wang, Xiangnan He Code: GitHub Framework: Zero-shot Learning (ZSL) Hugo Larochelle, Dumitru Erhan, and Yoshua Bengio. Zerodata learning of new tasks. In AAAI, volume 1, page 3, 2008. 1 目标 解决训练时缺少例子或标签的问题 Conventional ZSL / Inductive ZSL 核心挑战 在存在Class Relevance的条件下，使得分类器能从 Seen Classes 提取信息迁移到 Unseen Classes 当中 Class Relevance 通常作为 Auxiliary Data 提供 Auxiliary Data 可以为人工标注、文字描述、知识图谱或 Formal Description of Knowledge（如嵌入向量） Domain Shift Problem 仅从 Auxiliary Data 学习容易导致 Unseen Classes 的真实分布与其建模分布之间存在差异 Proposed: Transductive ZSL (TZSL) 允许在训练中额外加入为目标类别收集的无标签示例 Generative Models 作用 Synthesize Examples 合成样本 Learn the Unseen Data Distribution 学习 unseen 数据分布 分类 Unconditional Generation Conditional Generation Auxiliary 信息是信息量更丰富的类标签，通过 Auxiliary 信息作为 Condition，可以学习到 Data-Auxiliary 联合分布，这连接了 Visual 空间和 Auxiliary 空间，使得生成器具有信息迁移的能力 难点 将 seen classes 所学迁移到 unseen classes f-VAEGAN 提出方法 Transductive Regressor Normalization Class Prior Estimation (CPE) 架构 VAE 编码器，得到维隐藏表征向量 条件生成器，以类别属性为条件，从正态分布采样维向量用于视觉特征生成 Wasserstein GAN（WGAN）的判别器，用于 seen classes WGAN 的判别器，用于 unseen classes 映射视觉空间到特征空间的 Regressor WGAN 的判别器，用于特征判别 Workflow Alt text Level-1 和对抗性训练 Level-2 和、对抗性训练","link":"/2024/06/21/paper/Bi-VAEGAN/"},{"title":"Code of Pixel-to-Prototype Constrast","text":"Generate CAMs Feature map Class feature map Score of class CAMs Pixel-to-Prototype Contrast Pseudo mask Pixel-wise projected feature Pixel-to-prototype contrast Prototype set Temperature Contrast 像素特征与原型的相似度 Prototype Estimation in Batch Top K pixels of class c CAM as confidences Estimate prototypes from pixel-wise feature embeddings that are with the top K confidences Prototype Loss Cross Prototype Contrast Cross CAM Contrast Intra-view Contrast Strategy to slove the matter of in accurate pseudo label [50] Semi-hard prototype mining Hard pixel sampling Code 归一化 归一化 作用 保证所有元素之和为 1 将向量转换为概率分布 归一化 1234# 按通道执行L2归一化v = v / (torch.norm(v, dim=1, keepdim=True) + 1e-5)# orv = torch.nn.functional.normalize(v, dim=1) 作用 方向不变性：向量的方向不变，长度变为 1，使得向量表示不再依赖于其大小 数值稳定性：将向量的大小规范在一个相对较小的区间 减小特征尺度的差异 便于执行相似性度量 Max 归一化 归一化后向量的最大值为 1 Max-Min 归一化 归一化后向量值范围为[0, 1] Forward cam 1234# fea是最后一层输出的特征图self.fc8 = nn.Conv2d(4096, 21, 1, bias=False)cam = self.fc8(fea)cam = torch.nn.functional.interpolate(cam, (H, W), mode='bilinear', align_corners=True) cam_rv_down 清洗 CAM 12345678910with torch.no_grad(): cam_d = torch.nn.functional.relu(cam.detach()) # max norm cam_d_max = torch.max(cam_d.view(n, c, -1), dim=-1)[0].view(n, c, 1, 1)+1e-5 cam_d_norm = torch.nn.functional.relu(cam_d - 1e-5) / cam_d_max # 计算保留概率值最大分类，反相为背景概率，其余分类置0 cam_d_norm[:, 0, :, :] = 1 - torch.max(cam_d_norm[:, 1:, :, :], dim=1)[0] cam_max = torch.max(cam_d_norm[:,1:,:,:], dim=1, keepdim=True)[0] cam_d_norm[:,1:,:,:][cam_d_norm[:,1:,:,:] &lt; cam_max] = 0 增强 CAM 1234567891011121314151617181920# 根据像素相似度调整CAMcam_rv_down = self.PCM(cam_d_norm, f)# PCMdef PCM(self, cam, f): n,c,h,w = f.size() cam = torch.nn.functional.interpolate(cam, (h,w), mode='bilinear', align_corners=True).view(n,-1,h*w) # 多尺度特征融合 f = self.f9(f) f = f.view(n, -1, h*w) # 特征按通道L2归一化 f = f / (torch.norm(f, dim=1, keepdim=True) + 1e-5) # 计算像素相似度矩阵 aff = torch.nn.functional.relu(torch.matmul(f.transpose(1, 2), f), inplace=True) # 相似度矩阵L1归一化 aff = aff/(torch.sum(aff, dim=1, keepdim=True) + 1e-5) # CAM加权 cam_rv = torch.matmul(cam, aff).view(n, -1, h, w) return cam_rv cam_rv 1cam_rv = torch.nn.functional.interpolate(cam_rv_down, (H,W), mode='bilinear', align_corners=True) f_proj 12self.fc_proj = torch.nn.Conv2d(4096, 128, 1, bias=False)f_proj = torch.nn.functional.relu(self.fc_proj(fea), inplace=True) prototype 12345678910111213141516171819202122232425262728293031323334353637f_proj1 = torch.nn.functional.interpolate(f_proj1, size=(128 // 8, 128 // 8), mode='bilinear', align_corners=True)cam_rv1_down = torch.nn.functional.interpolate(cam_rv1_down, size=(128 // 8, 128 // 8), mode='bilinear', align_corners=True)cam_rv2_down = cam_rv2_downwith torch.no_grad(): fea1 = f_proj1.detach() c_fea1 = fea1.shape[1] cam_rv1_down = torch.nn.functional.relu(cam_rv1_down.detach()) # CAM Max-min归一化 n1, c1, h1, w1 = cam_rv1_down.shape max1 = torch.max(cam_rv1_down.view(n1, c1, -1), dim=-1)[0].view(n1, c1, 1, 1) min1 = torch.min(cam_rv1_down.view(n1, c1, -1), dim=-1)[0].view(n1, c1, 1, 1) cam_rv1_down[cam_rv1_down &lt; min1 + 1e-5] = 0. norm_cam1 = (cam_rv1_down - min1 - 1e-5) / (max1 - min1 + 1e-5) cam_rv1_down = norm_cam1 # 设置背景阈值 cam_rv1_down[:, 0, :, :] = args.bg_threshold # 根据图像级标签保留相应的类别 scores1 = torch.nn.functional.softmax(cam_rv1_down * label, dim=1) # 计算伪标签 pseudo_label1 = scores1.argmax(dim=1, keepdim=True) n_sc1, c_sc1, h_sc1, w_sc1 = scores1.shape scores1 = scores1.transpose(0, 1) fea1 = fea1.permute(0, 2, 3, 1).reshape(-1, c_fea1) # 获取各个分类CAM值最高的值与索引 top_values, top_indices = torch.topk(cam_rv1_down.transpose(0, 1).reshape(c_sc1, -1), k=h_sc1 * w_sc1 // 8, dim=-1) prototypes1 = torch.zeros(c_sc1, c_fea1).cuda() # [21, 128] # 遍历各个分类 for i in range(c_sc1): # 获取k个像素对应的特征 top_fea = fea1[top_indices[i]] # CAM值加权平均k个特征得到分类原型 prototypes1[i] = torch.sum(top_values[i].unsqueeze(-1) * top_fea, dim=0) / torch.sum(top_values[i]) # 各个原型L2归一化 prototypes1 = torch.nn.functional.normalize(prototypes1, dim=-1) prototype similarity 12345678910111213141516171819202122232425n_f, c_f, h_f, w_f = f_proj1.shape# [N, H, W, C] -&gt; [N x H x W, C]f_proj1 = f_proj1.permute(0, 2, 3, 1).reshape(n_f * h_f * w_f, c_f)# 特征L2归一化f_proj1 = torch.nn.functional.normalize(f_proj1, dim=-1)pseudo_label1 = pseudo_label1.reshape(-1)positives1 = prototypes2[pseudo_label1]negitives1 = prototypes2# for targetn_f, c_f, h_f, w_f = f_proj2.shapef_proj2 = f_proj2.permute(0, 2, 3, 1).reshape(n_f * h_f * w_f, c_f)f_proj2 = torch.nn.functional.normalize(f_proj2, dim=-1)pseudo_label2 = pseudo_label2.reshape(-1)positives2 = prototypes1[pseudo_label2]negitives2 = prototypes1A1 = torch.exp(torch.sum(f_proj1 * positives1, dim=-1) / 0.1)A2 = torch.sum(torch.exp(torch.matmul(f_proj1, negitives1.transpose(0, 1)) / 0.1), dim=-1)loss_nce1 = torch.mean(-1 * torch.log(A1 / A2))A3 = torch.exp(torch.sum(f_proj2 * positives2, dim=-1) / 0.1)A4 = torch.sum(torch.exp(torch.matmul(f_proj2, negitives2.transpose(0, 1)) / 0.1), dim=-1)loss_nce2 = torch.mean(-1 * torch.log(A3 / A4))loss_cross_nce = 0.1 * (loss_nce1 + loss_nce2) / 2","link":"/2023/11/14/paper/Code-of-Pixel-to-Prototype-Constrast/"},{"title":"BiFormer","text":"Paper: BiFormer: Vision Transformer with Bi-Level Routing Attention Authors: Lei Zhu, Xinjiang Wang, Zhanghan Ke, Wayne Zhang, Rynson Lau Code: GitHub Framework: Transformer 优势 long-range dependency inductive-bias-free high parallelism 劣势 计算量大 内存占用大 现有方案：引入稀疏性 局部窗口 轴向注意力 空洞注意力 存在问题 筛选 key/value 时没有区分 query Bi-level Routing Attention (BRA) Sparsity 利用稀疏性来节省计算量和内存，同时只包含 GPU 友好的稠密矩阵乘法 Query-aware 为各个 Query 筛选语义最相关的 Key-Value 对 伪代码 1234567891011121314151617181920212223242526272829# input: features (H, W, C). Assume H==W.# output: features (H, W, C).# S: square root of number of regions.# k: number of regions to attend.# patchify input (H, W, C) -&gt; (Sˆ2, HW/Sˆ2, C)x = patchify(input, patch_size=H//S)# linear projection of query, key, valuequery, key, value = linear_qkv(x).chunk(3, dim=-1)# regional query and key (Sˆ2, C)query_r, key_r = query.mean(dim=1), key.mean(dim=1)# adjacency matrix for regional graph (Sˆ2, Sˆ2)A_r = mm(query_r, key_r.transpose(-1, -2))# compute index matrix of routed regions (Sˆ2, K)I_r = topk(A_r, k).index# gather key-value pairskey_g = gather(key, I_r)# (Sˆ2, kHW/Sˆ2, C)value_g = gather(value, I_r)# (Sˆ2, kHW/Sˆ2, C)# token-to-token attentionA = bmm(query, key_g.transpose(-2, -1))A = softmax(A, dim=-1)output = bmm(A, value_g) + dwconv(value)# recover to (H, W, C) shapeoutput = unpatchify(output, patch_size=H//S)","link":"/2024/06/26/paper/biformer/"},{"title":"machine-learning","text":"K-Nearest Neighbors（KNN） KNN 算法是一种基本的监督学习方法，用于分类和回归问题。在分类问题中，KNN 算法通过测量不同特征值之间的距离，找出输入实例与训练数据集中最接近的 k 个样本，然后根据这 k 个邻居的类别来预测新实例的类别。对于回归问题，KNN 则是预测新实例的连续值输出。 基本步骤 选择 K 值 K 值是算法中的一个超参数，表示考虑最近的邻居数量。选择 K 值时，较小的 K 值会使决策边界更加复杂，可能过拟合；较大的 K 值会简化决策边界，可能欠拟合。 计算距离 对于每一个训练样本，计算其与待分类样本之间的距离。常见的距离度量有欧氏距离、曼哈顿距离等。 找到 K 个最近邻 从计算出的距离中选出最小的 k 个，这 k 个样本即为待分类样本的最近邻。 预测类别或值 对于分类任务，对这 k 个样本的类别进行投票，票数最多的类别作为预测结果。 对于回归任务，通常采用这 k 个样本的目标值的平均值或加权平均值作为预测结果。 优点 算法简单直观，易于理解和实现。 不需要假设数据分布，适用于非线性分类问题。 可以处理多类问题。 缺点 计算成本高，特别是在大数据集上，因为需要计算测试样本与所有训练样本之间的距离。 需要选择合适的距离度量和 K 值。 对于不平衡的数据集，少数类别的预测可能会受到影响。 支持向量机 Support Vector Machine（SVM） 是一种非常强大的监督学习模型，主要用于分类和回归分析。在分类问题中，SVM 寻找一个最优的超平面（在高维空间中可能是一条线、一个平面或一个超平面），该超平面能够最大化地将不同类别的数据分开。这个超平面被称为最大间隔超平面，而那些离超平面最近的训练样本点则被称为支持向量。 SVM 的核心思想在于找到一个决策边界，使得正负两类样本被尽可能远地分开，同时确保边界两侧的样本被正确分类。在非线性情况下，SVM 通过使用核函数（Kernel Function）将低维空间中的数据映射到高维空间，从而在高维空间中找到一个线性的决策边界。 多类分类问题解决策略（如 K 类） 一对一（One-vs-One, OvO） 在这种策略中，每一对类别都会进行一次二分类，这意味着总共需要训练 C(K, 2) = K(K-1)/2 个 SVM 模型。最终分类时，每个样本都会被 K(K-1)/2 个模型分类，然后根据多数投票原则决定最终类别。 一对多（One-vs-All, OvA） 也称为 One-vs-Rest。对于 K 类问题，需要训练 K 个 SVM 模型，每个模型区分一类样本和其他所有类别的样本。当分类新的样本时，每个模型都会给出一个决策，样本会被分配给具有最高置信度分值的那类。 多分类 SVM 这是一种直接将多类分类问题建模的方法，它试图一次性找到一个超平面矩阵来区分所有的 K 类。 逐步二分类 DAGSVM Directed Acyclic Graph SVM 策略，这是一个专门设计用于多类分类的 SVM 变体，可以避免传统的投票机制。在逐步二分类（DAGSVM）策略中，构造了一个有向无环图（DAG），其中每个节点代表一个二分类器。DAG 的构建方式是从所有类别中选择两个类别进行比较，胜者（即被分类器认为更有可能的类别）继续与其他类别进行比较，直到确定最终的类别。这种方式确保了每个测试样本只需要通过一系列的二分类器，而不必经历所有可能的二分类器组合，从而避免了简单的多数投票机制。 svm 为什么要转成对偶问题进行求解，为什么对偶问题的解是原问题的解? svm 如何进行多分类，多分类 hinge loss 什么形式? 朴素贝叶斯分类器 朴素贝叶斯分类器是一种基于概率论的分类方法，尤其在文本分类、情感分析、垃圾邮件过滤等领域有着广泛的应用。它基于贝叶斯定理以及特征条件独立的假设，尽管这个假设在现实世界的数据中往往不成立，但朴素贝叶斯分类器在许多情况下仍然表现出良好的性能，尤其是在数据量较大时。 贝叶斯定理 朴素贝叶斯分类器的基础是贝叶斯定理，它描述了已知某些条件下，事件 A 发生的概率 P(A|B)，可以用以下公式表示： 先验概率 (Prior Probability) 先验概率是指在观察到任何数据或证据之前，基于以往知识或经验对某事件发生概率的估计。它是我们的“先验”信念，反映了我们对某个假设在没有任何新信息情况下的初始信心程度。例如，在投掷一枚硬币的情况下，如果我们没有任何关于这枚硬币是否公平的信息，我们可能会设定正反两面出现的概率都是 50%，这就是一种先验概率。 后验概率 (Posterior Probability) 后验概率是指在观察到某些数据或证据之后，对事件发生概率的更新估计。它是根据先验概率以及新的观测数据，通过应用贝叶斯定理计算得出的。后验概率反映了我们对某个假设的新信念，这种信念是在考虑到所有可用信息后形成的。继续用硬币的例子，如果我们连续投掷硬币十次，其中有九次正面朝上，我们可能会调整我们对硬币公平性的信念，认为硬币可能是偏向正面的，此时我们对硬币偏向正面的概率的估计就是后验概率。 特征条件独立假设 朴素贝叶斯分类器假设所有特征在给定类别的情况下是相互独立的。这意味着，对于分类问题中的每个特征，其取值不会受到其它特征取值的影响，即使在实际中这种独立性可能并不存在。这一假设简化了计算，使得分类器的训练变得相对简单，同时也使得朴素贝叶斯分类器能够处理高维数据。 分类过程 朴素贝叶斯分类器的工作流程通常包括以下步骤： 训练阶段：计算每个类别的先验概率以及每个特征在各类别下的条件概率。 预测阶段：对于一个新的输入样本，计算其属于每个类别的后验概率，然后选择具有最大后验概率的类别作为预测结果。 优点 算法简单，易于实现。 训练速度快，所需的计算资源较少。 在特征条件独立假设下，可以处理高维数据。 缺点 条件独立假设在实际应用中往往是不成立的，这可能会影响分类器的准确性。 对于从未在训练数据中出现过的特征值，朴素贝叶斯分类器会给出 0 的概率，这可能导致分类失败。为了避免这种情况，通常会采用平滑技术，如拉普拉斯平滑。 独立成分分析（ICA） 独立成分分析是一种统计和计算技术，用于估计一组随机变量的潜在因素或成分，这些成分被认为是相互独立的。ICA 的目标是找到一组变换，将混合的观测信号分解成独立的源信号。与主成分分析不同，ICA 关注的是信号的统计独立性，而非方差最大化。ICA 在信号分离、盲源分离等领域有广泛应用。 数据降维 奇异值分解（SVD） 异值分解是一种矩阵分解技术，广泛应用于信号处理、数据压缩、模式识别等多个领域。对于任意一个矩阵 A（m×n），SVD 将其分解为三个矩阵的乘积：UΣV^T，其中 U 是 m×m 的正交矩阵，Σ 是 m×n 的对角矩阵，其对角线上的元素是 A 的奇异值（通常是降序排列的非负实数），V 是 n×n 的正交矩阵。SVD 在数据分析中常用于提取矩阵的主要成分，如降维、数据压缩、特征提取等。 主成分分析（PCA） 主成分分析是一种常用的统计方法，用于识别数据中的模式和结构。PCA 通过正交变换将可能相关的变量转换为一组线性不相关的变量，称为主成分。主成分是原变量的线性组合，第一个主成分具有数据的最大方差，第二个主成分具有次大方差，同时与第一个主成分正交，以此类推。PCA 常用于降维、数据可视化和噪声消除。 线性判别分析（Linear Discriminant Analysis, LDA） 是一种常用的数据分析和分类技术，最初由 R.A. Fisher 提出，因此有时也被称作 Fisher 判别分析。LDA 主要用于寻找最佳的低维空间投影，使得不同类别的样本在该空间中的分离度最大化，同时保持同类样本之间的紧凑性。LDA 经常被用于降维和分类任务，特别是在监督学习的场景下。 LDA 的主要目标 LDA 的主要目标是找到一个投影，使得投影后的数据在不同类别间具有最大的可区分性，同时在同一类别内具有最小的差异性。具体而言，LDA 试图最大化类间散度和最小化类内散度，以此来增强分类效果。 LDA 的步骤 计算类内散度矩阵 Sw 和类间散度矩阵 Sb： 类内散度矩阵 Sw 反映了同一类别内部样本之间的变异程度。 类间散度矩阵 Sb 则反映了不同类别之间中心点的变异程度。 求解特征向量和特征值： 通过求解 Sw 和 Sb 的广义特征值问题，找到能最大化类间差异的投影方向。 选择投影方向： 选择前 k 个最大的特征值对应的特征向量作为投影方向，其中 k 是希望得到的降维后的特征数量。 投影数据： 将原始数据投影到找到的投影方向上，得到降维后的数据。 分类： 降维后的数据可以使用各种分类器进行分类，如线性分类器。 LDA 的应用 LDA 在多个领域都有广泛应用，包括但不限于： 人脸识别：LDA 可以用于提取人脸图像的重要特征，从而在低维空间中进行人脸识别。 生物信息学：在基因表达数据的分析中，LDA 可以帮助识别不同疾病状态的基因表达模式。 市场细分：在市场营销中，LDA 可以用来识别消费者群体的不同特征，帮助进行市场细分。 医学诊断：LDA 可用于诊断疾病，通过分析患者数据来区分健康个体和患病个体。 LDA 与主成分分析（PCA） 方法类似，但 PCA 是一种无监督的降维方法，而 LDA 是有监督的，它利用了类标签信息，这使得 LDA 在分类任务中通常能提供更好的性能。然而，LDA 也有其局限性，比如它假设数据服从高斯分布，且不同类别的协方差矩阵相等，这在某些情况下可能不成立。 拉普拉斯特征映射（Laplacian Eigenmaps） 是一种非线性降维技术，由 Belkin 和 Niyogi 在 2003 年提出，主要用于在保持数据局部结构的同时，将高维数据嵌入到低维空间中。Laplacian Eigenmaps 的核心思想是利用数据点之间的相似性来构建一个图，然后在图上找到最优的低维表示，这种表示能够尽可能地保持原数据点间的局部距离不变。 拉普拉斯特征映射的基本步骤： 构建邻接矩阵：首先，为数据集中的每个点 i 找到其最近的 k 个邻居，然后构建邻接矩阵 W，其中 W(i,j) &gt; 0 如果点 j 是点 i 的邻居，否则 W(i,j) = 0。W(i,j)的具体值可以是点 i 和点 j 之间的相似性度量，比如高斯核函数。 构建度矩阵：度矩阵 D 是一个对角矩阵，其中 D(i,i)等于第 i 个点的度，即邻接矩阵 W 中第 i 行元素之和。 构建拉普拉斯矩阵：拉普拉斯矩阵 L 定义为 L = D - W。这是图拉普拉斯算子的一种形式，它捕获了图的连通性和拓扑性质。 求解特征值问题：寻找拉普拉斯矩阵 L 的特征向量和特征值。最小的几个非零特征值对应的特征向量提供了数据点在低维空间中的坐标。 数据点的低维表示：将数据点映射到低维空间中，使用最小的几个非零特征值对应的特征向量作为坐标轴。这些特征向量捕捉了数据集的内在几何结构，使得数据点在低维空间中的相对位置保持了它们在高维空间中的局部关系。 应用场景： 数据可视化：Laplacian Eigenmaps 可以用于将高维数据可视化，通过降维将数据投影到二维或三维空间，以便于人类观察和理解。 聚类分析：通过在低维嵌入空间中应用聚类算法，可以发现数据的潜在结构和簇。 图像处理：在图像分析和识别中，可以利用 Laplacian Eigenmaps 来提取图像的关键特征。 与其它降维技术的对比： 与 PCA（主成分分析）相比，Laplacian Eigenmaps 更关注数据的局部结构，而 PCA 则关注全局的方差最大化。PCA 是一种线性降维技术，而 Laplacian Eigenmaps 能够处理非线性数据集，这对于复杂数据结构特别有用。 总的来说，Laplacian Eigenmaps 是一种强有力的非线性降维工具，特别适用于保持数据的局部连通性和拓扑结构，使其成为分析高维数据集的有效手段。 LLE KPCA ISOMAP 决策树（Decision Trees） 决策树是一种用于分类和回归的机器学习算法。它通过构建一棵树形结构，其中内部节点表示特征上的测试，分支表示测试结果，而叶子节点表示类别或数值。决策树通过递归地分割数据集，每次分割都选择最优的特征和分割点，以最大程度地减少不纯度（如基尼不纯度或熵）。决策树易于理解和解释，可以处理数值和分类数据，但容易过拟合，通常需要剪枝技术来控制复杂度。 决策树对连续值和离散值特征是否会重复利用作为分割特征? ID3 算法的局限性 只处理离散属性：ID3 算法只能处理离散属性，无法直接处理连续属性。 信息增益偏好：ID3 使用信息增益作为分裂标准，这导致它偏好具有更多值的属性，即使这些属性可能并不提供更多的分类信息。 缺失值处理：ID3 算法没有提供处理缺失值的有效机制。 过拟合问题：ID3 算法容易过拟合，因为它总是尽可能深地生长树，直到所有叶子节点都是纯的。 C4.5 的改进 处理连续属性：C4.5 可以处理连续属性，通过寻找最佳的切分点将连续属性转化为二元离散属性。 使用增益率：C4.5 使用增益率（Gain Ratio）代替信息增益作为分裂属性的选择标准。增益率考虑了信息增益与属性的熵的比率，从而避免了信息增益偏好的问题。 处理缺失值：C4.5 提供了一种处理缺失值的方法，允许在不知道完整信息的情况下仍能进行分类。 剪枝策略：C4.5 引入了剪枝技术，以防止过拟合。它使用两种剪枝方法：悲观剪枝（使用一个用户定义的阈值来决定是否剪枝）和错误估计剪枝（基于树的错误率来决定是否剪枝）。 生成规则集：C4.5 不仅生成决策树，还可以从决策树中导出一套规则，这使得模型更容易被理解和解释。 处理多类分类：C4.5 可以很好地处理多类分类问题，而不仅仅是二分类。 CART 算法的特点： 二叉树结构：CART 生成的决策树是二叉树，每个非叶节点都有两个子节点，这使得树的结构更加简单，易于理解和实现。 变量选择：对于分类树，CART 使用基尼不纯度（Gini Impurity）作为特征选择的标准；对于回归树，则使用平方误差（Mean Squared Error）来评估分裂的质量。 可处理连续和离散变量：CART 能够自动处理连续和离散变量，对于连续变量，它会找到最佳的分割点来创建二元分裂。 缺失值处理：CART 提供了处理缺失值的策略，例如使用替代值或者通过预测缺失值的可能性来分配样本到不同的分支。 剪枝策略：CART 采用成本复杂性剪枝（Cost Complexity Pruning）来防止过拟合，这是一种后剪枝方法，通过在树的复杂性和拟合数据的误差之间寻找平衡点来简化决策树。 BP 神经网络（Backpropagation Neural Networks） BP 神经网络是人工神经网络的一种，通过前向传播计算输出，再通过反向传播调整权重，以最小化网络输出与期望输出之间的误差。BP 算法使用梯度下降法更新权重，使网络能够学习复杂的非线性映射。神经网络可以处理大量输入和输出，具有很强的学习能力，但训练时间可能较长，且可能遇到局部最优解的问题。 随机森林（Random Forests） 随机森林是一种集成学习方法，通过构建多个决策树并将它们的预测结果进行投票来改进预测性能。随机森林在构建每棵树时，不仅随机选择样本，还随机选择特征，这样可以减少过拟合风险并提高模型的泛化能力。随机森林能够处理高维数据，对于异常值和缺失值具有较好的鲁棒性。 随机森林与 GBDT，解释、比较异同、哪种方法单棵决策树的深度更大? 判别式模型 vs 生成式模型 判别式模型（Discriminative Models） 直接学习输入特征和输出标签之间的映射关系，目标是最小化预测错误。例如，BP 神经网络、支持向量机都属于判别式模型的范畴，它们直接学习如何将输入映射到输出。 生成式模型（Generative Models） 试图学习数据和标签的联合分布，通常用于估计类条件概率。例如，朴素贝叶斯分类器就是一种生成式模型，它估计了给定类别的条件下特征的概率分布。 NLP TF-IDF（Term Frequency-Inverse Document Frequency） 是一种用于信息检索和文本挖掘的统计方法，它评估一个词对一个文档集合或语料库中的某篇文档的重要程度。TF-IDF 由两部分组成：词频（TF）和逆文档频率（IDF）。词频是指一个词在文档中出现的频率，而逆文档频率则反映了一个词在整个文档集合中的罕见程度。 文本分类中，从含有兼类可分为多标签分类和单标签分类 距离算法 1. 欧几里得距离（Euclidean Distance） 欧几里得距离是最直观的距离度量方式，它计算的是两个点在多维空间中的直线距离。在二维空间中，如果点 A 的坐标是((x_1, y_1))，点 B 的坐标是((x_2, y_2))，那么它们之间的欧几里得距离为： [ d_{AB} = ] 推广到 n 维空间，如果有两个 n 维向量( = (a_1, a_2, ..., a_n))和( = (b_1, b_2, ..., b_n))，它们之间的欧几里得距离为： [ d*{AB} = ] 2. 曼哈顿距离（Manhattan Distance） 曼哈顿距离，也称为城市街区距离或 L1 距离，它衡量的是两点在坐标轴上的绝对距离之和。在 n 维空间中，两个向量()和()之间的曼哈顿距离为： [ d*{AB} = ^{n}|a_i - b_i| ] 曼哈顿距离在网格布局或出租车行驶路径计算中很有用，因为它忽略了两点之间的直线距离，只考虑沿坐标轴方向的移动。 3. Canberra 距离 Canberra 距离是一种用于非负向量的度量，它在计算距离时考虑了分母，使得距离受向量元素值的影响更大。两个非负向量()和()之间的 Canberra 距离定义为： [ d*{AB} = ^{n} ] Canberra 距离在处理稀疏数据时特别有用，因为它可以更好地处理零值。 4. 切比雪夫距离（Chebyshev Distance） 切比雪夫距离，也称为棋盘距离或 L∞ 距离，它衡量的是两个点在任一坐标轴上的最大绝对差值。在 n 维空间中，两个向量()和()之间的切比雪夫距离为： [ d*{AB} = (|a_i - b_i|) ] 切比雪夫距离在某些应用场景中特别有用，比如在国际象棋中计算棋子移动的最短步数。 每种距离度量都有其特点和适用场景，选择合适的距离算法对于数据的分析和建模至关重要。例如，欧几里得距离适用于测量连续空间中的相似性，而曼哈顿距离和切比雪夫距离可能在某些特殊的空间布局或数据结构中更有优势。 梯度算子 梯度算子是数字图像处理和计算机视觉领域中用于边缘检测和特征提取的重要工具。梯度算子能够突出图像中的边缘和纹理，通过计算图像像素强度的局部变化来检测边界。以下是几种常用的梯度算子模板： 1. Sobel 算子 Sobel 算子是使用最广泛的边缘检测算子之一，它通过计算图像在水平和垂直方向上的梯度来检测边缘。Sobel 算子由两个 3x3 的卷积核构成： 水平方向梯度算子： 垂直方向梯度算子： Sobel 算子对图像进行卷积操作后，可以得到水平和垂直方向上的梯度分量，最后通过合并这两个分量来获得边缘强度。 2. Prewitt 算子 Prewitt 算子与 Sobel 算子类似，也使用两个 3x3 的卷积核来检测水平和垂直方向上的梯度，但 Prewitt 算子的权重分布较为均匀： 水平方向梯度算子： 垂直方向梯度算子： 3. Roberts 交叉算子 Roberts 交叉算子使用 2x2 的小型卷积核，可以快速计算图像的梯度： 水平方向梯度算子： 垂直方向梯度算子： Roberts 算子对噪声敏感，但在计算速度上有优势。 4. Laplacian 算子 Laplacian 算子不是直接检测梯度，而是检测二阶导数，用于检测边缘的拐点。一个常用的 Laplacian 算子模板是： 5. LoG 算子 全称为 Laplacian of Gaussian 算子，是一种在图像处理和计算机视觉中用于边缘检测和特征提取的算法。LoG 算子结合了高斯平滑滤波器（Gaussian filter）和拉普拉斯算子（Laplacian operator），其主要目的是在抑制噪声的同时检测图像中的边缘和其他显著特征。 工作原理： 高斯平滑： 首先，LoG 算子使用高斯滤波器对图像进行平滑处理。高斯滤波器通过应用一个高斯分布的权重矩阵（核）来对每个像素及其邻域进行加权平均，从而减少图像中的高频噪声。 拉普拉斯算子： 接着，经过高斯平滑后的图像会被进一步应用拉普拉斯算子。拉普拉斯算子是一个二阶微分算子，它对图像的亮度变化进行二次导数计算，能够突出图像中的突变区域，如边缘。 LoG 滤波器： 将上述两个步骤合在一起，LoG 算子实际上是计算高斯平滑后的图像的拉普拉斯变换。数学上，LoG 算子定义为高斯函数的拉普拉斯变换，表达式为： [ ^2 g(x,y,) = g(x,y,) + g(x,y,) ] 其中 (g(x,y,)) 是二维高斯函数，() 是标准差，决定了高斯核的宽度。 应用： LoG 算子常被用于边缘检测和图像特征点检测，特别是在尺度空间理论中，它可以用来检测图像中的关键点，这些点可能对应于角点、边缘交叉点或其他重要的图像特征。 由于 LoG 算子的输出可能包含大量的零过零点（zero-crossings），这些点被认为是潜在的边缘或特征位置。因此，在实际应用中，通常会对 LoG 响应进行阈值处理，只保留那些超过一定阈值的零过零点作为有效的特征点。 实现： 在实践中，LoG 算子可以通过卷积操作实现，即将图像与预定义的 LoG 核进行卷积。LoG 核是高斯核和拉普拉斯算子的离散近似，其形状取决于选择的()值。 为了简化计算，有时会先应用高斯核，再单独应用拉普拉斯算子，尽管这在数学上不完全等同于 LoG 算子，但可以达到类似的效果，这种方法被称为 DoG（Difference of Gaussians）算子，是 LoG 的一个近似版本，常用于尺度不变特征变换（SIFT）等算法中。 LoG 算子因其在抑制噪声和定位边缘及特征点方面的有效性，在图像处理领域有着广泛的应用。 色彩模型 是用来描述颜色的一种数学表示方法，它们按照不同的用途和目的对颜色进行分类和编码。你提到的色彩模型可以大致分为两大类：面向硬件设备的色彩模型和面向视觉感知的色彩模型。 面向硬件设备的彩色模型 这类模型主要用于设备的颜色表示，它们的设计是为了与特定类型的硬件或色彩再现过程兼容。 RGB 模型（Red, Green, Blue）： RGB 模型是基于加色法原理的色彩模型，通常用于显示设备如电视、电脑显示器和手机屏幕。在 RGB 模型中，红、绿、蓝三种颜色的光以不同的强度混合，可以产生广泛的色彩范围。 CMYK 模型（Cyan, Magenta, Yellow, Key/Black）： CMYK 模型是基于减色法原理的色彩模型，主要用于印刷行业。在印刷过程中，青色、洋红色、黄色和黑色油墨以不同比例混合，可以复制出各种色彩。 YCrCb 模型（Luminance/Chrominance）： YCrCb 模型是一种用于视频系统的色彩模型，其中 Y 代表亮度（Luminance），Cr 和 Cb 分别代表红色和蓝色的色差（Chrominance）。这种模型被用于数字电视和图像压缩中，如 JPEG 2000 和 H.26x 系列视频编码标准。 面向视觉感知的彩色模型 这类模型的设计更贴近人类视觉系统对颜色的感知方式，通常用于色彩管理和色彩选择界面。 HSI 模型（Hue, Saturation, Intensity）： HSI 模型将颜色分为色调（Hue）、饱和度（Saturation）和强度（Intensity）三个组成部分。色调描述了颜色的基本类型，饱和度表示颜色的纯度，强度则是颜色的亮度。 HSV 模型（Hue, Saturation, Value）： HSV 模型与 HSI 模型类似，但将强度（Intensity）替换为价值（Value）。价值表示颜色的明暗程度，而色调和饱和度的定义与 HSI 模型相同。 HSB 模型（Hue, Saturation, Brightness）： HSB 模型实际上与 HSV 模型是相同的，只是将“Value”称为“Brightness”。在某些软件中，如 Adobe Photoshop，使用 HSB 来表示 HSV 模型。","link":"/2024/06/26/knowledge/machine-learning/"}],"tags":[{"name":"leetcode","slug":"leetcode","link":"/tags/leetcode/"},{"name":"排序辅助","slug":"排序辅助","link":"/tags/%E6%8E%92%E5%BA%8F%E8%BE%85%E5%8A%A9/"},{"name":"algorithm","slug":"algorithm","link":"/tags/algorithm/"},{"name":"tree","slug":"tree","link":"/tags/tree/"},{"name":"双指针","slug":"双指针","link":"/tags/%E5%8F%8C%E6%8C%87%E9%92%88/"},{"name":"企业","slug":"企业","link":"/tags/%E4%BC%81%E4%B8%9A/"},{"name":"动态规划","slug":"动态规划","link":"/tags/%E5%8A%A8%E6%80%81%E8%A7%84%E5%88%92/"},{"name":"区间合并","slug":"区间合并","link":"/tags/%E5%8C%BA%E9%97%B4%E5%90%88%E5%B9%B6/"},{"name":"哈希表","slug":"哈希表","link":"/tags/%E5%93%88%E5%B8%8C%E8%A1%A8/"},{"name":"堆","slug":"堆","link":"/tags/%E5%A0%86/"},{"name":"链表","slug":"链表","link":"/tags/%E9%93%BE%E8%A1%A8/"},{"name":"数学推理","slug":"数学推理","link":"/tags/%E6%95%B0%E5%AD%A6%E6%8E%A8%E7%90%86/"},{"name":"矩阵处理","slug":"矩阵处理","link":"/tags/%E7%9F%A9%E9%98%B5%E5%A4%84%E7%90%86/"},{"name":"滑动窗口","slug":"滑动窗口","link":"/tags/%E6%BB%91%E5%8A%A8%E7%AA%97%E5%8F%A3/"},{"name":"栈","slug":"栈","link":"/tags/%E6%A0%88/"},{"name":"字符串","slug":"字符串","link":"/tags/%E5%AD%97%E7%AC%A6%E4%B8%B2/"},{"name":"树","slug":"树","link":"/tags/%E6%A0%91/"},{"name":"interview","slug":"interview","link":"/tags/interview/"},{"name":"code","slug":"code","link":"/tags/code/"},{"name":"pytorch","slug":"pytorch","link":"/tags/pytorch/"},{"name":"CVPR2023","slug":"CVPR2023","link":"/tags/CVPR2023/"},{"name":"CL","slug":"CL","link":"/tags/CL/"},{"name":"ZSL","slug":"ZSL","link":"/tags/ZSL/"},{"name":"GAN","slug":"GAN","link":"/tags/GAN/"},{"name":"CVPR2022","slug":"CVPR2022","link":"/tags/CVPR2022/"},{"name":"Prototype","slug":"Prototype","link":"/tags/Prototype/"},{"name":"Transformer","slug":"Transformer","link":"/tags/Transformer/"}],"categories":[{"name":"algorithm","slug":"algorithm","link":"/categories/algorithm/"},{"name":"interview","slug":"interview","link":"/categories/interview/"},{"name":"source code","slug":"source-code","link":"/categories/source-code/"},{"name":"paper","slug":"paper","link":"/categories/paper/"}],"pages":[]}